<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Strongly Convex - Daniel Duckworth</title><link href="/" rel="alternate"></link><link href="None/feeds/daniel-duckworth.atom.xml" rel="self"></link><id>/</id><updated>2014-08-01T00:00:00-07:00</updated><entry><title>The Big Table of Convergence Rates</title><link href="/blog/big-table-of-convergence-rates.html" rel="alternate"></link><published>2014-08-01T00:00:00-07:00</published><updated>2014-08-01T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2014-08-01:/blog/big-table-of-convergence-rates.html</id><summary type="html">&lt;p&gt;In the past 50+ years of convex optimization research, a great many
algorithms have been developed, each with slight nuances to their assumptions,
implementations, and guarantees. In this article, I'll give a shorthand
comparison of these methods in terms of the number of iterations required
to reach a desired accuracy …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the past 50+ years of convex optimization research, a great many
algorithms have been developed, each with slight nuances to their assumptions,
implementations, and guarantees. In this article, I'll give a shorthand
comparison of these methods in terms of the number of iterations required
to reach a desired accuracy $\epsilon$ for convex and strongly convex objective
functions.&lt;/p&gt;
&lt;p&gt;Below, methods are grouped according to what "order" of information they
require about the objective function. In general, the more information you
have, the faster you can converge; but beware, you will also need more memory
and computation. Zeroth and first order methods are typically appropriate for
large scale problems, whereas second order methods are limited to
small-to-medium scale problems that require a high degree of precision.&lt;/p&gt;
&lt;p&gt;At the bottom, you will find algorithms aimed specifically at minimizing
supervised learning problems and other meta-algorithms useful for distributing
computation across multiple nodes.&lt;/p&gt;
&lt;p&gt;Unless otherwise stated, all objectives are assumed to be Lipschitz
continuous (though not necssarily differentiable) and the domain convex. The
variable being optimized is $x \in \mathbb{R}^n$.&lt;/p&gt;
&lt;h1&gt;Zeroth Order Methods&lt;/h1&gt;
&lt;p&gt;Zeroth order methods are characterized by not requiring any gradients or
subgradients for their objective functions. In exchange, however, it is
assumed that the objective is "simple" in the sense that a subset of variables
(a "block") can be minimized exactly while holding all other variables fixed.&lt;/p&gt;
&lt;table class="table table-bordered table-centered"&gt;
&lt;p&gt;&lt;colgroup&gt;
    &lt;col style="width:20%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:40%"&gt;
  &lt;/colgroup&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Algorithm          &lt;/th&gt;
      &lt;th&gt;Problem Formulation&lt;/th&gt;
      &lt;th&gt;Convex             &lt;/th&gt;
      &lt;th&gt;Strongly Convex    &lt;/th&gt;
      &lt;th&gt;Per-Iteration Cost &lt;/th&gt;
      &lt;th&gt;Notes              &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;&lt;/p&gt;
&lt;p&gt;&lt;tbody&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Randomized Block Coordinate Descent&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^{n}} f(x) + g(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon)$&lt;sup id="fnref-richtarik-2011"&gt;&lt;a class="footnote-ref" href="#fn-richtarik-2011"&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1 / \epsilon))$&lt;sup id="fnref2-richtarik-2011"&gt;&lt;a class="footnote-ref" href="#fn-richtarik-2011"&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(1)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f(x)$ is differentiable and $g(x)$ is separable in
        each block. $g(x)$ may be a barrier function.
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;&lt;/p&gt;
&lt;/table&gt;
&lt;h1&gt;First Order Methods&lt;/h1&gt;
&lt;p&gt;First order methods typically require access to an objective function's
gradient or subgradient. The algorithms typically take the form $x^{(t+1)}
= x^{(t)} - \alpha^{(t)} g^{(t)}$ for some step size $\alpha^{(t)}$ and descent
direction $g^{(t)}$. As such, each iteration takes approximately $O(n)$ time.&lt;/p&gt;
&lt;table class="table table-bordered table-centered"&gt;
&lt;p&gt;&lt;colgroup&gt;
    &lt;col style="width:20%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:40%"&gt;
  &lt;/colgroup&gt;&lt;/p&gt;
&lt;p&gt;&lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Algorithm          &lt;/th&gt;
      &lt;th&gt;Problem Formulation&lt;/th&gt;
      &lt;th&gt;Convex             &lt;/th&gt;
      &lt;th&gt;Strongly Convex    &lt;/th&gt;
      &lt;th&gt;Per-Iteration Cost &lt;/th&gt;
      &lt;th&gt;Notes              &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;&lt;/p&gt;
&lt;p&gt;&lt;tbody&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Subgradient Descent&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle  \min_{x \in \mathbb{R}^n} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon^{2})$&lt;sup id="fnref-blog-sd"&gt;&lt;a class="footnote-ref" href="#fn-blog-sd"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Cannot be improved upon without further assumptions.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Mirror Descent&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathcal{C}} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon^{2} )$&lt;sup id="fnref-ee381-md"&gt;&lt;a class="footnote-ref" href="#fn-ee381-md"&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(1 / \epsilon )$&lt;sup id="fnref-nedich-2013"&gt;&lt;a class="footnote-ref" href="#fn-nedich-2013"&gt;26&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Different parameterizations result in gradient descent and
        exponentiated gradient descent.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Dual Averaging&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle  \min_{x \in \mathcal{C}} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon^{2})$&lt;sup id="fnref-nesterov-2007"&gt;&lt;a class="footnote-ref" href="#fn-nesterov-2007"&gt;25&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Cannot be improved upon without further assumptions.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Gradient Descent&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon)$&lt;sup id="fnref-blog-gd"&gt;&lt;a class="footnote-ref" href="#fn-blog-gd"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1 / \epsilon))$&lt;sup id="fnref-ee381-gd"&gt;&lt;a class="footnote-ref" href="#fn-ee381-gd"&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f(x)$ is differentiable.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Accelerated Gradient Descent&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \sqrt{\epsilon})$&lt;sup id="fnref-blog-agd"&gt;&lt;a class="footnote-ref" href="#fn-blog-agd"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1 / \epsilon))$&lt;sup id="fnref-bubeck-agd"&gt;&lt;a class="footnote-ref" href="#fn-bubeck-agd"&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f(x)$ is differentiable.
        Cannot be improved upon without further assumptions.
        Has better constants than Gradient Descent for "Strongly Convex" case.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Proximal Gradient Descent&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathcal{C}} f(x) + g(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon)$&lt;sup id="fnref-blog-pgd"&gt;&lt;a class="footnote-ref" href="#fn-blog-pgd"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1 / \epsilon))$&lt;sup id="fnref3-mairal-2013"&gt;&lt;a class="footnote-ref" href="#fn-mairal-2013"&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f(x)$ is differentiable and
        $\text{prox}&lt;em _in="\in" _mathcal_C="\mathcal{C" x&gt;{\tau_t g}(x)$ is easily computable.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Proximal Accelerated Gradient Descent&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min&lt;/em&gt;} f(x) + g(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \sqrt{\epsilon})$&lt;sup id="fnref-blog-apgd"&gt;&lt;a class="footnote-ref" href="#fn-blog-apgd"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1 / \epsilon))$&lt;sup id="fnref4-mairal-2013"&gt;&lt;a class="footnote-ref" href="#fn-mairal-2013"&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f(x)$ is differentiable and
        $\text{prox}&lt;em _in="\in" _mathcal_C="\mathcal{C" x&gt;{\tau_t g}(x)$ is easily computable.
        Has better constants than Proximal Gradient Descent for "Strongly
        Convex" case.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Frank-Wolfe Algorithm / Conditional Gradient Algorithm&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min&lt;/em&gt;} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1/\epsilon)$&lt;sup id="fnref-blog-fw"&gt;&lt;a class="footnote-ref" href="#fn-blog-fw"&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(1/\sqrt{\epsilon})$&lt;sup id="fnref-garber-2014"&gt;&lt;a class="footnote-ref" href="#fn-garber-2014"&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $\mathcal{C}$ is bounded and $h_{g}(x) = \arg\min_{x \in
        \mathcal{C}} \langle g, x \rangle$ is easily computable. Most useful
        when $\mathcal{C}$ is a polytope in a very high dimensional space with
        sparse extrema.
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;&lt;/p&gt;
&lt;/table&gt;
&lt;h1&gt;Second Order Methods&lt;/h1&gt;
&lt;p&gt;Second order methods either use or approximate the hessian ($\nabla^2 f(x)$)
of the objective function to result in better-than-linear rates of convergence.
As such, each iteration typically requires $O(n^2)$ memory and between $O(n^2)$
and $O(n^3)$ computation per iteration.&lt;/p&gt;
&lt;table class="table table-bordered table-centered"&gt;
&lt;p&gt;&lt;colgroup&gt;
    &lt;col style="width:20%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:40%"&gt;
  &lt;/colgroup&gt;&lt;/p&gt;
&lt;p&gt;&lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Algorithm          &lt;/th&gt;
      &lt;th&gt;Problem Formulation&lt;/th&gt;
      &lt;th&gt;Convex             &lt;/th&gt;
      &lt;th&gt;Strongly Convex    &lt;/th&gt;
      &lt;th&gt;Per-Iteration Cost &lt;/th&gt;
      &lt;th&gt;Notes              &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;&lt;/p&gt;
&lt;p&gt;&lt;tbody&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Newton's Method&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log \log (1/\epsilon))$&lt;sup id="fnref-ee364a-unconstrained"&gt;&lt;a class="footnote-ref" href="#fn-ee364a-unconstrained"&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n^3)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Only applicable when $f(x)$ is twice differentiable. Constraints can be
        incorporated via interior point methods.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Conjugate Gradient Descent&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n^2)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Converges in exactly $n$ steps for quadratic $f(x)$. May fail to
        converge for non-quadratic objectives.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;L-BFGS&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;Between $O(\log (1/\epsilon))$ and $O(\log \log (1/\epsilon))$&lt;sup id="fnref-ee236c-qnewton"&gt;&lt;a class="footnote-ref" href="#fn-ee236c-qnewton"&gt;15&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n^2)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f(x)$ is differentiable, but works best when twice
        differentiable. Convergence rate is not guaranteed.
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;&lt;/p&gt;
&lt;/table&gt;
&lt;h1&gt;Stochastic Methods&lt;/h1&gt;
&lt;p&gt;The following algorithms are specifically designed for supervised machine
learning where the objective can be decomposed into independent "loss"
functions and a regularizer,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \min_{x} \frac{1}{N} \sum_{i=1}^{N} f_{i}(x) + \lambda g(x)
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;The intuition is that finding the optimal solution to this problem is
unnecessary as the goal is to minimize the "risk" (read: error) with respect to
a set of &lt;em&gt;samples&lt;/em&gt; from the true distribution of potential loss functions.
Thus, the following algorithms' convergence rates are for the &lt;em&gt;expected&lt;/em&gt; rate
of convergence (as opposed to the above algorithms which upper bound the &lt;em&gt;true&lt;/em&gt;
rate of convergence).&lt;/p&gt;
&lt;table class="table table-bordered table-centered"&gt;
&lt;p&gt;&lt;colgroup&gt;
    &lt;col style="width:20%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:40%"&gt;
  &lt;/colgroup&gt;&lt;/p&gt;
&lt;p&gt;&lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Algorithm          &lt;/th&gt;
      &lt;th&gt;Problem Formulation&lt;/th&gt;
      &lt;th&gt;Convex             &lt;/th&gt;
      &lt;th&gt;Strongly Convex    &lt;/th&gt;
      &lt;th&gt;Per-Iteration Cost &lt;/th&gt;
      &lt;th&gt;Notes              &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;&lt;/p&gt;
&lt;p&gt;&lt;tbody&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Stochastic Gradient Descent (SGD)&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \lambda g(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(n/\epsilon^2)$&lt;sup id="fnref-bach-2012"&gt;&lt;a class="footnote-ref" href="#fn-bach-2012"&gt;16&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(n/\epsilon)$&lt;sup id="fnref2-bach-2012"&gt;&lt;a class="footnote-ref" href="#fn-bach-2012"&gt;16&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Assumes objective is differentiable.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Stochastic Dual Coordinate Ascent (SDCA)&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \frac{\lambda}{2} \norm{x}&lt;em _in="\in" _mathbb_C="\mathbb{C" x&gt;2^2$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(\frac{1}{\lambda \epsilon})$&lt;sup id="fnref-shalevshwartz-2012"&gt;&lt;a class="footnote-ref" href="#fn-shalevshwartz-2012"&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(( \frac{1}{\lambda} ) \log ( \frac{1}{\lambda \epsilon} ))$&lt;sup id="fnref2-shalevshwartz-2012"&gt;&lt;a class="footnote-ref" href="#fn-shalevshwartz-2012"&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Accelerated Proximal Stochastic Dual Coordinate Ascent (APSDCA)&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min&lt;/em&gt;} \sum_{i} f_{i}(x) + \lambda g(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(\min (\frac{1}{\lambda \epsilon}, \sqrt{\frac{N}{\lambda \epsilon}} ))$&lt;sup id="fnref-shalevshwartz-2013"&gt;&lt;a class="footnote-ref" href="#fn-shalevshwartz-2013"&gt;18&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\min (\frac{1}{\lambda}, \sqrt{\frac{N}{\lambda}}) \log ( \frac{1}{\epsilon} ))$&lt;sup id="fnref2-shalevshwartz-2013"&gt;&lt;a class="footnote-ref" href="#fn-shalevshwartz-2013"&gt;18&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Stochastic Average Gradient (SAG)&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \lambda g(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon)$&lt;sup id="fnref-schmidt-2013"&gt;&lt;a class="footnote-ref" href="#fn-schmidt-2013"&gt;19&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1/\epsilon))$&lt;sup id="fnref2-schmidt-2013"&gt;&lt;a class="footnote-ref" href="#fn-schmidt-2013"&gt;19&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f_{i}(x)$ is differentiable.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Stochastic Variance Reduced Gradient (SVRG)&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \lambda g(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon)$&lt;sup id="fnref-johnson-2013"&gt;&lt;a class="footnote-ref" href="#fn-johnson-2013"&gt;22&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1/\epsilon))$&lt;sup id="fnref2-johnson-2013"&gt;&lt;a class="footnote-ref" href="#fn-johnson-2013"&gt;22&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f_{i}(x)$ is differentiable.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;MISO&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \lambda g(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1 / \epsilon)$&lt;sup id="fnref-mairal-2013"&gt;&lt;a class="footnote-ref" href="#fn-mairal-2013"&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1/\epsilon))$&lt;sup id="fnref2-mairal-2013"&gt;&lt;a class="footnote-ref" href="#fn-mairal-2013"&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $f_{i}(x)$ is differentiable. $g(x)$ may be used as
        a barrier function.
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;&lt;/p&gt;
&lt;/table&gt;
&lt;h1&gt;Other Methods&lt;/h1&gt;
&lt;p&gt;The following methods do not fit well into any of the preceding categories.
Included are meta-algorithms like ADMM, which are good for distributing
computation across machines, and methods whose per-iteration complexity depends
on iteration count $t$.&lt;/p&gt;
&lt;table class="table table-bordered table-centered"&gt;
&lt;p&gt;&lt;colgroup&gt;
    &lt;col style="width:20%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:10%"&gt;
    &lt;col style="width:40%"&gt;
  &lt;/colgroup&gt;&lt;/p&gt;
&lt;p&gt;&lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Algorithm          &lt;/th&gt;
      &lt;th&gt;Problem Formulation&lt;/th&gt;
      &lt;th&gt;Convex             &lt;/th&gt;
      &lt;th&gt;Strongly Convex    &lt;/th&gt;
      &lt;th&gt;Per-Iteration Cost &lt;/th&gt;
      &lt;th&gt;Notes              &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;&lt;/p&gt;
&lt;p&gt;&lt;tbody&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Alternating Direction Method of Multipliers (ADMM)&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;
        $$
          \begin{align&lt;em&gt;}
            \min_{x,z} \quad
              &amp;amp; f(x) + g(z) \
            \text{s.t.} \quad
              &amp;amp; Ax + Bz = c
          \end{align&lt;/em&gt;}
        $$
      &lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1/\epsilon)$&lt;sup id="fnref-blog-admm"&gt;&lt;a class="footnote-ref" href="#fn-blog-admm"&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1/\epsilon))$&lt;sup id="fnref-hong-2012"&gt;&lt;a class="footnote-ref" href="#fn-hong-2012"&gt;21&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(n)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        The stated convergence rate for "Strongly Convex" only requires $f(x)$ to
        be strongly convex, not $g(x)$. This same rate can also be applied to
        the "Convex" case under several non-standard assumptions&lt;sup id="fnref2-hong-2012"&gt;&lt;a class="footnote-ref" href="#fn-hong-2012"&gt;21&lt;/a&gt;&lt;/sup&gt;.
        Matrices $A$ and $B$ may also need to be full column rank&lt;sup id="fnref-deng-2012"&gt;&lt;a class="footnote-ref" href="#fn-deng-2012"&gt;20&lt;/a&gt;&lt;/sup&gt; .
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Bundle Method&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathcal{C}} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(1/\epsilon)$&lt;sup id="fnref-smola-2007"&gt;&lt;a class="footnote-ref" href="#fn-smola-2007"&gt;23&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1 / \epsilon))$&lt;sup id="fnref2-smola-2007"&gt;&lt;a class="footnote-ref" href="#fn-smola-2007"&gt;23&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;$O(tn)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;!-- Algorithm          --&gt;
      &lt;td&gt;Center of Gravity Algorithm&lt;/td&gt;
      &lt;!-- Problem            --&gt;
      &lt;td&gt;$\displaystyle \min_{x \in \mathcal{C}} f(x)$&lt;/td&gt;
      &lt;!-- Convex             --&gt;
      &lt;td&gt;$O(\log (1 / \epsilon))$&lt;sup id="fnref-ee236c-localization"&gt;&lt;a class="footnote-ref" href="#fn-ee236c-localization"&gt;24&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Strongly Convex    --&gt;
      &lt;td&gt;$O(\log (1 / \epsilon))$&lt;sup id="fnref2-ee236c-localization"&gt;&lt;a class="footnote-ref" href="#fn-ee236c-localization"&gt;24&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;!-- Per-Iteration Cost --&gt;
      &lt;td&gt;At least $O(tn)$&lt;/td&gt;
      &lt;!-- Notes              --&gt;
      &lt;td&gt;
        Applicable when $\mathcal{C}$ is bounded. Each iteration requires
        finding a near-central point in a convex set; this may be
        computationally expensive.
      &lt;/td&gt;
    &lt;/tr&gt;&lt;/p&gt;
&lt;/tbody&gt;

&lt;/table&gt;
&lt;!-- Footnotes --&gt;

&lt;!-- References --&gt;

&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-blog-gd"&gt;
&lt;p&gt;&lt;a href="/blog/gradient-descent.html"&gt;Gradient Descent&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-blog-gd" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-blog-sd"&gt;
&lt;p&gt;&lt;a href="/blog/subgradient-descent.html"&gt;Subgradient Descent&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-blog-sd" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-blog-agd"&gt;
&lt;p&gt;&lt;a href="/blog/accelerated-gradient-descent.html"&gt;Accelerated Gradient Descent&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-blog-agd" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-blog-pgd"&gt;
&lt;p&gt;&lt;a href="/blog/proximal-gradient-descent.html"&gt;Proximal Gradient Descent&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-blog-pgd" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-blog-apgd"&gt;
&lt;p&gt;&lt;a href="/blog/accelerated-proximal-gradient-descent.html"&gt;Accelerated Proximal Gradient Descent&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-blog-apgd" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-blog-fw"&gt;
&lt;p&gt;&lt;a href="/blog/frank-wolfe.html"&gt;Franke-Wolfe Algorithm&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-blog-fw" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-blog-admm"&gt;
&lt;p&gt;&lt;a href="/blog/admm-revisited.html"&gt;Alternating Direction Method of Multipliers&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-blog-admm" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-richtarik-2011"&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1107.2848"&gt;Richtarik and Takac, 2011&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-richtarik-2011" title="Jump back to footnote 8 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-richtarik-2011" title="Jump back to footnote 8 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-ee381-md"&gt;
&lt;p&gt;&lt;a href="http://users.ece.utexas.edu/~cmcaram/EE381V_2012F/Lecture_24_Scribe_Notes.final.pdf"&gt;EE381 Slides on Mirror Descent&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-ee381-md" title="Jump back to footnote 9 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-ee381-gd"&gt;
&lt;p&gt;&lt;a href="http://users.ece.utexas.edu/~cmcaram/EE381V_2012F/Lecture_4_Scribe_Notes.final.pdf"&gt;EE381 Slides on Gradient Descent&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-ee381-gd" title="Jump back to footnote 10 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-bubeck-agd"&gt;
&lt;p&gt;&lt;a href="https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/"&gt;Sebastien Bubeck's article on Accelerated Gradient Descent for Smooth and Strongly Convex objectives&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-bubeck-agd" title="Jump back to footnote 11 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-garber-2014"&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1406.1305"&gt;Garber and Hazan, 2014&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-garber-2014" title="Jump back to footnote 12 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-mairal-2013"&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1305.3120"&gt;Mairal, 2013&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-mairal-2013" title="Jump back to footnote 13 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-mairal-2013" title="Jump back to footnote 13 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3-mairal-2013" title="Jump back to footnote 13 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref4-mairal-2013" title="Jump back to footnote 13 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-ee364a-unconstrained"&gt;
&lt;p&gt;&lt;a href="http://web.stanford.edu/class/ee364a/lectures/unconstrained.pdf"&gt;EE364a Slides on Unconstrained Optimization Algorithms&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-ee364a-unconstrained" title="Jump back to footnote 14 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-ee236c-qnewton"&gt;
&lt;p&gt;&lt;a href="http://www.seas.ucla.edu/~vandenbe/236C/lectures/qnewton.pdf"&gt;EE236c Slides on Quasi-Newton Methods&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-ee236c-qnewton" title="Jump back to footnote 15 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-bach-2012"&gt;
&lt;p&gt;&lt;a href="http://www.ann.jussieu.fr/~plc/bach2012.pdf"&gt;Bach's slides on Stochastic Methods&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-bach-2012" title="Jump back to footnote 16 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-bach-2012" title="Jump back to footnote 16 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-shalevshwartz-2012"&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1209.1873"&gt;Shalev-Shwartz and Zhang, 2012&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-shalevshwartz-2012" title="Jump back to footnote 17 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-shalevshwartz-2012" title="Jump back to footnote 17 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-shalevshwartz-2013"&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1309.2375"&gt;Shalev-Shwartz and Zhang, 2013&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-shalevshwartz-2013" title="Jump back to footnote 18 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-shalevshwartz-2013" title="Jump back to footnote 18 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-schmidt-2013"&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1309.2388"&gt;Schmidt et al, 2013&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-schmidt-2013" title="Jump back to footnote 19 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-schmidt-2013" title="Jump back to footnote 19 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-deng-2012"&gt;
&lt;p&gt;&lt;a href="ftp://ftp.math.ucla.edu/pub/camreport/cam12-52.pdf"&gt;Deng and Yin, 2012&lt;/a&gt;, Table 1.1&amp;#160;&lt;a class="footnote-backref" href="#fnref-deng-2012" title="Jump back to footnote 20 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-hong-2012"&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1208.3922"&gt;Hong and Luo, 2012&lt;/a&gt;, Section 2&amp;#160;&lt;a class="footnote-backref" href="#fnref-hong-2012" title="Jump back to footnote 21 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-hong-2012" title="Jump back to footnote 21 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-johnson-2013"&gt;
&lt;p&gt;&lt;a href="http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf"&gt;Johnson and Zhang, 2013&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-johnson-2013" title="Jump back to footnote 22 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-johnson-2013" title="Jump back to footnote 22 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-smola-2007"&gt;
&lt;p&gt;&lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2007_470.pdf"&gt;Smola and Zhang, 2007&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-smola-2007" title="Jump back to footnote 23 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-smola-2007" title="Jump back to footnote 23 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-ee236c-localization"&gt;
&lt;p&gt;&lt;a href="http://www.seas.ucla.edu/~vandenbe/236C/lectures/localization.pdf"&gt;EE236c Slides on Cutting Planes&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-ee236c-localization" title="Jump back to footnote 24 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2-ee236c-localization" title="Jump back to footnote 24 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-nesterov-2007"&gt;
&lt;p&gt;&lt;a href="http://ium.mccme.ru/postscript/s12/GS-Nesterov%20Primal-dual.pdf"&gt;Nesterov, 2007&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-nesterov-2007" title="Jump back to footnote 25 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-nedich-2013"&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1307.1879"&gt;Nedich and Lee, 2013&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref-nedich-2013" title="Jump back to footnote 26 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="optimization"></category><category term="convergence"></category><category term="reference"></category></entry><entry><title>From ADMM to Proximal Gradient Descent</title><link href="/blog/admm-to-prox-grad.html" rel="alternate"></link><published>2014-07-26T00:00:00-07:00</published><updated>2014-07-26T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2014-07-26:/blog/admm-to-prox-grad.html</id><summary type="html">&lt;p&gt;At first blush, &lt;a href="/blog/admm.html"&gt;ADMM&lt;/a&gt; and &lt;a href="/blog/proximal-gradient-descent.html"&gt;Proximal Gradient Descent&lt;/a&gt;
(ProxGrad) appear to have very little in common. The convergence analyses for
these two methods are unrelated, and the former operates on an Augmented
Lagrangian while the latter directly minimizes the primal objective. In this
post, we'll show that after a slight …&lt;/p&gt;</summary><content type="html">&lt;p&gt;At first blush, &lt;a href="/blog/admm.html"&gt;ADMM&lt;/a&gt; and &lt;a href="/blog/proximal-gradient-descent.html"&gt;Proximal Gradient Descent&lt;/a&gt;
(ProxGrad) appear to have very little in common. The convergence analyses for
these two methods are unrelated, and the former operates on an Augmented
Lagrangian while the latter directly minimizes the primal objective. In this
post, we'll show that after a slight modification to ADMM, we recover the
proximal gradient algorithm applied to Lagrangian &lt;em&gt;dual&lt;/em&gt; of the ADMM objective.&lt;/p&gt;
&lt;p&gt;To be precise, we'll first make a slight modification to ADMM to construct
another algorithm known as the &lt;a href="http://dspace.mit.edu/bitstream/handle/1721.1/3103/P-1836-19477130.pdf"&gt;Alternating Minimization Algorithm&lt;/a&gt; (AMA).
We'll then show this algorithm is an instance of a more general technique for
&lt;a href="http://supernet.isenberg.umass.edu/austria_lectures/fvisli.pdf"&gt;Variational Inequality problems&lt;/a&gt; called
&lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/jmlr10_duchi09a.pdf"&gt;Forward-Backward Splitting&lt;/a&gt; (FOBOS). Finally, we'll show that ProxGrad
is also an instance of FOBOS with the exact same form. We conclude that these
two algorithms are equivalent.&lt;/p&gt;
&lt;h1&gt;&lt;a name="ama" href="#ama"&gt;Alternating Minimization Algorithm&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The &lt;a href="http://dspace.mit.edu/bitstream/handle/1721.1/3103/P-1836-19477130.pdf"&gt;Alternating Minimization Algorithm&lt;/a&gt; (AMA), originally proposed by
Paul Tseng in 1988, is an algorithm very similar to ADMM. In fact, the only
difference between these two methods is in the first step of each iteration.
Recall the pseudocode for ADMM; whereas ADMM minimizes the &lt;em&gt;Augmented&lt;/em&gt;
Lagrangian with respect to $x$, AMA minimizes the &lt;em&gt;Non-Augmented&lt;/em&gt; Lagrangian,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt; Step size $\rho$, initial primal iterates $x^{(0)}$ and $z^{(0)}$,
            initial dual iterate $y^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $t = 0, 1, \ldots$&lt;ol&gt;
&lt;li&gt;Let $x^{(t+1)} = \underset{x}{\text{argmin}} \quad L_{   0}( x        , z^{(t)}, y^{(t)} )$&lt;/li&gt;
&lt;li&gt;Let $z^{(t+1)} = \underset{z}{\text{argmin}} \quad L_{\rho}( x^{(t+1)}, z      , y^{(t)} )$&lt;/li&gt;
&lt;li&gt;Let $y^{(t+1)} = y^{(t)} + \rho ( Ax^{(t+1)} + Bz^{(t+1)} - c )$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;Notice the $0$ instead of $\rho$ in the definition of $x^{(t+1)}$. This tiny
change, we'll see, is all that's necessary to turn ADMM into ProxGrad.&lt;/p&gt;
&lt;h1&gt;&lt;a name="vi" href="#vi"&gt;Variational Inequalties&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;To show the similarity between AMA and ProxGrad, we'll show that both
algorithms are instances of Forward-Backward Splitting (FOBOS). Unlike other
algorithms we've considered, FOBOS isn't about minimizing a real-valued
objective function subject to constraints. Instead, FOBOS solves Variational
Inequality problems, which we'll now describe.&lt;/p&gt;
&lt;p&gt;Variational Inequality (VI) problems involve a vector-to-vector function
$F: \mathbb{R}^n \rightarrow \mathbb{R}^n$ and a convex set $\mathcal{C}$. The
goal is to find an input $w^{*}$ such that,&lt;/p&gt;
&lt;p&gt;$$
\begin{equation&lt;em&gt;}
  \forall w \in \mathcal{C} \quad
  \langle F(w^{&lt;/em&gt;}), w - w^{&lt;em&gt;} \rangle \ge 0
\end{equation&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;If $\mathcal{C} = \mathcal{R}^n$, then this inequality can only hold when
$F(w^{&lt;em&gt;}) = 0$. For example, if $F = \nabla f$ for a differentiable convex
objective function $f$, then finding $F(w^{&lt;/em&gt;}) = 0$ is the same as a finding
$f$'s unconstrained global minimum. Incorporating constraints is as simple as
letting $F(w) = [\nabla_x L(x,y); -\nabla_y L(x,y)]$ for Lagrangian $L(x,y)$
with primal variable $x$ and dual variable $y$ and $w = [x; y]$.&lt;/p&gt;
&lt;p&gt;What is not covered in this setup, however, is the case when $L$ is not
differentiable with respect to all parameters. We can expand on the concept of
Variational Inequalties a bit by letting $F(w)$ be a &lt;em&gt;subset&lt;/em&gt; of
$\mathcal{R}^{n}$ instead of a single value (that is,
$F: \mathcal{R}^n \rightarrow 2^{\mathcal{R}^{n}}$). We'll say that $F$ is
a &lt;em&gt;monotone operator&lt;/em&gt; if,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \forall w,w' \in \mathcal{C}; \,
  \forall u \in F(w);           \,
  \forall v \in F(w')           \quad
  \langle u-v, w-w' \rangle \ge 0
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Now if $\mathcal{C} = \mathcal{R}^n$ and
$F = [\partial_x L(x,y); -\partial_y L(x,y)]$, we can see that finding
$0 \in F(w^{*})$ is the same as solving the optimization described by $L$ for
non-smooth objective and constraint functions.&lt;/p&gt;
&lt;h1&gt;&lt;a name="fobos" href="#fobos"&gt;Forward-Backward Splitting&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/jmlr10_duchi09a.pdf"&gt;Forward-Backward Splitting&lt;/a&gt; FOBOS is an algorithm for finding
a $w^{*}$ that solves VI problems for particular choices of $F$. Namely,
we'll make the following assumptions.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$F(w) = \Psi(w) + \Theta(w)$ for &lt;a href="http://web.stanford.edu/class/ee364b/lectures/monotone_slides.pdf"&gt;monotone operators&lt;/a&gt; $\Psi$ and
  $\Theta$.&lt;/li&gt;
&lt;li&gt;$\Psi(w)$ has exactly one value for each $w$ in its domain.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given this, FOBOS will converge to a $w^{&lt;em&gt;}$ such that $0 \in F(w^{&lt;/em&gt;})$. The
algorithm itself is,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt; Step sizes ${ \rho_t }_{t=1}^{\infty}$, initial iterate $w^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $t = 0, 1, \ldots$&lt;ol&gt;
&lt;li&gt;Let $w^{(t+1/2)} = w^{(t)} - \rho_t \Psi(w^{(t)})$&lt;/li&gt;
&lt;li&gt;Let $w^{(t+1)}$ be such that $w^{(t+1)} + \rho_t \Theta(w^{(t+1)}) = w^{(t+1/2)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;An equivalent, more concise way to describe FOBOS is with
$w^{(t+1)} = (I + \rho_t \Theta)^{-1} (I - \rho_t \Psi) (w^{(t)})$. With this
formulation in mind, we'll now show that both AMA and ProxGrad are instances of
FOBOS performing the same set of operations.&lt;/p&gt;
&lt;h1&gt;&lt;a name="reductions" href="#reductions"&gt;Reductions to FOBOS&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;We'll now show that for the specific optimization problem tackled by ADMM,
AMA is the same as Proximal Gradient Descent on the dual problem. First, recall
the problem ADMM is solving,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\begin{split}
  \underset{x,z}{\text{minimize}} \qquad
    &amp;amp; f(x) + g(z) \
  \text{s.t.}                     \qquad
    &amp;amp; Ax + Bz = c \
\end{split} \label{eqn:primal}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;The dual problem to this is then,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\begin{split}
  - \underset{y}{\text{minimize}} \qquad
    &amp;amp; f^{&lt;em&gt;}(A^{T} y) + g^{&lt;/em&gt;}(B^{T} z) - \langle y, c \rangle \
\end{split} \label{eqn:dual}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;where $f^{&lt;em&gt;}$ and $g^{&lt;/em&gt;}$ are the &lt;a href="http://en.wikipedia.org/wiki/Convex_conjugate"&gt;convex conjugates&lt;/a&gt; to
$f$ and $g$, respectively. We'll now show that both AMA and Proximal Gradient
Descent are optimizing this same dual.&lt;/p&gt;
&lt;h2&gt;&lt;a name="prox-grad-to-fobos" href="#prox-grad-to-fobos"&gt;Proximal Gradient Descent to FOBOS&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Suppose we want to minimize $f^{&lt;em&gt;}(A^T y) + g^{&lt;/em&gt;}(B^T y)$. If the problem is
unconstrained, this is equivalent to finding&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0 \in F(y)
  &amp;amp;= \partial_y \left( f^{&lt;/em&gt;}(A^T y) + g(B^T y) - \langle y, c \rangle \right) \
  &amp;amp;= A (\nabla_y f^{&lt;em&gt;})(A^T y) + B (\partial_y g^{&lt;/em&gt;})(B^T y) - c
  \end{align*}
$$&lt;/p&gt;
&lt;p&gt;Let's now define,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  \Psi(y)   &amp;amp;= A (\nabla_y   f^{&lt;em&gt;})(A^T y)     &amp;amp;
  \Theta(y) &amp;amp;= B (\partial_y g^{&lt;/em&gt;})(B^T y) - c
  \label{eqn:fobos-def}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Clearly, $(I - \rho_{t} \Psi)(y) = y - \rho_{t} A (\nabla_y f^{*})(A^T y)$
matches the first part of FOBOS and the "gradient step" part of ProxGrad, but
we also need to show that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \text{prox}_{\rho_t g^{&lt;/em&gt;}(B^T \cdot) - \langle \cdot, c \rangle}(y)
  &amp;amp; = (I + \rho_{t} \Theta)^{-1}(y)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;To do this, let's recall the definition of the prox operator,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \bar{y}
  &amp;amp; = \text{prox}_{\rho_t g^{&lt;/em&gt;}(B^T \cdot) - \langle \cdot, c \rangle}(y) \
  &amp;amp; = \argmin_{y'}  g^{&lt;em&gt;}(B^T y')
                    - \langle y', c \rangle
                    + \frac{1}{2\rho_t}\norm{y'-y}_2^2
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Since this is an unconstrained minimization problem, we know that $0$ must be
in the subgradient of this expression at $\bar{y}$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0
  &amp;amp; \in B (\partial_{\bar{y}} g^{&lt;/em&gt;})(B^T \bar{y}) - c + \frac{1}{\rho_t} (\bar{y}-y)  \
  y
  &amp;amp; \in \bar{y} + \rho_t \left( B (\partial_{\bar{y}} g^{&lt;em&gt;})(B^T \bar{y}) - c \right) \
  &amp;amp; = (I + \rho_t \Theta)(\bar{y})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Apply $(I + \rho_t \Theta)^{-1}$ to both sides gives us the desired result,
We now have that for the above choices of $\Psi$ and $\Theta$, ProxGrad can
be reframed as identical to FOBOS,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  y^{(t+1)} = (I + \rho_t \Theta)^{-1} (I - \rho_t \Psi) (y^{(t)})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;h2&gt;&lt;a name="ama-to-fobos" href="#ama-to-fobos"&gt;AMA to FOBOS&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We'll now show that AMA as applied to the ADMM objective is
simply an instance of FOBOS. Similar to the &lt;a href="#prox-grad-to-fobos"&gt;ProxGrad
reduction&lt;/a&gt;, we'll use the following definitions for
$\Psi$ and $\Theta$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \Psi(y)   &amp;amp;= A (\nabla   f^{&lt;/em&gt;})(A^T y)      &amp;amp;
  \Theta(y) &amp;amp;= B (\partial g^{&lt;em&gt;})(B^T y) - c
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;First, recall the subgradient optimality condition as applied to Step B of
ADMM (same as AMA). In particular, for $z^{(t+1)}$ to be the argmin of
$L(x^{(t+1)}, z, y^{(t)})$, it must be the case that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0
  &amp;amp;\in \partial g(z^{(t+1)}) - B^T y^{(t)} - \rho B^T (c - Ax^{(t+1)} - Bz^{(t+1)}) \
  B^T ( y^{(t)} + \rho (c - Ax^{(t+1)} - Bz^{(t+1)}) )
  &amp;amp;\in \partial g(z^{(t+1)})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Using $y \in \partial f(x) \Rightarrow x \in \partial f^{*}(y)$, we obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  z^{(t+1)}
  &amp;amp; \in \partial g^{&lt;/em&gt;}(B^T ( y^{(t)} + \rho (c - Ax^{(t+1)} - Bz^{(t+1)}) ))
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;We now left-multiply by $B$, subtract $c$ from both sides to obtain, and use
the definition of $\Theta$ to obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  B z^{(t+1)} - c
  &amp;amp; \in \Theta( y^{(t)} + \rho (c - Ax^{(t+1)} - Bz^{(t+1)}) )
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Now we multiply both sides by $\rho$ and add,
$y^{(t)} + \rho (c - Ax^{(t+1)} - Bz^{(t+1)})$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  y^{(t)} - \rho Ax^{(t+1)}
  &amp;amp; \in (I + \rho \Theta)( y^{(t)} + \rho (c - Ax^{(t+1)} - Bz^{(t+1)}) )
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;We can invert $I + \rho \Theta$ and notice that the other side is
single-valued to obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  (I + \rho \Theta)^{-1} (y^{(t)} - \rho Ax^{(t+1)})
  &amp;amp; = y^{(t)} + \rho (c - Ax^{(t+1)} - Bz^{(t+1)})   \notag \
  (I + \rho \Theta)^{-1} (y^{(t)} - \rho Ax^{(t+1)})
  &amp;amp; = y^{(t+1)}                                                 \label{eqn:ama1} \
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Now, let's apply the same subgradient optimality to Step A of AMA.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0
  &amp;amp;\in \partial f(x^{(t+1)}) - A^T y^{(t)} \
  A^T y^{(t)}
  &amp;amp;= \nabla f(x^{(t+1)})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Using $y = \nabla f(x) \Rightarrow x = \nabla f^{*}(y)$ for strongly convex
$f$ and multiplying both sides by $A$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  A f^{*} (A^T y^{(t)}) &amp;amp;= A f(x^{(t+1)}) \notag            \
  \Psi(y^{(t)})         &amp;amp;= A x^{(t+1)}    \label{eqn:ama2}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Substituting in equation $\ref{eqn:ama2}$ into $\ref{eqn:ama1}$, we obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  y^{(t+1)} = (I + \rho \Theta)^{-1} (I - \rho \Psi) (y^{(t)})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Notice that this is exactly the same thing we concluded in the reduction from
ProxGrad to FOBOS. Thus, we have shown that both AMA and ProxGrad are the same
algorithm for the ADMM objective.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Proximal Gradient Descent and ADMM&lt;/strong&gt; I was first made aware of the
relationship between AMA and ADMM in &lt;a href="http://arxiv.org/abs/1304.0499"&gt;Chi&lt;/a&gt;'s article on
convex clustering via ADMM and AMA. The relationship between Proximal Gradient
Descent and FoBoS is taken from &lt;a href="http://www.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf"&gt;Berkeley's EE227a slides&lt;/a&gt; and
the relationship between FoBoS and AMA from &lt;a href="ftp://ftp.math.ucla.edu/pub/camreport/cam12-35.pdf"&gt;Goldstein et
al&lt;/a&gt;'s work on Accelerated ADMM and AMA.&lt;/p&gt;
&lt;!-- internal references --&gt;

&lt;!-- papers --&gt;</content><category term="optimization"></category><category term="fobos"></category><category term="admm"></category><category term="ama"></category><category term="proximal"></category></entry><entry><title>ADMM revisited</title><link href="/blog/admm-revisited.html" rel="alternate"></link><published>2014-07-20T00:00:00-07:00</published><updated>2014-07-20T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2014-07-20:/blog/admm-revisited.html</id><summary type="html">&lt;p&gt;When I originally wrote about the &lt;a href="/blog/admm.html"&gt;Alternating Direction Method of
Multipliers&lt;/a&gt; algorithm, the community's understanding of its
convergence properties was light to say the least. While it has long been
known (See &lt;a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf"&gt;Boyd's excellent article&lt;/a&gt;, Appendix A) that ADMM &lt;em&gt;will&lt;/em&gt;
converge, it is only recently that the community has begun …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When I originally wrote about the &lt;a href="/blog/admm.html"&gt;Alternating Direction Method of
Multipliers&lt;/a&gt; algorithm, the community's understanding of its
convergence properties was light to say the least. While it has long been
known (See &lt;a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf"&gt;Boyd's excellent article&lt;/a&gt;, Appendix A) that ADMM &lt;em&gt;will&lt;/em&gt;
converge, it is only recently that the community has begun to establish &lt;em&gt;how
fast&lt;/em&gt; it converges (e.g. &lt;a href="http://arxiv.org/abs/1208.3922"&gt;Hong&lt;/a&gt;, &lt;a href="ftp://ftp.math.ucla.edu/pub/camreport/cam12-52.pdf"&gt;Deng&lt;/a&gt;, &lt;a href="http://iqua.ece.toronto.edu/~cfeng/notes/cfeng-admm12.pdf"&gt;Feng&lt;/a&gt;, &lt;a href="http://www.math.hkbu.edu.hk/~xmyuan/Paper/HeYuan-SecondRevision.pdf"&gt;He&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In this article, we'll explore one way to establish an $O(1 / \epsilon)$ rate
of convergence. Unlike previous convergence proofs presented in this blog, we
won't directly show that the primal objective value alone converges to its
optimal value; instead, we'll show that a particular function involving the
primal objective and a &lt;a href="http://supernet.isenberg.umass.edu/austria_lectures/fvisli.pdf"&gt;Variational Inequality&lt;/a&gt;
converges at the desired rate.&lt;/p&gt;
&lt;h1&gt;&lt;a name="implementation" href="#implementation"&gt;How does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Let's begin by introducing the optimization problem ADMM solves,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\begin{split}
  \underset{x,z}{\text{minimize}} \qquad
    &amp;amp; f(x) + g(z) \
  \text{s.t.}                     \qquad
    &amp;amp; Ax + Bz = c \
\end{split} \label{eqn:objective}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;This problem is characterized by 2 primal variables, $x$ and $z$, which are
related by a linear equation. In machine learning, a common scenario is to
choose $A$, $B$, and $c$ such that $x = z$, making the setup particularly
simple. For the rest of this article, we'll assume that $Ax + Bz = c$ is the
only constraint we consider -- other constraints can be incorporated into
$f$ and $g$ by letting them be infinite when constraints are broken.&lt;/p&gt;
&lt;p&gt;The ADMM algorithm then finds the "saddle point" of the Augmented
Lagrangian for the corresponding problem,&lt;/p&gt;
&lt;p&gt;$$
\begin{align} \label{eqn:lagrangian}
  L_{\rho}(x, z, y) = f(x) + g(z) + \langle y, Ax + Bz - c \rangle
                      + \frac{\rho}{2} || Ax + Bz - c ||_2^2
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Note that we say &lt;em&gt;Augmented&lt;/em&gt; Lagrangian, as the typical Lagrangian does not
include the final quadratic term. It's easy to see, however, that the quadratic
does not affect the problem's optimal solution, as the constraint $Ax + Bz = c$
holds for all valid solutions.&lt;/p&gt;
&lt;p&gt;The ADMM algorithm iteratively minimizes $L_{\rho}$ with respect to $x$ for
fixed $z$ and $y$, then minimizes $z$ for fixed $x$ and $y$, and finally
takes a gradient step with respect to $y$ for fixed $x$ and $z$.&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt; Step size $\rho$, initial primal iterates $x^{(0)}$ and $z^{(0)}$,
            initial dual iterate $y^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $t = 0, 1, \ldots$&lt;ol&gt;
&lt;li&gt;Let $x^{(t+1)} = \underset{x}{\text{argmin}} \quad L_{\rho}( x        , z^{(t)}, y^{(t)} )$&lt;/li&gt;
&lt;li&gt;Let $z^{(t+1)} = \underset{z}{\text{argmin}} \quad L_{\rho}( x^{(t+1)}, z      , y^{(t)} )$&lt;/li&gt;
&lt;li&gt;Let $y^{(t+1)} = y^{(t)} + \rho ( Ax^{(t+1)} + Bz^{(t+1)} - c )$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;Intuitively, the extra quadratic term prevents each iteration of the
algorithm from stepping "too far" from the last iteration, an idea that's also
at the core of &lt;a href="/blog/proximal-gradient-descent.html"&gt;Proximal Gradient Descent&lt;/a&gt;.&lt;/p&gt;
&lt;div class="img-center"&gt;
&lt;p&gt;&lt;img src="/assets/img/admm/convergence.gif"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    Animation of $x_t$ and $z_t$ converging to the minimum of the sum of
    2 quadratics.
  &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the remainder of the article, we'll often use the following notation for
conciseness,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  w    &amp;amp;= \begin{bmatrix}
            x \
            z \
            y
          \end{bmatrix} \
  h(w) &amp;amp;= f(x) + g(z) \
  F(w) &amp;amp;= \begin{bmatrix}
            A^T y \
            B^T y \
            - (Ax + Bz - c)
          \end{bmatrix}
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;h1&gt;&lt;a name="proof" href="#proof"&gt;Why does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Unlike other convergence proofs presented on this website, we won't directly
show that the objective converges to its minimum as $t \rightarrow \infty$.
Indeed, limiting ourselves to analysis of the objective completely ignores the
constraint $Ax + Bz = c$. Instead, we'll use the following variational
inequality condition to describe an optimal solution. In particular, a solution
$w^{*}$ is optimal if,&lt;/p&gt;
&lt;p&gt;$$
\begin{align} \label{vi}
  \forall w \in \mathbb{R}^{n} \qquad
    h(w) - h(w^{&lt;em&gt;}) + \langle w - w^{&lt;/em&gt;}, F(w^{*}) \rangle &amp;amp;\ge 0
\end{align}
$$&lt;/p&gt;
&lt;div class="img-center"&gt;
&lt;p&gt;&lt;img src="/assets/img/admm/vi.jpg"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    Geometric interpretation of a the optimality condition for a variational
    inequality when ignoring $h(w)$ from &lt;a href="http://supernet.isenberg.umass.edu/austria_lectures/fvisli.pdf"&gt;Anna Nagurney&lt;/a&gt;.
  &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For the following proof, we'll replace $w^{*}$ with
$\bar{w}&lt;em _tau="1"&gt;t = (1/t) \sum&lt;/em&gt;^{t} w_{\tau}$ and $0$ on the right hand side
with $-\epsilon_t$ where $\epsilon_t = O(1/t)$. By showing that we can
approximately satisfy this inequality at a rate $O(1/t)$, we establish the
desired convergence rate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The assumptions on ADMM are almost as light as we can imagine. This is
largely due to the fact that we needn't use gradients or subgradients for
$h(z)$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f(x) + g(z)$ is convex.&lt;/li&gt;
&lt;li&gt;There exists a solution $[ x^{&lt;em&gt;}; z^{&lt;/em&gt;} ]$ that minimizes $f(x) + g(z)$
  while respecting the constraint $Ax + Bz = c$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Proof Outline&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The proof presented hereafter is a particularly simple if unintuitive one.
Theoretically, the only tools necessary are the linear lower bound definition
of a convex function, the subgradient condition for optimality in an
unconstrained optimization problem, and Jensen's Inequality. Steps 1 and
2 below rely purely on the first 2 of these tools. Step 3 merely massages
a preceding equation into a simpler form via completing squares. Step 4 closes
by exploiting a telescoping sum and Jensen's Inequality to obtain the desired
result,&lt;/p&gt;
&lt;p&gt;$$
\forall w \qquad
h(\bar{w}_t) - h(w) + \langle
  F(\bar{w}^{(t)}),
  \bar{w}^{(t)} - w
\rangle
\le \frac{1}{t} \left(
  \frac{\rho}{2} \norm{Ax-c}_2^2 + \frac{1}{2\rho} \norm{y}_2^2
\right)
$$&lt;/p&gt;
&lt;p&gt;As $t \rightarrow \infty$, the right hand side of this equation goes to 0,
rendering the same statement as the variational inequality optimality condition
in Equation $\ref{vi}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; Optimality conditions for Step A. In this portion of the proof,
we'll use the fact that $x^{(t+1)}$ is defined as the solution of an
optimization problem to derive a subgradient for $f$ at $x^{(t+1)}$. We'll then
substitute this into $f$'s definition of convexity. Finally, terms are
rearranged and the contents of Step C of the algorithm are used to derive
a final expression.&lt;/p&gt;
&lt;p&gt;We begin by recognizing that $x^{(t+1)}$ minimizes
$L_{\rho}(x, z^{(t)}, y^{(t)})$ as a function of $x$. As $x$ is unconstrained,
zero must be a valid subgradient for $L_{\rho}$ evaluated at $x^{(t+1)},
z^{(t)}, y^{(t)}$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0
  &amp;amp;\in \partial_x L_{\rho}(x^{(t+1)}, z^{(t)}, y^{(t)})                               \
  &amp;amp;= \partial_{x} f(x^{(t+1)}) + A^T y^{(t)} + \rho A^T (Ax^{(t+1)} + Bz^{(t)} - c)   \
  - A^T \left( y^{(t)} + \rho (Ax^{(t+1)} + Bz^{(t)} - c) \right)
  &amp;amp;\in \partial_x f(x^{(t+1)})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;As $f$ is convex, we further know that it is lower bounded by its linear
approximation everywhere,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \forall x \qquad
    f(x) &amp;amp;\ge f(x^{(t+1)}) + \langle
      \partial_x f(x^{(t+1)}),
      x - x^{(t+1)}
    \rangle
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Substituting in our subgradient for $\partial_x f(x^{(t+1)})$ and subtracting
the contents of the right hand side from both sides, we obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \forall x \qquad
    0 &amp;amp;\le f(x) - f(x^{(t+1)}) + \langle
      A^T (y^{(t)} + \rho (A x^{(t+1)} + B z^{(t)} - c ),
      x - x^{(t+1)}
    \rangle
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Now recall Step C of the algorithm:
$y^{(t+1)} = y^{(t)} + \rho (A x^{(t+1)} + Bz^{(t+1)} - c)$. The left side of
the inner product looks very similar to this, so we'll substitute it in as best
we can,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \forall x \qquad
    0 &amp;amp;\le f(x) - f(x^{(t+1)}) + \langle
      A^T (y^{(t+1)} + \rho Bz^{(t)} - \rho Bz^{(t+1)}),
      x - x^{(t+1)}
    \rangle
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;We finish by moving everything &lt;em&gt;not&lt;/em&gt; multiplied by $\rho$ to the opposite
side of the inequality,&lt;/p&gt;
&lt;p&gt;$$
\begin{align} \label{eqn:36}
  \forall x \qquad
    f(x^{(t+1)}) - f(x) + \langle
      x^{(t+1)} - x,
      A^T y^{(t+1)}
    \rangle
    &amp;amp;\le \rho \langle
      Bz^{(t)} - Bz^{(t+1)},
      A x - A x^{(t+1)}
    \rangle
\end{align}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Optimality conditions for Step B. Similar to Step 1, we'll use the
fact that $z^{(t+1)}$ is the solution to an unconstrained optimization problem
and will substitute in Step C's definition for $y^{(t+1)}$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0
  &amp;amp;\in \partial_z L(x^{(t+1)}, z, y^{(t)})                                          \
  &amp;amp;= \partial_z g(z^{(t+1)}) + B^T y^{(t)} + \rho B^T (Ax^{(t+1)} + Bz^{(t+1)} - c) \
  - B^T \left( y^{(t)} + \rho (Ax^{(t+1)} + Bz^{(t+1)} - c) \right)
  &amp;amp;\in \partial_z g(z^{(t+1)})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;As $g$ is convex, it is lower bounded by its linear approximation,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \forall z \qquad
    g(z) &amp;amp;\ge g(z^{(t+1)}) + \langle
      \partial_z g(z^{(t+1)}),
      z - z^{(t+1)}
    \rangle
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Substituting in the previously derived subgradient and moving all terms to
the left side, we obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \forall z \qquad
    0 &amp;amp;\le g(z) - g(z^{(t+1)}) + \langle
      B^T (y^{(t)} + \rho (A x^{(t+1)} + B z^{(t+1)} - c )),
      z - z^{(t+1)}
    \rangle
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Substituting in Step C's definition for $y^{(t+1)}$ again and moving
everything to the opposite side of the inequality, we conclude that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align} \label{eqn:37}
  \forall z \qquad
    g(z^{(t+1)}) - g(z) + \langle
      B^T y^{(t+1)},
      z^{(t+1)} - z
    \rangle
    &amp;amp;\le 0
\end{align}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; We now sum Equation $\ref{eqn:36}$ with Equation $\ref{eqn:37}$.
We'll end up with an expression that is not easy to understand initially, but
by factoring several of its terms into quadratic forms and substituting them
back in, we obtain a simpler expression that can be described as a sum of
squared 2-norms.&lt;/p&gt;
&lt;p&gt;We begin by summing equations $\ref{eqn:36}$ and $\ref{eqn:37}$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; f(x^{(t+1)}) + g(z^{(t+1)}) - f(x) - g(z) + \langle
    B^T y^{(t+1)},
    z^{(t+1)} - z
  \rangle + \langle
    A^T y^{(t+1)},
    x^{(t+1)} - x
  \rangle \
  &amp;amp; \qquad \le \rho \langle
    Ax - Ax^{(t+1)},
    Bz^{(t)} - Bz^{(t+1)}
  \rangle
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Next, we use the definitions of $h(w)$ and $F(w)$ on the left hand side,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; h(w^{(t+1)}) - h(w) + \langle
    F(w^{(t+1)}),
    w^{(t+1)} - w
  \rangle + \langle
    Ax^{(t+1)} + Bz^{(t+1)} - c,
    y^{(t+1)} - y
  \rangle \
  &amp;amp; \qquad \le \rho \langle
    Ax - Ax^{(t+1)},
    Bz^{(t)} - Bz^{(t+1)}
  \rangle
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Then, moving the last term on the left side of the inequality over and
observing that Step C implies
$(1/\rho) (y^{(t+1)} - y^{(t)}) = Ax^{(t+1)} + Bz^{(t+1)} - c$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\begin{split}
  &amp;amp; h(w^{(t+1)}) - h(w) + \langle
    F(w^{(t+1)}),
    w^{(t+1)} - w
  \rangle  \
  &amp;amp; \qquad \le \rho \langle
    Ax - Ax^{(t+1)},
    Bz^{(t)} - Bz^{(t+1)}
  \rangle + \frac{1}{\rho} \langle
    y^{(t+1)} - y^{(t)},
    y - y^{(t+1)}
  \rangle
\end{split} \label{eqn:38}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;We will now tackle the two components on the right hand side of the
inequality in isolation. Our goal is to rewrite these inner products in terms
of sums of $\norm{\cdot}_2^2$ terms.&lt;/p&gt;
&lt;p&gt;We'll start with $\langle Ax - Ax^{(t+1)}, Bz^{(t)} - Bz^{(t+1)} \rangle$. In
the next equations, we'll add many terms that will cancel themselves out, then
we'll group them together into a sum of 4 terms,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  &amp;amp;
  \begin{split}
    2 \langle Ax - Ax^{(t+1)}, Bz^{(t)} - Bz^{(t+1)} \rangle
  \end{split} \notag \
  &amp;amp;
  \begin{split}
  = &amp;amp; + \norm{Ax        -c}_2^2 &amp;amp; + 2 \langle Ax         - c, B z^{(t  )} \rangle &amp;amp; + \norm{Bz^{(t  )}}_2^2 \
    &amp;amp; - \norm{Ax        -c}_2^2 &amp;amp; - 2 \langle Ax         - c, B z^{(t+1)} \rangle &amp;amp; - \norm{Bz^{(t+1)}}_2^2 \
    &amp;amp; + \norm{Ax^{(t+1)}-c}_2^2 &amp;amp; + 2 \langle Ax^{(t+1)} - c, B z^{(t+1)} \rangle &amp;amp; + \norm{Bz^{(t+1)}}_2^2 \
    &amp;amp; - \norm{Ax^{(t+1)}-c}_2^2 &amp;amp; - 2 \langle Ax^{(t+1)} - c, B z^{(t  )} \rangle &amp;amp; - \norm{Bz^{(t  )}}_2^2
  \end{split} \notag \
  &amp;amp;
  \begin{split}
  = &amp;amp; + \norm{Ax         + Bz^{(t)}   - c}_2^2 &amp;amp; - \norm{Ax         + Bz^{(t+1)} - c}_2^2 \
    &amp;amp; + \norm{Ax^{(t+1)} + Bz^{(t+1)} - c}_2^2 &amp;amp; - \norm{Ax^{(t+1)} + Bz^{(t  )} - c}_2^2
  \end{split} \label{eqn:39}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;We'll do the same for $\langle y^{(t+1)} - y^{(t)}, y - y^{(t+1)} \rangle$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  &amp;amp;
  \begin{split}
    2 \langle y^{(t+1)} - y^{(t)}, y - y^{(t+1)} \rangle
  \end{split} \notag \
  &amp;amp;
  \begin{split}
  = &amp;amp; + \norm{y      }_2^2 &amp;amp; + 2 \langle y      , - y^{(t  )} \rangle &amp;amp; + \norm{y^{(t  )}}_2^2 \
    &amp;amp; - \norm{y      }_2^2 &amp;amp; - 2 \langle y      , - y^{(t+1)} \rangle &amp;amp; - \norm{y^{(t+1)}}_2^2 \
    &amp;amp; - \norm{y^{(t)}}_2^2 &amp;amp; - 2 \langle y^{(t)}, - y^{(t+1)} \rangle &amp;amp; - \norm{y^{(t+1)}}_2^2 \
  \end{split} \notag \
  &amp;amp;
  \begin{split}
  = &amp;amp; + \norm{y       - y^{(t  )}}_2^2
      - \norm{y       - y^{(t+1)}}_2^2
      - \norm{y^{(t)} - y^{(t+1)}}_2^2
  \end{split} \label{eqn:40}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Finally, let's plug equations $\ref{eqn:39}$ and $\ref{eqn:40}$ into
$\ref{eqn:38}$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
\begin{split}
  &amp;amp; h(w^{(t+1)}) - h(w) + \langle
    F(w^{(t+1)}),
    w^{(t+1)} - w
  \rangle  \
  &amp;amp; \qquad \le \frac{\rho}{2} \left( \begin{split}
    &amp;amp; + \norm{Ax         + Bz^{(t  )} - c}_2^2 &amp;amp; - \norm{Ax         + Bz^{(t+1)} - c}_2^2 \
    &amp;amp; + \norm{Ax^{(t+1)} + Bz^{(t+1)} - c}_2^2 &amp;amp; - \norm{Ax^{(t+1)} + Bz^{(t  )} - c}_2^2
  \end{split} \right) \
  &amp;amp; \qquad + \frac{1}{2\rho} \left( \begin{split}
    &amp;amp; + \norm{y       - y^{(t  )}}_2^2 \
    &amp;amp; - \norm{y       - y^{(t+1)}}_2^2 \
    &amp;amp; - \norm{y^{(t)} - y^{(t+1)}}_2^2
  \end{split} \right)
\end{split}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Recall that $(1/\rho)(y^{(t+1)} - y^{(t)}) = Ax^{(t+1)} + Bz^{(t+1)} - c$. Then,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \frac{\rho}{2} \norm{ Ax^{(t+1)} + Bz^{(t+1)} - c }
  &amp;amp;= \frac{\rho}{2}  \norm{ \frac{1}{\rho} (y^{(t+1)} - y^{(t  )}) } \
  &amp;amp;= \frac{1}{2\rho} \norm{                 y^{(t  )} - y^{(t+1)}  }
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;We can substitute that into the right hand side of the preceding equation to
cancel out a couple terms,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  = &amp;amp;
  \frac{\rho}{2} \left( \begin{split}
    &amp;amp; + \norm{Ax + Bz^{(t)} - c}&lt;em 0_="0$" _le="$\le" _text_="\text{" always&gt;2^2 &amp;amp;             - \norm{Ax         + Bz^{(t+1)} - c}_2^2 \
    &amp;amp;                                &amp;amp; \underbrace{- \norm{Ax^{(t+1)} + Bz^{(t  )} - c}_2^2}&lt;/em&gt; }
  \end{split} \right) \
  &amp;amp; + \frac{1}{2\rho} \left( \begin{split}
    &amp;amp; + \norm{y - y^{(t  )}}_2^2 \
    &amp;amp; - \norm{y - y^{(t+1)}}_2^2
  \end{split} \right)
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Finally dropping the portion of the equation that's always non-positive
(doing so doesn't affect the validity of the inequality), we obtain a concise
inequality in terms of sums of $\norm{\cdot}_2^2$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; h(w^{(t+1)}) - h(w) + \langle
    F(w^{(t+1)}),
    w^{(t+1)} - w
  \rangle  \
  &amp;amp; \qquad \le \frac{\rho}{2} \left(
      \norm{Ax + Bz^{(t  )} - c}_2^2
    - \norm{Ax + Bz^{(t+1)} - c}_2^2
  \right) \
  &amp;amp; \qquad + \frac{1}{2\rho} \left(
      \norm{y - y^{(t  )}}_2^2
    - \norm{y - y^{(t+1)}}_2^2
  \right)
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; Averaging across iterations. We're now in the home stretch. In this
step, we'll sum the previous equation across $t$. The sum will "telescope",
crossing out terms until we're left only with the initial and final conditions.
A quick application of Jensen's inequality will get us the desired result.&lt;/p&gt;
&lt;p&gt;We begin by summing the previous equation across iterations,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \sum_{\tau=0}^{t-1} h(w^{(\tau+1)}) - h(w) + \langle
    F(w^{(\tau+1)}),
    w^{(\tau+1)} - w
  \rangle  \
  &amp;amp; \qquad \le \frac{\rho}{2} \left(
                  \norm{Ax + Bz^{(0)} - c}&lt;em 0 _le="\le"&gt;2^2
    \underbrace{- \norm{Ax + Bz^{(t)} - c}_2^2}&lt;/em&gt;
  \right) + \frac{1}{2\rho} \left(
                  \norm{y - y^{(0)}}&lt;em 0 _le="\le"&gt;2^2
    \underbrace{- \norm{y - y^{(t)}}_2^2}&lt;/em&gt;
  \right)
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;For convenience, we'll choose $z^{(0)}$ and $y^{(0)}$ equal to zero. We'll
also drop the terms $-\norm{Ax + Bz^{(t)} - c}_2^2$ and
$-\norm{y - y^{(t)}}_2^2$ from the expression, as both terms are always
non-positive. This gives us,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \sum_{\tau=0}^{t-1} h(w^{(\tau+1)}) - h(w) + \langle
    F(w^{(\tau+1)}),
    w^{(\tau+1)} - w
  \rangle
  \le \frac{\rho}{2}  \norm{Ax - c}_2^2
             + \frac{1}{2\rho} \norm{ y    }_2^2
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Finally, recall that for a convex function $h(w)$, Jensen's Inequality states that&lt;/p&gt;
&lt;p&gt;$$
  h(\bar{w}&lt;em _tau="1"&gt;t)
  = h \left( \frac{1}{t} \sum&lt;/em&gt;^{t} w_{\tau} \right)
  \le \frac{1}{t} \sum_{\tau=1}^{t} h(w_{\tau})
$$&lt;/p&gt;
&lt;p&gt;The same is true for each of $F(w)$'s components (they're linear in $w$).
Thus, we can apply this statement to the left hand side of the preceding
equation after multiplying by $1/t$ to obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  h(\bar{w}^{(t)}) - h(w) + \langle
    F(\bar{w}^{(t)}),
    \bar{w}^{(t)} - w
  \rangle
  \le \frac{1}{t} \left(
      \frac{\rho}{2}  \norm{Ax - c}_2^2
    + \frac{1}{2\rho} \norm{ y    }_2^2
  \right)
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;The right hand side decreases as $O(1/t)$, thus ADMM converges at a rate of
at least $O(1/\epsilon)$ as desired.&lt;/p&gt;
&lt;h1&gt;&lt;a name="usage" href="#usage"&gt;When should I use it?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Similar to the proximal methods presented on this website, ADMM is only
efficient if we can perform each of its steps efficiently. Solving
2 optimization problems at each iteration may be very fast or very slow,
depending on if a closed form solution exists for $x^{(t+1)}$ and $z^{(t+1)}$.&lt;/p&gt;
&lt;p&gt;ADMM has been particularly useful in supervised machine learning, where $A$,
$B$, and $c$ are chosen such that $x = z$. In this scenario, $f$ is taken to be
the prediction loss on the training set, and $g$ an appropriate regularizer,
typically a norm such as $\ell_1$ or a &lt;a href="http://arxiv.org/pdf/1104.1872.pdf"&gt;group sparsity norm&lt;/a&gt;.
&lt;a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martins_150.pdf"&gt;ADMM also lends&lt;/a&gt; itself to inferring the most likely setting for
settings for latent variables in a factor graph. The primary benefit of ADMM in
both of these cases is not its rate of convergence but how &lt;a href="http://www.ece.umn.edu/users/alfonso/pubs/jmlr2010.pdf"&gt;easily it
lends itself to distributed computation&lt;/a&gt;. &lt;a href="http://arxiv.org/pdf/1009.1128.pdf"&gt;Applications in
Compressed Sensing&lt;/a&gt; see similar benefits.&lt;/p&gt;
&lt;p&gt;All in all, ADMM is &lt;em&gt;not&lt;/em&gt; a quick method, but it is a scalable one. ADMM is
best suited when data is too large to fit on a single machine or when
$x^{(t+1)}$ and $z^{(t+1)}$ can be solved for in closed form. While very
interesting in its own right, ADMM should rarely your algorithm of choice.&lt;/p&gt;
&lt;h1&gt;&lt;a name="extensions" href="#extensions"&gt;Extensions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Accelerated&lt;/strong&gt; As ADMM is so closely related to Proximal Gradient-based
methods, one might ask if there exists an accelerated variant with a better
convergence rate. The answer is a resounding yes, as shown by &lt;a href="ftp://ftp.math.ucla.edu/pub/camreport/cam12-35.pdf"&gt;Goldstein et
al.&lt;/a&gt;, though care must be taken for non-strongly convex
objectives. In their article, Goldstein et al. show that a convergence rate of
$O(1/\sqrt{\epsilon})$ can be guaranteed if both $f$ and $g$ are strongly
convex. If this isn't the case, only a rate of $O(1/\epsilon)$ is shown.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Online&lt;/strong&gt; In online learning, one is interested in solving a series of
supervised machine learning instances in sequence with minimal error. At each
iteration, the algorithm is presented with an input $x_t$, to which it responds
with a prediction $\hat{y}&lt;em t&gt;t$. The world then presents the algorithm with the
correct answer $y_t$, and the algorithm suffers loss $l_t(y_t, \hat{y}_t)$. The
goal of the algorithm is to minimize the sum of errors $\sum&lt;/em&gt; l_t(y_t,
\hat{y}_t)$.&lt;/p&gt;
&lt;p&gt;In this setting, &lt;a href="http://icml.cc/2012/papers/577.pdf"&gt;Wang&lt;/a&gt; has shown that an online variant to ADMM
can achieve regret competitive with the best possible ($O(\sqrt{T})$ for
convex loss functions, $O(\log(T))$ for strongly convex loss functions).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stochastic&lt;/strong&gt; In a stochastic setting, one is interested in minimizing the
&lt;em&gt;average&lt;/em&gt; value of $f(x)$ via a series of samples. In &lt;a href="http://arxiv.org/pdf/1211.0632.pdf"&gt;Ouyang et
al&lt;/a&gt;, convergence rates for a linearized variant of ADMM when
$f$ can only be accessed through samples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multi Component&lt;/strong&gt; Traditional ADMM considers an objective with only
2 components $f(x)$ and $g(z)$. While applying the same logic to 3 or more is
straightforward, proving convergence for this scenario is more difficult. This
was the task taken by &lt;a href="http://www.optimization-online.org/DB_FILE/2010/12/2871.pdf"&gt;He et al&lt;/a&gt;. In particular, they showed that
a special variant of ADMM using "Gaussian back substitution" is ensured to
converge.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;ADMM&lt;/strong&gt; While ADMM has existed for decades, it has only recently been brought
to light by &lt;a href="/blog/admm.html"&gt;Boyd&lt;/a&gt;'s article describing its applications for statistical
machine learning. It is from this work from which I initially learned of ADMM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof of Convergence&lt;/strong&gt; The proof of convergence presented here is a verbose
expansion of that presented in &lt;a href="http://icml.cc/2012/papers/577.pdf"&gt;Wang&lt;/a&gt;'s paper on Online ADMM.&lt;/p&gt;
&lt;!-- internal references --&gt;

&lt;!-- papers --&gt;

&lt;!-- convergence proofs --&gt;

&lt;!-- extensions --&gt;

&lt;!-- uses --&gt;

&lt;h1&gt;&lt;a name="reference-impl" href="#reference-impl"&gt;Reference Implementation&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Using the &lt;a href="https://github.com/duckworthd/optim"&gt;&lt;code&gt;optim&lt;/code&gt;&lt;/a&gt; Python package, we can generate the animation above,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;Example usage of ADMM solver.&lt;/span&gt;

&lt;span class="sd"&gt;A gif is generated showing the iterates as they converge.&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;matplotlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;animation&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;optim.admm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;optim.tests.test_admm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;quadratic1&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;it&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;     &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Usage: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt; OUTPUT&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],))&lt;/span&gt;
  &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;quadratic1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;admm&lt;/span&gt;        &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ADMM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rho&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;iterates&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;it&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;islice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;admm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;solve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;xs&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;zs&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;xs2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;primal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;State&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;zs2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;primal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;State&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;zs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;animate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;iteration:&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cla&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Iteration #&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;k-&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;f(x)+g(z)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;             &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;g--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;f(x)&amp;#39;&lt;/span&gt;     &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;   &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;            &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;b--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;     &lt;span class="s1"&gt;&amp;#39;g(z)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;xs2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;g&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;zs2&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;z&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;anim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;animation&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FuncAnimation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gcf&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;animate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;frames&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;anim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;imagemagick&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fps&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="optimization"></category><category term="distributed"></category><category term="admm"></category></entry><entry><title>Frank-Wolfe Algorithm</title><link href="/blog/frank-wolfe.html" rel="alternate"></link><published>2013-05-04T00:00:00-07:00</published><updated>2013-05-04T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-05-04:/blog/frank-wolfe.html</id><summary type="html">&lt;p&gt;In this post, we'll take a look at the &lt;a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm"&gt;Frank-Wolfe Algorithm&lt;/a&gt;
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the &lt;a href="/blog/proximal-gradient-descent.html"&gt;Proximal
Gradient&lt;/a&gt; and &lt;a href="/blog/accelerated-proximal-gradient-descent.html"&gt;Accelerated Proximal
Gradient&lt;/a&gt; algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem. Our …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this post, we'll take a look at the &lt;a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm"&gt;Frank-Wolfe Algorithm&lt;/a&gt;
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the &lt;a href="/blog/proximal-gradient-descent.html"&gt;Proximal
Gradient&lt;/a&gt; and &lt;a href="/blog/accelerated-proximal-gradient-descent.html"&gt;Accelerated Proximal
Gradient&lt;/a&gt; algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem. Our
reward for doing so is a converge rate of $O(1/\epsilon)$ and the potential for
&lt;em&gt;extremely sparse solutions&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Returning to my &lt;a href="/blog/gradient-descent.html"&gt;valley-finding metaphor&lt;/a&gt;, Frank-Wolfe is a
bit like this,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;ol&gt;
&lt;li&gt;Look around you and see which way points the most downwards&lt;/li&gt;
&lt;li&gt;Walk as far as possible in that direction until you hit a wall&lt;/li&gt;
&lt;li&gt;Go back in the direction you started, stop part way along the path, then
     repeat.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h1&gt;&lt;a name="implementation" href="#implementation"&gt;How does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Frank-Wolfe is designed to solve problems of the form,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x \in D} f(x)
$$&lt;/p&gt;
&lt;p&gt;where $D$ is compact and $f$ is differentiable. For example, in $R^n$ any
closed and bounded set is compact. The algorithm for Frank-Wolfe is then,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: Initial iterate $x^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $t = 0, 1, 2, \ldots$&lt;ol&gt;
&lt;li&gt;Let $s^{(t+1)} = \arg\min_{s \in D} \langle \nabla f(x^{(t)}), s \rangle$&lt;/li&gt;
&lt;li&gt;If $g(x) = \langle \nabla f(x^{(t)}), x - s^{(t+1)} \rangle \le \epsilon$, break&lt;/li&gt;
&lt;li&gt;Let $x^{(t+1)} = (1 - \alpha^{(t)}) x^{(t)} + \alpha^{(t)} s^{(t+1)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;The proof relies on $\alpha^{(t)} = 2 / (t+2)$, but line search works as
well.  The intuition for the algorithm is that at each iteration, we minimize
a linear approximation to $f$,&lt;/p&gt;
&lt;p&gt;$$
  s^{(t+1)} = \arg\min_{s \in D} f(x^{(t)}) + \nabla f(x^{(t)})^T (s - x^{(t)})
$$&lt;/p&gt;
&lt;p&gt;then take a step in that direction. We can immediately see that if $D$
weren't compact, $s^{(t)}$ would go off to infinity.&lt;/p&gt;
&lt;p&gt;&lt;a id="upper_bound"&gt;&lt;/a&gt;
  &lt;strong&gt;Upper Bound&lt;/strong&gt; One nice property of Frank-Wolfe is that it comes with its
own upper bound on $f(x^{(t)}) - f(x^{*})$ calculated during the course of
the algorithm. Recall the linear upper bound on $f$ due to convexity,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{&lt;/em&gt;})
  &amp;amp; \ge f(x) + \nabla f(x)^T (x^{&lt;em&gt;} - x) \
  f(x) - f(x^{&lt;/em&gt;})
  &amp;amp; \le \nabla f(x)^T (x - x^{&lt;em&gt;}) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Since,&lt;/p&gt;
&lt;p&gt;$$
  s^{(t+1)}
  = \arg\min_{s} \nabla f(x^{(t)})^T s
  = \arg\max_{s} \nabla f(x^{(t)})^T (x^{(t)} - s)
$$
  we know that $\nabla f(x^{(t)})^T (x^{(t)} - x^{*}) \le \nabla f(x^{(t)})^T
(x^{(t)} - s^{(t+1)})$ and thus,&lt;/p&gt;
&lt;p&gt;$$
  f(x) - f(x^{*}) \le \nabla f(x^{(t)})^T (x^{(t)} - s^{(t+1)})
$$&lt;/p&gt;
&lt;p&gt;&lt;a id="example"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name="example" href="#example"&gt;A Small Example&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;For this example, we'll minimize a simple univariate quadratic function
constrained to lie in an interval,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x \in [-1,2]} (x-0.5)^2 + 2x
$$&lt;/p&gt;
&lt;p&gt;Its derivative is given by $2(x-0.5) + 2$, and since we are dealing with real
numbers, the minimizers of the linear approximation must be either $-1$ or
$2$ if the gradient is positive or negative, respectively. We'll use a stepsize
of $\alpha^{(t)} = 2 / (t+2)$ as prescribed by the convergence proof in the
next section.&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/frank_wolfe/animation.gif"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    Frank-Wolfe in action. The red circle is the current value for
    $f(x^{(t)})$, and the green diamond is $f(x^{(t+1)})$. The dotted line is
    the linear approximation to $f$ at $x^{(t)}$. Notice that at each step,
    Frank-Wolfe stays closer and closer to $x^{(t)}$ when moving in the
    direction of $s^{(t+1)}$.
  &lt;/span&gt;
&lt;/div&gt;

&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/frank_wolfe/convergence.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows how quickly the objective function decreases as the
    number of iterations increases. Notice that it does not monotonically
    decrease, as with Gradient Descent.
  &lt;/span&gt;
&lt;/div&gt;

&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/frank_wolfe/iterates.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number. Since
    Frank-Wolfe uses linear combinations of $s^{(t+1)}$ and $x^{(t)}$, it
    tends to "bounce around" a lot, especially in earlier iterations.
  &lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a id="proof"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name="proof" href="#proof"&gt;Why does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;We begin by making the two assumptions given earlier,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f$ is convex, differentiable, and finite for all $x \in D$&lt;/li&gt;
&lt;li&gt;$D$ is compact&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; First, notice that we never needed to assume that a solution
$x^{*}$ exists. This is because $D$ is compact and $f$ is finite, meaning $x$
cannot get bigger and bigger to make $f(x)$ arbitrarily small.&lt;/p&gt;
&lt;p&gt;Secondly, we never made a Lipschitz assumption on $f$ or its gradient. Since
$D$ is compact, we don't have to -- instead, we get the following for free.
Define $C_f$ as,&lt;/p&gt;
&lt;p&gt;$$
  C_f = \max_{\substack{
                x,s \in D \
                \alpha \in [0,1] \
                y = x + \alpha (s-x)
              }}
          \frac{2}{\alpha^2} \left(
            f(y) - f(x) - \langle \nabla f(x), y - x \rangle
          \right)
$$&lt;/p&gt;
&lt;p&gt;This immediate implies the following upper bound on $f$ for all $x, y \in
D$ and $\alpha \in [0,1]$,&lt;/p&gt;
&lt;p&gt;$$
  f(y) \le f(x) + \langle \nabla f(x), y-x \rangle + \frac{\alpha^2}{2} C_f
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof Outline&lt;/strong&gt; The proof for Frank-Wolfe is surprisingly simple. The idea
is to first upper bound $f(x^{(t+1)})$ in terms of $f(x^{(t)})$, $g(x^{(t)})$,
and $C_f$. We then transform this per-iteration bound into a bound on
$f(x^{(t)}) - f(x^{*})$ depending on $t$ using induction. That's it!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; Upper bound $f(x^{(t+1)})$. As usual, we'll denote $x^{+} \triangleq
x^{(t+1)}$, $x \triangleq x^{(t)}$, $s^{+} \triangleq s^{(t+1)}$, and $\alpha
\triangleq \alpha^{(t)}$. We begin by using the upper bound we just obtained for
$f$ in terms of $C_f$, substituting $x^{+} = (1 - \alpha) x + \alpha s^{+}$ and
then $g(x) = \nabla f(x)^T (x - s^{+})$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{+}) 
  &amp;amp; \le f(x) + \nabla f(x)^T (x^{+} - x) + \frac{\alpha^2}{2} C_f \
  &amp;amp; = f(x) + \nabla f(x)^T ( (1-\alpha) x + \alpha s^{+} - x ) + \frac{\alpha^2}{2} C_f \
  &amp;amp; = f(x) + \nabla f(x)^T ( \alpha s^{+} - \alpha x ) + \frac{\alpha^2}{2} C_f \
  &amp;amp; = f(x) - \alpha \nabla f(x)^T ( x - s^{+} ) + \frac{\alpha^2}{2} C_f \
  &amp;amp; = f(x) - \alpha g(x) + \frac{\alpha^2}{2} C_f \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Use induction on $t$. First, recall the upper bound on $f(x) -
f(x^{&lt;em&gt;}) \le g(x)$ &lt;a href="#upper_bound"&gt;we derived above&lt;/a&gt;. Let's add $-f(x^{&lt;/em&gt;})$ into
what we got from Step 1, then use the upper bound on $f(x) - f(x^{*})$ to get,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{+}) - f(x^{&lt;/em&gt;})
  &amp;amp; \le f(x) - f(x^{&lt;em&gt;}) - \alpha g(x) + \frac{\alpha^2}{2} C_f \
  &amp;amp; \le f(x) - f(x^{&lt;/em&gt;}) - \alpha ( f(x) - f(x^{&lt;em&gt;}) ) + \frac{\alpha^2}{2} C_f \
  &amp;amp; = (1 - \alpha) (f(x) - f(x^{&lt;/em&gt;})) + \frac{\alpha^2}{2} C_f \
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Now, we employ induction on $t$ to show that,&lt;/p&gt;
&lt;p&gt;$$
  f(x^{(t)}) - f(x^{*}) \le \frac{4 C_f / 2}{t+2}
$$&lt;/p&gt;
&lt;p&gt;We'll assume that the step size is $\alpha^{(t)} = \frac{2}{t+2}$, giving us
$\alpha^{(0)} = 2 / (0+2) = 1$ and the base case,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{(1)} - f(x^{&lt;/em&gt;})
  &amp;amp; \le (1 - \alpha^{(0)}) ( f(x^{(0)}) - f(x^{&lt;em&gt;}) ) + \frac{\alpha^2}{2} C_f \
  &amp;amp; = (1 - 1) ( f(x^{(0)}) - f(x^{&lt;/em&gt;}) ) + \frac{1}{2} C_f \
  &amp;amp; \le \frac{4 C_f / 2}{(0 + 1) + 2}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Next, for the recursive case, we use the inductive assumption on $f(x) - f(x^{*})$, the definition of $\alpha^{(t)}$, and some algebra,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{+}) - f(x^{&lt;/em&gt;})
  &amp;amp; \le (1 - \alpha) ( f(x) - f(x^{&lt;em&gt;}) ) + \frac{ \alpha^2}{2} C_f \
  &amp;amp; \le \left(1 - \frac{2}{t+2} \right) \frac{4 C_f / 2}{t + 2} + \left( \frac{2}{t+2} \right)^2 C_f / 2 \
  &amp;amp; \le \frac{4 C_f / 2}{t + 2} \left( 1 - \frac{2}{t+2} + \frac{1}{t+2} \right) \
  &amp;amp; = \frac{4 C_f / 2}{t + 2} \left( \frac{t+1}{t+2} \right) \
  &amp;amp; \le \frac{4 C_f / 2}{t + 2} \left( \frac{t+2}{t+3} \right) \
  &amp;amp; = \frac{4 C_f / 2}{(t + 1) + 2} \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Thus, if we want an error tolerance of $\epsilon$, we need
$O(\frac{1}{\epsilon})$ iterations to find it. This matches the convergence
rate of Gradient Descent an Proximal Gradient Descent, but falls short of their
accelerated brethren.&lt;/p&gt;
&lt;h1&gt;&lt;a name="usage" href="#usage"&gt;When should I use it?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Like Proximal Gradient, efficient use of Frank-Wolfe requires solving a
mini-optimization problem at each iteration. Unlike Proximal Gradient, however,
this mini-problem will lead to unbounded iterates if the input space is not
compact -- in other words, Frank-Wolfe cannot directly be applied when your
domain is all of $R^{n}$. However, there is a very special case wherein
Frank-Wolfe shines.&lt;/p&gt;
&lt;p&gt;&lt;a id="sparsity"&gt;&lt;/a&gt;
  &lt;strong&gt;Sparsity&lt;/strong&gt; The primary reason machine learning researchers have recently
taken an interest in Frank-Wolfe is because in certain problems the iterates
$x^{(t)}$ will be extremely sparse.  Suppose that $D$ is a polyhedron defined
by a set of linear constraints. Then $s^{(t)}$ is a solution to a Linear
Program, meaning that each $s^{(t)}$ lies on one of the vertices of the
polyhedron. If these vertices have only a few non-zero entries, then $x^{(t)}$
will too, as $x^{(t)}$ is a linear combination of $s^{(1)} \ldots s^{(t)}$.
This is in direct contrast to gradient and proximal based methods, wherein
$x^{(t)}$ is the linear combination of a set of non-sparse &lt;em&gt;gradients&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Atomic Norms&lt;/strong&gt; One particular case where Frank-Wolfe shines is when
minimizing $f(x)$ subject to $||x|| \le c$ where $|| \cdot ||$ is an "atomic
norm". We say that $||\cdot||$ is an atomic norm if $||x||$ is the smallest $t$
such that $x/t$ is in the convex hull of a finite set of points $\mathcal{A}$,
that is,&lt;/p&gt;
&lt;p&gt;$$
  ||x|| = \inf { t : x \in t \, \text{Conv}(\mathcal{A}) }
$$&lt;/p&gt;
&lt;p&gt;For example, $||x||&lt;em _le="\le" _s_="||s||" c&gt;1$ is an atomic norm with $\mathcal{A}$ being the set of
all vectors with only one $+1$ or one $-1$ entry. In these cases, finding
$\arg\min&lt;/em&gt; \langle \nabla f(x), s \rangle$ is tantamount to
finding which element of $\mathcal{A}$ minimizes $\langle \nabla f(x), s
\rangle$ (since $\text{Conv}(\mathcal{A})$ defines a polyhedron). For a whole
lot more on Atomic Norms, see &lt;a href="http://pages.cs.wisc.edu/~brecht/papers/2010-crpw_inverse_problems.pdf"&gt;this tome&lt;/a&gt; by
Chandrasekaranm et al.&lt;/p&gt;
&lt;h1&gt;&lt;a name="extensions" href="#extensions"&gt;Extensions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Step Size&lt;/strong&gt; The proof above relied on a step size of $\alpha^{(t)} =
\frac{2}{t+2}$, but as usual &lt;a href="/blog/gradient-descent.html#line_search"&gt;Line Search&lt;/a&gt; can be applied to
accelerate convergence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Approximate Linear Solutions&lt;/strong&gt; Though not stated in the proof above,
another cool point about Frank-Wolfe is that you don't actually need to solve
the linear mini-problem exactly, but you will still converge to the optimal
solution (albet at a slightly slower rate). In particular, assume that each
mini-problem can be solved approximately with additive error $\frac{\delta
C_f}{t+2}$ at iteration $t$,&lt;/p&gt;
&lt;p&gt;$$
  \langle s^{(t+1)}, \nabla f(x^{(t)}) \rangle
  \le \min_{s} \langle s, \nabla f(x^{(t)}) \rangle + \frac{\delta C_f}{t+2}
$$&lt;/p&gt;
&lt;p&gt;then Frank-Wolfe's rate of convergence is&lt;/p&gt;
&lt;p&gt;$$
  f(x^{(t)}) - f(x^{*}) \le \frac{2 C_f}{t+2} (1 + \delta)
$$&lt;/p&gt;
&lt;p&gt;The proof for this can be found in the supplement to &lt;a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf"&gt;Jaggi's&lt;/a&gt;
excellent survey on Frank-Wolfe for machine learning.&lt;/p&gt;
&lt;h1&gt;&lt;a name="invariance" href="#invariance"&gt;Linear Invariance&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Another cool fact about Frank-Wolfe is that it's &lt;em&gt;linearly invariant&lt;/em&gt; -- that
is, if you rotate and scale the space, nothing changes about the convergence
rate. This is in direct contrast to many other methods which depend on the
&lt;a href="http://en.wikipedia.org/wiki/Condition_number"&gt;condition number&lt;/a&gt; of a function (for functions with
Hessians, this is the ratio between the largest and smallest eigenvalues,
$\sigma_{\max} / \sigma_{\min})$.&lt;/p&gt;
&lt;p&gt;Suppose we transform our input space with a surjective (that is, onto) linear
transformation $M: \hat{D} \rightarrow D$. Let's now try to solve the problem,&lt;/p&gt;
&lt;p&gt;$$
  \min_{\hat{x} \in \hat{D}} \hat{f}(\hat{x}) = f(M \hat{x}) = f(x)
$$&lt;/p&gt;
&lt;p&gt;Let's look at the solution to the per-iteration mini-problem we need to solve
for Frank-Wolfe,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \min_{\hat{s} \in \hat{D}} \langle \nabla \hat{f}(\hat{x}), \hat{s} \rangle
  = \min_{\hat{s} \in \hat{D}} \langle M^T \nabla f( M \hat{x}), \hat{s} \rangle
  = \min_{\hat{s} \in \hat{D}} \langle \nabla f( x ), M \hat{s} \rangle
  = \min_{s \in D} \langle \nabla f( x ), s \rangle
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;In other words, we will find the same $s$ if we solve in the original space,
or if we find $\hat{s}$ and then map it back to $s$. No matter how $M$ warps
the space, Frank-Wolfe will do the same thing. This also means that if there's
a linear transformation you can do to make the points of your polyhedron
sparse, you can do it with no penalty!&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Proof of Convergence, Linear Invariance&lt;/strong&gt; Pretty much everything in this
article comes from &lt;a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf"&gt;Jaggi's&lt;/a&gt; fantastic article on Frank-Wolfe for
machine learning.&lt;/p&gt;
&lt;h1&gt;&lt;a name="reference-impl" href="#reference-impl"&gt;Reference Implementation&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;frank_wolfe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minisolver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-2&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Frank-Wolfe Algorithm&lt;/span&gt;

&lt;span class="sd"&gt;  Parameters&lt;/span&gt;
&lt;span class="sd"&gt;  ----------&lt;/span&gt;
&lt;span class="sd"&gt;  minisolver : function&lt;/span&gt;
&lt;span class="sd"&gt;      minisolver(x) = argmin_{s \in D} &amp;lt;x, s&amp;gt;&lt;/span&gt;
&lt;span class="sd"&gt;  gradient : function&lt;/span&gt;
&lt;span class="sd"&gt;      gradient(x) = gradient[f](x)&lt;/span&gt;
&lt;span class="sd"&gt;  alpha : function&lt;/span&gt;
&lt;span class="sd"&gt;      learning rate&lt;/span&gt;
&lt;span class="sd"&gt;  x0 : array&lt;/span&gt;
&lt;span class="sd"&gt;      initial value for x&lt;/span&gt;
&lt;span class="sd"&gt;  epsilon : float&lt;/span&gt;
&lt;span class="sd"&gt;      desired accuracy&lt;/span&gt;
&lt;span class="sd"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;iteration&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
  &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;s_next&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;minisolver&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;s_next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;epsilon&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="k"&gt;break&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;direction&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;s_next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x_next&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;s_next&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_next&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;iteration&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;default_learning_rate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iteration&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iteration&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yannopt.plotting&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plotting&lt;/span&gt;

  &lt;span class="c1"&gt;### FRANK WOLFE ALGORITHM ###&lt;/span&gt;

  &lt;span class="c1"&gt;# problem definition&lt;/span&gt;
  &lt;span class="n"&gt;function&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
  &lt;span class="n"&gt;gradient&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
  &lt;span class="n"&gt;minisolver&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="c1"&gt;# D = [-1, 2]&lt;/span&gt;
  &lt;span class="n"&gt;x0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;

  &lt;span class="c1"&gt;# run gradient descent&lt;/span&gt;
  &lt;span class="n"&gt;iterates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;frank_wolfe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minisolver&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default_learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;### PLOTTING ###&lt;/span&gt;

  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iterates_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/iterates.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iteration_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                      &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/convergence.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;# make animation&lt;/span&gt;
  &lt;span class="n"&gt;iterates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;makedirs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/animation&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;OSError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;

  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;s_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;minisolver&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;
    &lt;span class="n"&gt;f_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xmax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;limits&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;

    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;xmax&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xmax&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xmax&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymax&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;f(x)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xmax&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f_hat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xmin&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;f_hat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xmax&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vlines&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ymin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ymax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;solid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vlines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_hat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;dotted&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s_plus&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_hat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s_plus&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/animation/&lt;/span&gt;&lt;span class="si"&gt;%02d&lt;/span&gt;&lt;span class="s1"&gt;.png&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="optimization"></category><category term="first-order"></category><category term="sparsity"></category></entry><entry><title>Variational Inference</title><link href="/blog/variational-inference.html" rel="alternate"></link><published>2013-04-28T00:00:00-07:00</published><updated>2013-04-28T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-04-28:/blog/variational-inference.html</id><summary type="html">&lt;p&gt;&lt;a href="http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf"&gt;Variational Inference&lt;/a&gt; and Monte Carlo Sampling are
currently the two chief ways of doing approximate Bayesian inference. In the
Bayesian setting, we typically have some observed variables $x$ and
unobserved variables $z$, and our goal is to calculate $P(z|x)$. In all but
the simplest cases, calculating $P(z …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf"&gt;Variational Inference&lt;/a&gt; and Monte Carlo Sampling are
currently the two chief ways of doing approximate Bayesian inference. In the
Bayesian setting, we typically have some observed variables $x$ and
unobserved variables $z$, and our goal is to calculate $P(z|x)$. In all but
the simplest cases, calculating $P(z|x)$ for all values of $z$ in closed form
is impossible, so approximations must be made.&lt;/p&gt;
&lt;p&gt;Variational Inference's approximation is made by choosing a family of
distributions $q(z|\eta)$ parameterized by $\eta$ and choosing a setting for
$\eta$ that brings $q(z|\eta)$ "close" to $P(z|x)$.  In particular,
Variational Inference is about finding,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \arg\min_{\eta} KL \left[ q(z|\eta) || P(z|x) \right] \
  &amp;amp; = \arg\min_{\eta} \sum_{z} q(z|\eta) \log \frac{ q(z|\eta) }{ P(z|x) }
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Looking at this formulation, the first thing you should be thinking is, "We
don't even know how to calculate $P(z|x)$ much less take an expectation with
respect to it. How can I possibly solve this problem?" The key is to restrict
$q(z|\eta)$ to decompose into a product of independent distributions, 1 for
each hidden variable $z_i$. In other words,&lt;/p&gt;
&lt;p&gt;$$
  q(z|\eta) = \prod_{i} q(z_i | \eta_i)
$$&lt;/p&gt;
&lt;p&gt;This is the "mean field approximation" and will allow us to optimize each
$\eta_i$ one at a time. The final key $P(z_i|z_{-i},x)$ must lie in the
exponential family, and that $q(z_i|\eta_i)$ be of the same form. For example,
if the former is a Dirichlet distribution, so should the latter. When this is
the case, we can solve the Coordinate Ascent update in closed form.&lt;/p&gt;
&lt;p&gt;When all 3 conditions are met -- the mean field approximation, the univariate
posteriors lie in the exponential family, and that the individual variational
distributions match -- we can apply Coordinate Ascent to minimize the
KL-divergence between the mean field distribution and the posterior.&lt;/p&gt;
&lt;h1&gt;&lt;a name="derivation" href="#derivation"&gt;Derivation of the Objective&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The original intuition for Variational Inference stems from lower bounding
the marginal likelihood of the observed variables $P(x)$, then maximizing that
lower bound. For many choices of $q(z|\eta)$ doing this will be computationally
infeasible, but we'll see that if we make the mean field approximation and
choose the right variational distributions, then we can efficiently do
Coordinate Ascent.&lt;/p&gt;
&lt;p&gt;First, let's derive a lower bound on the likelihood of the observed
variables,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \log P(x)
  &amp;amp; = \log \left(
    \sum_{z} P(x, z) \frac{ q(z | \eta) } { q(z | \eta) }
  \right) \
  &amp;amp; = \log \left(
    P(x)  \sum_{z} q(z | \eta) \frac{ P(z | x) } { q(z | \eta) }
  \right) \
  &amp;amp; = \log \left(
    \sum_{z} q(z | \eta) \frac{ P(z | x) } { q(z | \eta) }
  \right) + \log P(x) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Since $\log$ is a concave function, we can apply Jensen's inequality to see
that $\log(p x + (1-p)y) \ge p \log(x) + (1-p) \log y$ for any $p \in [0,
1]$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \log P(x)
  &amp;amp; = \log \left(
    \sum_{z} q(z | \eta) \frac{ P(z | x) } { q(z | \eta) }
  \right) + \log P(x) \
  &amp;amp; \ge \sum_{z} q(z | \eta) \log \left(
    \frac{ P(z | x) } { q(z | \eta) }
  \right) + \log P(x) \
  &amp;amp; = - \sum_{z} q(z | \eta) \log \left(
    \frac{ q(z | \eta) } { P(z | x) }
  \right) + \log P(x) \
  &amp;amp; = - \text{KL}[ q(z | \eta) || P(z | x) ] + \log P(x) \
  &amp;amp; = - \text{KL}[ q(z | \eta) || P(z , x) ] \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;From this expression, we can see that minimizing the KL divergence over
$\eta$, we're lower bounding the likelihood of the observed variables.
In addition, if $q(z|\eta)$ has the same form as $P(z|x)$, then the best choice
for $\eta$ is one that lets $q(z|\eta) = P(z|x)$ for all $z$.&lt;/p&gt;
&lt;p&gt;At this point, we still have an intractable problem. Even evaluating the KL
divergence requires taking an expectation over all settings for $z$ (an
exponential number in $z$'s length!), so applying an iterative algorithm to
choose $\eta$ is right out. However, we'll soon see that by restricting the
form of $q(z|\eta)$, we can potentially decompose the KL divergence into more
easily manageable bits.&lt;/p&gt;
&lt;h1&gt;&lt;a name="mean-field" href="#mean-field"&gt;The Mean Field Approximation&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The key to avoiding the massive sum of the previous equation is to assume that
$q(z|\eta)$ decomposes into a product of independent distributions. This is
known as the "Mean Field Approximation". Mathematically, the approximation
means that,&lt;/p&gt;
&lt;p&gt;$$
  q(z|\eta) = \prod_{i} q(z_i | \eta_i)
$$&lt;/p&gt;
&lt;p&gt;Suppose we make this assumption and that we want to perform coordinate ascent
on a single index $\eta_k$. By factoring $P(z|x) = \prod_{i=1}^{k} P(z_i |
z_{1:i-1}, x)$ and dropping all terms that are constant with respect to
$\eta_k$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \arg\max_{\eta_k} -KL \left[ q(z|\eta) || p(z|x) \right] + \underbrace{\log P(x)}&lt;em _eta_k="\eta_k"&gt;{\text{constant wrt $\eta_k$}} \
  &amp;amp; = \arg\max&lt;/em&gt; \sum_{z} q(z|\eta) \log P(z|x) - \sum_{z} q(z|\eta) \log q(z|\eta) \
  &amp;amp; = \arg\max_{\eta_k} \sum_{z} q(z|\eta) \log \left( \prod_{i} P(z_{i}|z_{1:i-1},x) \right)
    - \sum_{z} \left( \prod_{i} q(z_i|\eta_i) \right) \log \left( \prod_{i} q(z_i|\eta_i) \right) \
  &amp;amp; = \arg\max_{\eta_k} \sum_{j} \sum_{z} q(z|\eta)\log P(z_{j}|z_{1:j-1},x)
    - \underbrace{ \sum_{j} \sum_{z_j} q(z_j|\eta_j) \log q(z_j|\eta_j) }&lt;em _eta_k="\eta_k"&gt;{\text{only $j=k$ not const wrt. $\eta_k$}} \
  &amp;amp; = \arg\max&lt;/em&gt; \underbrace{ \sum_{j} \sum_{z_{1:j}} \left( \prod_{i \le j} q(z_i|\eta_i) \right) \log P(z_{j}|z_{1:j-1},x) }&lt;em z_k&gt;{\text{only last $j$ contains $q(z_k|\eta_k)$}}
    - \sum&lt;/em&gt; q(z_k|\eta_k) \log q(z_k|\eta_k)  \
  &amp;amp; = \arg\max_{\eta_k} \sum_{z} q(z_k|\eta_k) \underbrace{ \left( \prod_{i \ne k} q(z_i|\eta_i) \right) }&lt;em -k&gt;{\text{fixed wrt $\eta_k$}} \log P(z_k | z&lt;/em&gt;, x)
    - \sum_{z_k} q(z_k|\eta_k) \log q(z_k|\eta_k)  \
  &amp;amp; = \arg\max_{\eta_k} \mathbb{E}&lt;em -k&gt;{q(z|\eta)} \left[ \log P(z_k | z&lt;/em&gt;, x) \right]
    - \mathbb{E}_{ q(z_k|\eta_k) } \left[ \log q(z_k|\eta_k) \right] \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;At this point, we'll make the assumption that $P(z_k|z_{-k},x)$ is an
exponential family distribution ($z_{-k}$ is all $z_i$ with $i \ne k$), and
moreover that $q(z_k|\eta_k)$ and $P(z_k|z_{-k},x)$ lie in the same exponential
family.  Mathematically, this means that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  q(z_k|\eta_k)
  &amp;amp;= h(z_k) \exp( \eta_i^T t(z_k) - A(\eta_k) \
  P(z_k|z_{-k},x)
  &amp;amp;= h(z_k) \exp( g(z_{-k},x)^T t(z_k) - A(g(z_{-k},x)) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Here $t(\cdot)$ are sufficient statistics, $A(\cdot)$ is the log of the
normalizing constant, $g(\cdot)$ is a function of all other variables that
determines the parameters for $P(z_k|z_{-k},x)$, and $h(\cdot)$ is some
function that doesn't depend on the parameters of the distribution.&lt;/p&gt;
&lt;p&gt;Plugging this back into the previous equation (we define it to be
$L(\eta_k)$), applying the $\log$, and using the linearity property of the
expectation,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
&amp;amp; \arg\max_{\eta_k} &amp;amp;&amp;amp; L(\eta_k) \
= &amp;amp; \arg\max_{\eta_k} &amp;amp;&amp;amp; \mathbb{E}&lt;em -k&gt;{q(z|\eta)} \left[ \log P(z_k | z&lt;/em&gt;, x) \right]
    - \mathbb{E}&lt;em _eta_k="\eta_k"&gt;{ q(z_k|\eta_k) } \left[ \log q(z_k|\eta_k) \right] \
= &amp;amp; \arg\max&lt;/em&gt; &amp;amp;&amp;amp;\mathbb{E}&lt;em -k&gt;{q(z|\eta)} \left[
    \log h(z_k) + g(z&lt;/em&gt;,x)^T t(z_k) - A(g(z_{-k},x)
  \right]
  - \mathbb{E}&lt;em _eta_k="\eta_k"&gt;{q(z_k|\eta_k)} \left[ \log q(z_k|\eta_k) \right]  \
= &amp;amp; \arg\max&lt;/em&gt; &amp;amp;&amp;amp;\left(
    \underbrace{ \mathbb{E}&lt;em _text_cancels="\text{cancels" out&gt;{q(z_k|\eta_k)} \left[ \log h(z_k) \right] }&lt;/em&gt;}
    + \underbrace{
      \mathbb{E}&lt;em -k&gt;{q(z&lt;/em&gt;|\eta_{-k})} \left[ g(z_{-k},x) \right]^T \mathbb{E}&lt;em k&gt;{q(z&lt;/em&gt;|\eta_{k})} \left[ t(z_k) \right]
    }&lt;em -k&gt;{\text{$\mathbb{E}$ splits b/c $q(z&lt;/em&gt;|\eta_{-k})$ and $q(z_k|\eta_k)$ are indep.}}
    - \underbrace{ \mathbb{E}&lt;em -k&gt;{q(z&lt;/em&gt;|\eta_{-k})} \left[ A(g(z_{-k},x) \right] }&lt;em q_z_k_eta_k_="q(z_k|\eta_k)"&gt;{\text{const wrt $\eta_k$}}
  \right) \
&amp;amp;&amp;amp;&amp;amp; - \left(
    \underbrace{ \mathbb{E}&lt;/em&gt; \left[ \log h(z_k) \right] }&lt;em q_z_k_eta_k_="q(z_k|\eta_k)"&gt;{\text{cancels out}}
    + \eta_k^T \mathbb{E}&lt;/em&gt; \left[ t(z_k) \right]
    - A(\eta_k)
  \right) \
= &amp;amp; \arg\max_{\eta_k} &amp;amp;&amp;amp; \mathbb{E}&lt;em -k&gt;{q(z&lt;/em&gt;|\eta_{-k})} \left[ g(z_{-k},x) \right]^T \left( \nabla_{\eta_k} A(\eta_k) \right)
    + \eta_k^T \left( \nabla_{\eta_k} A(\eta_k) \right)
    - A(\eta_k) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;On this last line, we use the property $\nabla A_{\eta_k} (\eta_k) =
\mathbb{E}_{q(z_k|\eta_k)} [ t(z_k) ]$, a fact that holds for the exponential
family.  Finally, let's take the gradient of this expression and set it to
zero to solve for $\eta_k$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0
  &amp;amp; = \nabla_{\eta_k} L(\eta_k) \
  &amp;amp; = \left( \nabla_{\eta_k}^2 A(\eta_k) \right)
    \left(
      \mathbb{E}&lt;em -k&gt;{q(z&lt;/em&gt;|\eta_{-k})} \left[ g(z_{-k},x) \right]
      - \eta_k
    \right) \
  \eta_k
  &amp;amp; = \mathbb{E}&lt;em -k&gt;{q(z&lt;/em&gt;|\eta_{-k})} \left[ g(z_{-k},x) \right] \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;So what is this expression? It says that in order to update $\eta_k$, we need
to be able to evaluate the expected parameters for $P(z_k|z_{-k},x)$
under our approximation to the posterior $q(z_{-k}|\eta_{-k})$. How do we do
this? Let's take a look at an example to make this concrete.&lt;/p&gt;
&lt;h1&gt;&lt;a name="example" href="#example"&gt;Example&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;For this part, let's take a look at the model defined by Latent Dirichlet
Allocation (LDA),&lt;/p&gt;
&lt;div class="img-center" style="max-width: 200px;"&gt;
  &lt;img src="/assets/img/variational_inference/graphical_model.png"&gt;&lt;/img&gt;
&lt;/div&gt;

&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; document-topic prior $\alpha$, topic-word prior $\beta$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For each topic $k = 1 \ldots K$&lt;ol&gt;
&lt;li&gt;Sample topic-word parameters $\phi_{k} \sim \text{Dirichlet}(\beta)$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;For each document $i = 1 \ldots M$&lt;ol&gt;
&lt;li&gt;Sample document-topic parameters $\theta_i \sim \text{Dirichlet}(\alpha)$&lt;/li&gt;
&lt;li&gt;For each token $j = 1 \ldots N$&lt;ol&gt;
&lt;li&gt;Sample topic $z_{i,j} \sim \text{Categorical}(\theta_i)$&lt;/li&gt;
&lt;li&gt;Sample word $x_{i,j} \sim \text{Categorical}(\phi_{z_{i,j}})$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;First, a short word on notation. In the following I'll occasionally drop
indices to denote all variables with the same prefix. For example, when I say
$\theta$, I mean $\theta_{1:M}$, and when I say $z_i$, I mean $z_{i,1:N}$.
I'll also refer to $q(\theta_i|\eta_i)$ as "the variational distribution
corresponding to $P(\theta_i|\alpha,\theta_{-i},z,x)$", and similarly for
$q(z_{i,j}|\gamma_{i,j})$. Oh, and $z_{-i}$ means all $z_j$ with $j \ne i$, and
$\theta_{1:M}$ means $(\theta_1, \ldots \theta_M)$.&lt;/p&gt;
&lt;p&gt;Our goal now is to derive the posterior distribution over the latent
variables, given the hyperparameters and the observed variables,
$P(\theta, z, \phi| \alpha, x, \beta)$. We'll approximate it via the mean field
distribution,&lt;/p&gt;
&lt;p&gt;$$
  q(\theta,z,\phi | \eta,\gamma,\psi) = \left(
    \prod_{i} q(\theta_i | \eta_i) \prod_{j} q(z_{i,j} | \gamma_{i,j})
  \right) \left(
    \prod_{k} q(\phi_k | \psi_k)
  \right)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Outline&lt;/strong&gt; Deriving the update rules for Variational Inference requires we
do 3 things. First, we must derive the posterior distribution for each hidden
variable given all other variables, hidden and observed. This distribution must
lie in the exponential family, and the corresponding variational distribution for
that variable must be of the same form. For example, if
$P(\theta_i|\alpha,\theta_{-i},z,x)$ is a Dirichlet distribution, then
$q(\theta_i|\eta_i)$ must also be Dirichlet.&lt;/p&gt;
&lt;p&gt;Second, we need to derive, for each hidden variable, the function that gives
us the parameters for the posterior distribution over that variable given all
others, hidden and observed.&lt;/p&gt;
&lt;p&gt;Finally, we'll need to plug the functions we just derived into an expectation
with respect to the mean field distribution. If we are able to calculate this
expectation for a particular hidden variable, we can use it to update the
matching variational distribution's parameters.&lt;/p&gt;
&lt;p&gt;In the following, I'll show you how to derive the update for the variational
distribution of one of the hidden variables in LDA, $\theta_i$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; First, we must show that the posterior distribution over each
individual hidden variable lies in the exponential family. This is not always
the case, but for models that employ &lt;a href="http://lesswrong.com/lw/5sn/the_joys_of_conjugate_priors/"&gt;conjugate priors&lt;/a&gt;, this
can be guaranteed. A conjugate prior dictates that if $P(z)$ is a conjugate
prior to $P(x|z)$, then $P(z|x)$ is in the same family as $P(z)$ is. This is
the case for Dirichlet/Categorical distributions such as those that appear in
LDA. In other words, $P(\theta_i|\alpha,\theta_{-i},z,x) =
P(\theta_i|\alpha,z_{i})$ (by conditional independence) is a Dirichlet
distribution because $P(\theta_i|\alpha)$ is Dirichlet and
$P(z_{i,j}|\theta_i)$ is Categorical.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Next, we derive the parameter function for each hidden variable
as a function of all other variables, hidden and observed. Let's see how this
plays out for the Dirichlet distribution,&lt;/p&gt;
&lt;p&gt;The exponential family form of the Dirichlet distribution is,&lt;/p&gt;
&lt;p&gt;$$
  P(\theta_i|\alpha) = \exp \left(
    \sum_{k} (\alpha_k - 1) \log (\theta_i)&lt;em k&gt;k
    - \log \left(
      \frac{ \prod&lt;/em&gt; \Gamma(\alpha_k) }{ \Gamma( \sum_k \alpha_k ) }
    \right)
  \right)
$$&lt;/p&gt;
&lt;p&gt;The exponential family form of a Categorical distribution is,&lt;/p&gt;
&lt;p&gt;$$
  P(z_{i,j}|\theta_i) = \exp \left(
    \sum_{k} 1[z_{i,j} = k] \log (\theta_i)_k
  \right)
$$&lt;/p&gt;
&lt;p&gt;Thus, the posterior distribution for $\theta_i$ is proportional to,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  P(\theta_i|\alpha,z_{i})
  &amp;amp; \propto P(\theta_i, z_i | \alpha) \
  &amp;amp; = P(\theta_i|\alpha) \prod_{j} P(z_{i,j}|\theta_i) \
  &amp;amp; = \exp \left(
    \sum_{k} \left(\alpha_k - 1 + \sum_{j} 1[z_{i,j} = k] \right) \log (\theta_i)_k
  \right)
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Notice how $\alpha_k - 1$ changed to $\alpha_k - 1 + \sum_{j} 1[z_{i,j} = k]$?
These are the parameters for our posterior distribution over $\theta_i$. Thus,
the parameters for $P(\theta_i|\alpha,z_i)$ are,&lt;/p&gt;
&lt;p&gt;$$
  g_{\theta_i}(\alpha,z_{i}) = \begin{pmatrix}
    \alpha_1 - 1 + \sum_{j} 1[z_{i,j} = 1] \
    \alpha_2 - 1 + \sum_{j} 1[z_{i,j} = 2] \
    \vdots \
  \end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; Now we need to take the expectation over the parameter
function we just derived with respect to the mean field distribution. For
$g_{\theta_i}(\alpha, z_i)$, this is particularly easy -- all the indicators
simply turn into probabilities. Thus the update for $q(\theta_i|\eta_i)$ is,&lt;/p&gt;
&lt;p&gt;$$
  \eta_i
  = E_{q(z_{i}|\gamma_i)} [ g_{\theta_i}(\alpha, z_i) ]
  = \begin{pmatrix}
    \alpha_1 - 1 + \sum_{j} q(z_{i,j} = 1 | \gamma_{i,j}) \
    \alpha_2 - 1 + \sum_{j} q(z_{i,j} = 2 | \gamma_{i,j}) \
    \vdots \
  \end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt; We've now derived the update rule for one of the components of
the mean field distribution, $q(\theta_i|\eta_i)$. Left unexplained here is the
updates for $q(z_{i,j}|\gamma_{i,j})$ and $q(\phi_k|\psi_k)$, though you can
find a (messier) derivation in the original paper on &lt;a href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf"&gt;Latent Dirichlet
Allocation&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a name="aside" href="#aside"&gt;Aside: Coordinate Ascent is Gradient Ascent&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Coordinate Ascent on the Mean Field Approximation is the "traditional" way
one does Variational Inference, but Coordinate Ascent is far from the only
optimization method we know. What if we wanted to do Gradient Ascent? What
would an update look like then?&lt;/p&gt;
&lt;p&gt;It ends up that for the Variational Inference objective, Coordinate Ascent
&lt;em&gt;is&lt;/em&gt; Gradient Ascent with step size equal to 1. Actually, that's only half true
-- it's Gradient Ascent using a "Natural Gradient" (rather than the usual
gradient defined with respect to $||\cdot||_2^2$).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gradient Ascent&lt;/strong&gt; First, recall the Gradient Ascent update for $\eta_k$ (we
use the definition of $\nabla_{\eta_k} L(\eta_k)$ we found when deriving the
Coordinate Ascent update).&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \eta_k^{(t+1)}
  &amp;amp; = \eta_k^{(t)} + \alpha^{(t)} \nabla_{\eta_k} L(\eta_k^{(t)}) \
  &amp;amp; = \eta_k^{(t)} + \alpha^{(t)} \left[
      \left( \nabla_{\eta_k}^2 A(\eta_k^{(t)}) \right)
      \left(
        \mathbb{E}&lt;em -k&gt;{q(z&lt;/em&gt;|\eta_{-k})} \left[ g(z_{-k},x) \right]
        - \eta_k^{(t)}
      \right)
    \right] \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Natural Gradient&lt;/strong&gt; Hmm, that $\nabla_{\eta_k}^2 A(\eta_k^{(t)})$ term is a
bit of a nuisance. Is there any way to make it just go away? In fact, we can --
by replacing the concept of a gradient with a "natural gradient". Whereas a
regular gradient is the direction of steepest ascent with respect to Euclidean
distance, a natural gradient is a direction of steepest ascent with respect to
a function (in particular, one we want to minimize). The intuition is that for
a given function, some input coordinates might be more important than others,
and this should be taken into account when considering how far away 2 points
are.&lt;/p&gt;
&lt;p&gt;So what do I mean "a direction of steepest ascent"?  Let's look at the
gradient of a function as the solution to the following problem as $\epsilon
\rightarrow 0$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \nabla_{\eta_k} L(\eta_k)
  = &amp;amp; \arg\min_{d \eta_k} L(\eta_k + d \eta_k) \
    &amp;amp; \text{s.t.} \quad   ||d \eta_k||_2^2 &amp;lt; \epsilon
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;A natural gradient with respect to $L(\eta_k)$ is defined much the same way,
but with $D_{E}(x,y) = || x-y ||_2^2$ replaced with another squared metric.
In our case, we're going to use the symmetrized KL divergence,&lt;/p&gt;
&lt;p&gt;$$
  D_{KL}(\eta_k, \eta_k') = \text{KL} \left[ q(z_k|\eta_k) || q(z_k|\eta_k') \right]
                          + \text{KL} \left[ q(z_k|\eta_k') || q(z_k|\eta_k) \right]
$$&lt;/p&gt;
&lt;p&gt;Swapping the squared Euclidean metric $D_{E}$ with $D_{KL}$, we have a
definition for a "Natural Gradient",&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \hat{\nabla}&lt;em _eta_k="\eta_k" d&gt;{\eta_k} L(\eta_k)
  = &amp;amp; \arg\min&lt;/em&gt;   L(\eta_k + d \eta_k) \
    &amp;amp; \text{s.t.} \quad D_{KL}(\eta_k, \eta_k + d \eta_k) &amp;lt; \epsilon
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;While at first the gradient and natural gradient may seem difficult to
relate, suppose that $D_{KL}(\eta_k, \eta_k + d \eta_k) = d \eta_k^T
G(\eta_k) d \eta_k$ for some matrix $G(\eta_k)$. Then by plugging this into the
previous optimization problem, replacing $L(\eta_k + d \eta_k)$ by its first
order Taylor approximation (which holds when $\epsilon$ is small), and
requiring the derivative of the problem's Lagrangian be equal to 0, we see
that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0
  &amp;amp; = \nabla_{d \eta_k} \left[
      L(\eta_k + d \eta_k) + \lambda ( d \eta_k G(\eta_k) d \eta_k - \epsilon)
    \right] \
  &amp;amp; \approx \nabla_{d \eta_k} \left[
      L(\eta_k) + \nabla_{\eta_k} L(\eta_k)^T (\eta_k + d \eta_k - \eta_k) + \lambda ( d \eta_k G(\eta_k) d \eta_k - \epsilon)
    \right] \
  &amp;amp; = \nabla_{\eta_k} L(\eta_k) + 2 \lambda G(\eta_k) d \eta_k \
  d \eta_k
  &amp;amp; \propto G(\eta_k)^{-1} \nabla_{\eta_k} L(\eta_k)
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;As $\epsilon \rightarrow 0$, $d \eta_k$ becomes $\hat{\nabla}&lt;em _eta_k="\eta_k"&gt;{\eta_k}
L(\eta_k)$, resulting in $\hat{\nabla}&lt;/em&gt; L(\eta_k) \propto
G(\eta_k)^{-1} \nabla_{\eta_k} L(\eta_k)$. In other words, we can obtain
$\hat{\nabla}_{\eta_k} L(\eta_k)$ easily if we can simply compute $G(\eta_k)$.
Now let's derive $G(\eta_k)$.&lt;/p&gt;
&lt;p&gt;First, let's take the first-order Taylor approximation to $q(z|\eta_k + d
\eta_k)$ and its $\log$ about $\eta_k$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  q(z_k|\eta_k + d \eta_k)
  &amp;amp; \approx q(z_k|\eta_k) + (\nabla q(z_k|\eta_k))^T (\eta_k + d \eta_k - \eta_k) \
  &amp;amp; = q(z_k|\eta_k) + q(z_k|\eta_k) (\nabla \log q(z_k|\eta_k))^T d \eta_k \
  \log q(z_k|\eta_k + d \eta_k)
  &amp;amp; \approx \log q(z_k|\eta_k) + (\nabla \log q(z_k|\eta_k))^T (\eta_k + d \eta_k - \eta_k) \
  &amp;amp; = \log q(z_k|\eta_k) + (\nabla \log q(z_k|\eta_k))^T d \eta_k \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Plugging this back into the definition of $D_{KL}$ and cancelling out terms, we
get a nice expression for $G(\eta_k)$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  D_{KL}(\eta, \eta')
  &amp;amp; = \text{KL} \left[ q(z_k|\eta_k) || q(z_k|\eta_k + d\eta_k) \right] + \text{KL} \left[ q(z_k|\eta_k + d\eta_k) || q(z_k|\eta_k) \right] \
  &amp;amp; = \sum_{z} q(z|\eta_k) \log \frac{ q(z|\eta_k) }{ q(z|\eta_k+d\eta_k) }
      + \sum_{z} q(z|\eta_k+d\eta_k) \log \frac{ q(z|\eta_k+d\eta_k) }{ q(z|\eta_k) } \
  &amp;amp; = \sum_{z} \left[ q(z|\eta_k) - q(z|\eta_k+d\eta_k) \right]
               \left[ \log q(z|\eta_k) - \log q(z|\eta_k+d\eta_k) \right] \
  &amp;amp; \approx \sum_{z}
      \left[ q(z|\eta_k) - q(z_k|\eta_k) - q(z_k|\eta_k)(\nabla \log q(z_k|\eta_k))^T d \eta_k \right] \
      &amp;amp; \qquad \quad \times \left[ \log q(z|\eta_k) - \log q(z_k|\eta_k) - (\nabla \log q(z_k|\eta_k))^T d \eta_k \right] \
  &amp;amp; = \sum_{z}
      \left[ - q(z_k|\eta_k) (\nabla \log q(z_k|\eta_k))^T d \eta_k \right]
      \left[ - (\nabla \log q(z_k|\eta_k))^T d \eta_k \right] \
  &amp;amp; = d \eta_k^T \mathbb{E}_{q(z|\eta_k)} \left[ (\nabla \log q(z_k|\eta_k)) (\nabla \log q(z_k|\eta_k))^T \right] d \eta_k \
  &amp;amp; = d \eta_k^T G(\eta_k) d \eta_k \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Looking at the expression for $G(\eta_k)$, we can see that it is in fact the
&lt;a href="http://en.wikipedia.org/wiki/Fisher_information"&gt;Fisher Information Matrix&lt;/a&gt;. Since we already assumed that
$q(z_k|\eta_k)$ is in the exponential family, let's plug in its exponential
form $q(z_k|\eta_k) = h(z_k) \exp \left( \eta_k^T t(z_k) - A(\eta_k) \right)$
and apply the $\log$ to see that we are simply taking the covariance matrix of
the sufficient statistics $t(z_k)$. For exponential families, this also happens
to be the second derivative of the log normalizing constant,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  G(\eta_k)
  &amp;amp;= \mathbb{E}&lt;em _eta_k="\eta_k"&gt;{q(z_k|\eta_k)} \left[
      \left( \nabla&lt;/em&gt; \log q(z_k|\eta_k) \right) \left( \nabla_{\eta_k} \log q(z_k|\eta_k) \right)^T
    \right] \
  &amp;amp;= \mathbb{E}&lt;em _eta_k="\eta_k"&gt;{q(z_k|\eta_k)} \left[
      \left( t(z_k) - \nabla&lt;/em&gt; A(\eta_k) \right) \left( t(z_k) - \nabla_{\eta_k} A(\eta_k) \right)^T
    \right] \
  &amp;amp;= \mathbb{E}&lt;em q_z_k_eta_k_="q(z_k|\eta_k)"&gt;{q(z_k|\eta_k)} \left[
      \left( t(z_k) - \mathbb{E}&lt;/em&gt; [t(z_k)] \right) \left( t(z_k) - \mathbb{E}&lt;em _eta_k="\eta_k"&gt;{q(z_k|\eta_k)} [t(z_k)] \right)^T
    \right] \
  &amp;amp;= \nabla&lt;/em&gt;^2 A(\eta_k) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Finally, let's define a Gradient Ascent algorithm in terms of the Natural
Gradient, rather than the regular gradient,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \eta_k^{(t+1)}
  &amp;amp; = \eta_k^{(t)} + \alpha^{(t)} \hat{\nabla}&lt;em _eta_k="\eta_k"&gt;{\eta_k} L(\eta_k^{(t)}) \
  &amp;amp; = \eta_k^{(t)} + \alpha^{(t)} G(\eta_k^{(t)})^{-1} \nabla&lt;/em&gt; L(\eta_k^{(t)}) \
  &amp;amp; = \eta_k^{(t)} + \alpha^{(t)}
    \left( \nabla_{\eta_k}^2 A(\eta_k^{(t)}) \right)^{-1}
    \left[
      \left( \nabla_{\eta_k}^2 A(\eta_k^{(t)}) \right)
      \left(
        \mathbb{E}&lt;em -k&gt;{q(z&lt;/em&gt;|\eta_{-k})} \left[ g(z_{-k},x) \right]
        - \eta_k^{(t)}
      \right)
    \right] \
  &amp;amp; = (1 - \alpha^{(t)}) \eta_k^{(t)}
    + \alpha^{(t)} \mathbb{E}&lt;em -k&gt;{q(z&lt;/em&gt;|\eta_{-k})} \left[ g(z_{-k},x) \right] \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Look at that -- $G(\eta_k^{(t)})^{-1} = (\nabla_{\eta_k}^2 A(\eta_k))^{-1}$
perfectly cancels out $\nabla_{\eta_k}^2 A(\eta_k)$, and we're left with a
linear combination of the old parameters and the parameters Coordinate Ascent
would recommend. If $\alpha^{(t)} = 1$, then we just get the old Coordinate
Ascent update!&lt;/p&gt;
&lt;h1&gt;&lt;a name="extensions" href="#extensions"&gt;Extensions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The Variational Inference method I described here, while general in concept,
can only easily be applied to a very particular class models -- ones where
$P(z_k | z_{-k}, x)$ is in the exponential family. This more or less means that
$z_k$ be a discrete variable or that $P(z_k)$ be a conjugate prior to all other
variables depending on it.&lt;/p&gt;
&lt;p&gt;In addition, we restricted $q(z | \eta)$ to be a mean field approximation,
meaning that each variable is independent with its own distribution $q(z_k |
\eta_k)$. This approximation has no hope of representing any interactions
between variables, and perhaps surprisingly $q(z_k|\eta_k)$ does &lt;em&gt;not match the
marginal distribution over $z_k$ at all.&lt;/em&gt;  This is a common source of confusion
for first-time users, and makes debugging Variational Inference algorithms
rather difficult.&lt;/p&gt;
&lt;p&gt;Third, the Coordinate Ascent algorithm described is not necessarily quick. I
explained how Coordinate Ascent is really just Gradient Ascent on the natural
gradient, so it's easy to ask what other methods we might be able to apply.&lt;/p&gt;
&lt;p&gt;Here are a handful of papers that extend Variational Inference to faster
optimization methods, different variational distribution, and non-conjugate
models.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://books.nips.cc/papers/files/nips25/NIPS2012_1314.pdf"&gt;"Fast Variational Inference in the Conjugate Exponential
Family"&lt;/a&gt; -- Conjugate Gradient applied to the Marginalized
Variational Bound. Shows that the Marginalized Variational Bound upper bounds the
typical Variational Bound and that the former also has better curvature. That
means second-order optimizers like Conjugate Gradient can take larger steps and
render better performance.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1206.6679"&gt;"Fixed-Form Variational Posterior Approximation through Stochastic Linear
Regression"&lt;/a&gt; -- fits a (potentially) non-decomposable
exponential family distribution via Linear Regression. Involves looking at KL
divergence between unnormalized variational distribution and joint distribution
of model, taking derivative with respect to variational distribution's
parameters and setting to 0, then solving for the parameters. Can be applied to
non-conjugate models due to sampling for estimating expectations.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf"&gt;"Variational Inference in Nonconjugate Models"&lt;/a&gt; -- Getting
away from conjugate priors via Laplace and the Delta Method.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The seminal work on the Natural Gradient is due to Shunichi Amari's &lt;a href="http://www.maths.tcd.ie/~mnl/store/Amari1998a.pdf"&gt;"Natural
Gradient Works Efficiently in Learning"&lt;/a&gt;. The derivation for the natural
gradient is Theorem 1. Thanks to &lt;a href="https://twitter.com/atpassos_ml"&gt;Alexandre Passos&lt;/a&gt; for suggesting
this and giving a short-hand intuition of the proof.&lt;/p&gt;
&lt;p&gt;The derivation for Variational Inference and the correspondence between
Coordinate Ascent and Gradient Ascent is based on the introduction to Matt
Hoffman et al.'s &lt;a href="http://arxiv.org/abs/1206.7051"&gt;"Stochastic Variational Inference"&lt;/a&gt;.&lt;/p&gt;</content><category term="optimization"></category><category term="inference"></category><category term="bayesian"></category></entry><entry><title>Accelerated Proximal Gradient Descent</title><link href="/blog/accelerated-proximal-gradient-descent.html" rel="alternate"></link><published>2013-04-25T00:00:00-07:00</published><updated>2013-04-25T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-04-25:/blog/accelerated-proximal-gradient-descent.html</id><summary type="html">&lt;p&gt;$\def\prox{\text{prox}}$
  In a &lt;a href="/blog/proximal-gradient-descent.html"&gt;previous post&lt;/a&gt;, I presented Proximal Gradient, a
method for bypassing the $O(1 / \epsilon^2)$ convergence rate of Subgradient
Descent.  This method relied on assuming that the objective function could be
expressed as the sum of 2 functions, $g(x)$ and $h(x)$, with …&lt;/p&gt;</summary><content type="html">&lt;p&gt;$\def\prox{\text{prox}}$
  In a &lt;a href="/blog/proximal-gradient-descent.html"&gt;previous post&lt;/a&gt;, I presented Proximal Gradient, a
method for bypassing the $O(1 / \epsilon^2)$ convergence rate of Subgradient
Descent.  This method relied on assuming that the objective function could be
expressed as the sum of 2 functions, $g(x)$ and $h(x)$, with $g$ being
differentiable and $h$ having an easy to compute &lt;a href="/blog/proximal-gradient-descent.html#intuition"&gt;$\prox$
function&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;$$
  \prox_{\alpha h}(x) = \arg\min_{y} \alpha h(y) + \frac{1}{2}||y - x||_2^2
$$&lt;/p&gt;
&lt;p&gt;In the &lt;a href="/blog/accelerated-gradient-descent.html"&gt;post before that&lt;/a&gt;, I presented Accelerated
Gradient Descent, a method that outperforms Gradient Descent while making the
exact same assumptions. It is then natural to ask, "Can we combine Accelerated
Gradient Descent and Proximal Gradient to obtain a new algorithm?"  Well if we
couldn't, why the hell would I be writing about something called "Accelerated
Proximal Gradient."  C'mon people, work with me.  Now let's get on with it!&lt;/p&gt;
&lt;h1&gt;&lt;a name="implementation" href="#implementation"&gt;How does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;As you might guess, the setup is precisely the same as Proximal Gradient. Let
our objective be expressed as the sum of 2 functions,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x} g(x) + h(x)
$$&lt;/p&gt;
&lt;p&gt;where $g$ is differentiable and $h$ is "simple" in the sense that its $\prox$
function can cheaply be computed. Given that, the algorithm is pretty much what
you would expect from the lovechild of Proximal Gradient and Accelerated
Gradient Descent,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: initial iterate $x^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let $y^{(0)} = x^{(0)}$&lt;/li&gt;
&lt;li&gt;For $t = 1, 2, \ldots$&lt;ol&gt;
&lt;li&gt;Let $x^{(t)} = \prox_{\alpha^{(t)} h} (y^{(t-1)} - \alpha^{(t)} \nabla f(y^{(t-1)}) )$&lt;/li&gt;
&lt;li&gt;if converged, return $x^{(t)}$&lt;/li&gt;
&lt;li&gt;Let $y^{(t)} = x^{(t)} + \frac{t-1}{t+2} (x^{(t)} - x^{(t-1)})$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h1&gt;&lt;a name="example" href="#example"&gt;A Small Example&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;To illustrate Accelerated Proximal Gradient, I'll use the same objective function as I did in illustrating Proximal Gradient Descent. Namely,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x} \, \log(1 + \exp(-2x)) + ||x||_1
$$&lt;/p&gt;
&lt;p&gt;which has the following gradient for $g(x) = \log(1+\exp(-2x))$ and $\prox$
operator for $h(x) = ||x||_1$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \nabla g(x) &amp;amp;= \frac{1}{1 + \exp(-2x)} \left( \exp(-2x) \right) (-2) \
  \prox_{\alpha h}(x) &amp;amp; = \text{sign}(x) \max(0, \text{abs}(x) - \alpha) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;As before, we employ Backtracking Line Search to select the step size. In
this example, regular Proximal Gradient seems to beat out Accelerated
Proximal Gradient, but rest assured this is an artifact of the tiny problem
size.&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/accelerated_proximal_gradient_descent/convergence.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows how quickly the objective function decreases as the
    number of iterations increases. Notice how the objective function doesn't
    necessarily decrease at each step.
  &lt;/span&gt;
&lt;/div&gt;

&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/accelerated_proximal_gradient_descent/iterates.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number.
  &lt;/span&gt;
&lt;/div&gt;

&lt;h1&gt;&lt;a name="proof" href="#proof"&gt;Why does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;For the proof of Accelerated Proximal Gradient, we'll make the same
assumptions we did in Proximal Gradient. Namely,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$g(x)$ is convex, differentiable, and finite for all $x$&lt;/li&gt;
&lt;li&gt;a finite solution $x^{*}$ exists&lt;/li&gt;
&lt;li&gt;$\nabla g(x)$ is Lipschitz continuous with constant $L$. That is, there must
   be an $L$ such that,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
  || \nabla g(x) - \nabla g(y) ||_2 \le L || x - y ||_2 \qquad \forall x,y
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$h(x)$ is convex&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Proof Outline&lt;/strong&gt; In the same way that the proof for Proximal Gradient
largely follows the proof for regular Gradient Descent, the proof for
Accelerated Proximal Gradient follows the proof for Accelerated Gradient
Descent. Our goal is to prove a statement of the form,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
    (g+h)(x^{+})
    \le (1-\theta) (g+h)(x) + \theta (g+h)(x^{&lt;/em&gt;}) + \frac{\theta^2}{2 \alpha^{+}} \left(
        ||v - x^{&lt;em&gt;}||_2^2 - ||v^{+} - x^{&lt;/em&gt;}||_2^2
      \right) \
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Once we achieve this, the proof follows that of Accelerated Gradient with $f
\rightarrow g+h$ from Step 2 onwards.&lt;/p&gt;
&lt;p&gt;How will we do this? As with Accelerated Gradient, we define a new set of
iterates $v^{(t)}$ in terms of $x^{(t)}$ and $x^{(t-1)}$ and then define
$y^{(t)}$ in terms of $v^{(t)}$ and $x^{(t)}$. We then exploit the Lipschitz bound
on $g$ and a particular subgradient bound on $h$ to establish an upper bound
on $(g+h)(x^{(t)})$. Finally, through algebraic manipulations we show the
equation presented above, and we can simply copy-paste the Accelerated Gradient
Descent proof to completion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; Define a new set of iterates $v^{(t)}$. As with Accelerated
Gradient, we define a new set of iterates $v^{(t)}$ and a particular
$\theta^{(t)}$ as follows,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  v^{(t)}
  &amp;amp; = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)}
    = x^{(t-1)} + \frac{1}{\theta^{(t)}} (x^{(t)} - x^{(t-1)}) \
  \theta^{(t)}
  &amp;amp; = \frac{2}{t+1} \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;This definition also allows us to redefine $y^{(t)}$,&lt;/p&gt;
&lt;p&gt;$$
  y^{(t)}
  = (1 - \theta^{(t)}) x^{(t)} + \theta^{(t)} v^{(t)} \
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Use the Lipschitz property of $g$ and subgradient property of $h$
to upper bound $(g+h)(x^{(t+1)})$.  Let's begin by defining $x^{+} \triangleq
x^{(t)}$, $x \triangleq x^{(t-1)}$, $y \triangleq y^{(t-1)}$, $\theta
\triangleq \theta^{(t-1)}$, $v^{+} \triangleq v^{(t)}$, and $v \triangleq
v^{(t-1)}$.  From the Lipschitz property of $g$, we immediately get,&lt;/p&gt;
&lt;p&gt;$$
  g(x^{+}) \le g(y) + \nabla g(y)^T (x^{+} - y) + \frac{L}{2} ||x^{+} - y||_2^2
$$&lt;/p&gt;
&lt;p&gt;Let's immediately assume $\alpha \le \frac{1}{L}$, so we can replace $\frac{L}{2}$ with $\frac{1}{2 \alpha}$. Now let's derive the subgradient property of $h$ we need.  Recall the subgradient definition,&lt;/p&gt;
&lt;p&gt;$$
  h(z) \ge h(\tilde{x}) + G^T (z-\tilde{x}) \qquad G \in \partial h(\tilde{x})
$$&lt;/p&gt;
&lt;p&gt;Now let $x^{+} = \prox_{\alpha h}(\tilde{x}) = \arg\min_{w} \alpha h(w) + \frac{1}{2}||w - \tilde{x}||_2^2$.  According to the KKT conditions, 0 must be in the subdifferential of $\alpha h(x^{+}) + \frac{1}{2} || x^{+} - \tilde{x} ||_2^2$.  Plugging this in, we see that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0 &amp;amp; \in \alpha \partial h(x^{+}) + (x^{+} - \tilde{x}) \
  \frac{1}{\alpha} \left( \tilde{x} - x^{+} \right) &amp;amp; \in \partial h(x^{+})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;We now have a subgradient for $h(x^{+})$.  Plugging this back into the
subgradient condition with $\tilde{x} \rightarrow x^{+}$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  h(z)
  &amp;amp;\ge h(x^{+}) + \frac{1}{\alpha} \left( \tilde{x} - x^{+} \right)^T(z - x^{+}) \
  h(z) + \frac{1}{\alpha} \left( x^{+} - \tilde{x} \right)^T (z - x^{+})
  &amp;amp;\ge h(x^{+}) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Finally, substitute $\tilde{x} = y - \alpha \nabla g(y)$ to obtain our
desired upper bound on $h(x^{+})$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  h(x^{+})
  &amp;amp; \le h(z) + \frac{1}{\alpha} \left( x^{+} - \left(y - \alpha \nabla g(y) \right) \right)^T (z - x^{+}) \
  &amp;amp; = h(z) + \nabla g(y)^T (z - x^{+}) + \frac{1}{\alpha} ( x^{+} - y )^T (z - x^{+})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Nice. Now add the Lipschitz bound on $g$ and the subgradient bound on
$h$ to obtain an upper bound on $(g+h)(x^{+})$, then invoke convexity on $g(z)
\ge g(y) + \nabla g(y)^T (z-y)$ to get rid of the linear term involving $\nabla
g(y)$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  (g+h)(x^{+})
  &amp;amp; \le g(y) + h(z) + \nabla g(y)^T (z-y) + \frac{1}{\alpha} (x^{+} - y)^T (z - x^{+}) + \frac{1}{2\alpha} ||x^{+} - y||_2^2 \
  &amp;amp; \le g(z) + h(z) + \frac{1}{\alpha} (x^{+} - y)^T (z - x^{+}) + \frac{1}{2\alpha} ||x^{+} - y||_2^2
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; Use the previous upper bound to obtain the equation necessary for
invoking Accelerated Gradient Descent's proof. The core of this is to
manipulate and bound the following statement,&lt;/p&gt;
&lt;p&gt;$$
  (g+h)(x^{+}) - \theta (g+h)(x^{*}) - (1-\theta) (g+h)(x)
$$&lt;/p&gt;
&lt;p&gt;First, upper bound $-(g+h)(x^{&lt;em&gt;})$ and $-(g+h)(x)$ with $z = x^{&lt;/em&gt;}$ and $z =
x^{+}$ using the result of Step 2, then add zero and factor the quadratic,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; (g+h)(x^{+}) - \theta (g+h)(x^{&lt;/em&gt;}) - (1-\theta) (g+h)(x) \
  &amp;amp; \le (g+h)(x^{+}) + \theta \left(
      - (g+h)(x^{+}) + \frac{1}{\alpha} (x^{+} - y)^T (x^{&lt;em&gt;} - x^{+}) + \frac{1}{2 \alpha} ||x^{+} - y||_2^2
    \right) \
  &amp;amp; \qquad + (1-\theta) \left(
      - (g+h)(x    ) + \frac{1}{\alpha} (x^{+} - y)^T (x     - x^{+}) + \frac{1}{2 \alpha} ||x     - y||_2^2
    \right) \
  &amp;amp; = \frac{1}{\alpha} (x^{+} - y)^T ( \theta x^{&lt;/em&gt;} + (1-\theta) x - x^{+} ) + \frac{1}{2 \alpha} ||x^{+} - y||_2^2 \pm \frac{1}{2 \alpha} ||\theta x^{&lt;em&gt;} + (1-\theta) x - x^{+} ||_2^2 \
  &amp;amp; = \frac{1}{2 \alpha} \left(
    ||x^{+} - y + \theta x^{&lt;/em&gt;} + (1 - \theta) x - x^{+}||_2^2
    - ||\theta x^{&lt;em&gt;} + (1 - \theta) x - x^{+}||_2^2
  \right) \
  &amp;amp; = \frac{1}{2 \alpha} \left(
    ||y - \theta x^{&lt;/em&gt;} - (1 - \theta) x||_2^2
    - || x^{+} - \theta x^{&lt;em&gt;} - (1 - \theta) x||_2^2
  \right) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Finally, use $y = (1 - \theta) x + \theta v$ to get $y - (1-\theta) x =
\theta v$ and then $v^{+} = x + \frac{1}{\theta} ( x^{+} - x )$ to obtain
$\theta v^{+} = x^{+} - (1-\theta) x$. Substituting these in,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; (g+h)(x^{+}) - \theta (g+h)(x^{&lt;/em&gt;}) - (1-\theta) (g+h)(x) \
  &amp;amp; \le \frac{1}{2 \alpha} \left(
    ||\theta v - \theta x^{&lt;em&gt;}||_2^2 - || \theta v^{+} - \theta x^{&lt;/em&gt;} ||_2^2
  \right) \
  &amp;amp; = \frac{\theta^2}{2 \alpha} \left(
    || v - x^{&lt;em&gt;}||_2^2 - || v^{+} - x^{&lt;/em&gt;} ||_2^2
  \right) \
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Which was our original goal.  We then follow the proof for Accelerated
Gradient Descent with $f \rightarrow g + h$ starting from Step 2 to obtain
the desired rate of convergence, $O(1 / \sqrt{\epsilon})$.&lt;/p&gt;
&lt;p&gt;As a final note, you'll notice that in this proof $\theta^{(t)} =
\frac{2}{t+1}$, but in the original Accelerated Gradient proof $\theta^{(t)}
= \frac{2}{t+2}$. This ends up no mattering, as the only property we need being
$\frac{1 - \theta^{(t)}}{ (\theta^{(t)})^2 } \le \frac{1}{ (\theta^{(t)})^2 }$,
which holds for either definition.&lt;/p&gt;
&lt;h1&gt;&lt;a name="usage" href="#usage"&gt;When should I use it?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;As with &lt;a href="/blog/accelerated-gradient-descent.html#usage"&gt;Accelerated Gradient&lt;/a&gt;, the algorithm
works well &lt;em&gt;as long as you get the step size right&lt;/em&gt;. That means Backtracking
Line Search is an absolute must if you don't know $g$'s Lipschitz constant
analytically.  If Line Search is possible, you can only gain over Proximal
Gradient by employing Accelerated Proximal Gradient; with that said, test a
Proximal Gradient algorithm first, and advance to Accelerated Proximal Gradient
only if you're sure you need the faster convergence rate.&lt;/p&gt;
&lt;h1&gt;&lt;a name="extensions" href="#extensions"&gt;Extensions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Step Size&lt;/strong&gt; As with Accelerated Gradient, getting the correct step size is
of utmost importance. If $\alpha^{(t)} &amp;gt; \frac{1}{L}$, &lt;em&gt;the algorithm will
diverge&lt;/em&gt;. With that said, Backtracking Line Search will guarantee convergence.
You can find an implementation in &lt;a href="/blog/proximal-gradient-descent.html#line_search"&gt;my previous post on Proximal
Gradient&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Proof of convergence&lt;/strong&gt; The proof of convergence is taken from Lieven
Vandenberghe's fantastic &lt;a href="http://www.ee.ucla.edu/~vandenbe/236C/lectures/fgrad.pdf"&gt;EE236c slides&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a name="reference-impl" href="#reference-impl"&gt;Reference Implementation&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;accelerated_proximal_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                  &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Proximal Gradient Descent&lt;/span&gt;

&lt;span class="sd"&gt;  Parameters&lt;/span&gt;
&lt;span class="sd"&gt;  ----------&lt;/span&gt;
&lt;span class="sd"&gt;  g_gradient : function&lt;/span&gt;
&lt;span class="sd"&gt;      Compute the gradient of `g(x)`&lt;/span&gt;
&lt;span class="sd"&gt;  h_prox : function&lt;/span&gt;
&lt;span class="sd"&gt;      Compute prox operator for h * alpha&lt;/span&gt;
&lt;span class="sd"&gt;  x0 : array&lt;/span&gt;
&lt;span class="sd"&gt;      initial value for x&lt;/span&gt;
&lt;span class="sd"&gt;  alpha : function&lt;/span&gt;
&lt;span class="sd"&gt;      function computing step sizes&lt;/span&gt;
&lt;span class="sd"&gt;  n_iterations : int, optional&lt;/span&gt;
&lt;span class="sd"&gt;      number of iterations to perform&lt;/span&gt;

&lt;span class="sd"&gt;  Returns&lt;/span&gt;
&lt;span class="sd"&gt;  -------&lt;/span&gt;
&lt;span class="sd"&gt;  xs : list&lt;/span&gt;
&lt;span class="sd"&gt;      intermediate values for x&lt;/span&gt;
&lt;span class="sd"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;3.0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;backtracking_line_search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;alpha_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
  &lt;span class="n"&gt;beta&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha_0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;
      &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;search&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yannopt.plotting&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plotting&lt;/span&gt;

  &lt;span class="c1"&gt;### ACCELERATED PROXIMAL GRADIENT ###&lt;/span&gt;

  &lt;span class="c1"&gt;# problem definition&lt;/span&gt;
  &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;g_gradient&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;h_prox&lt;/span&gt;      &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;alpha&lt;/span&gt;       &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;backtracking_line_search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;x0&lt;/span&gt;          &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;
  &lt;span class="n"&gt;n_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

  &lt;span class="c1"&gt;# run gradient descent&lt;/span&gt;
  &lt;span class="n"&gt;iterates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accelerated_proximal_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;
             &lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;### PLOTTING ###&lt;/span&gt;

  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iterates_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/iterates.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.69314718055994529&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iteration_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                      &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/convergence.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                      &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.69314718055994529&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="optimization"></category><category term="first-order"></category><category term="accelerated"></category><category term="proximal"></category></entry><entry><title>Coordinate Ascent for Convex Clustering</title><link href="/blog/coordinate-ascent-convex-clustering.html" rel="alternate"></link><published>2013-04-23T00:00:00-07:00</published><updated>2013-04-23T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-04-23:/blog/coordinate-ascent-convex-clustering.html</id><summary type="html">&lt;p&gt;Convex clustering is the reformulation of k-means clustering as a convex
problem. While the two problems are not equivalent, the former can be seen as a
relaxation of the latter that allows us to easily find globally optimal
solutions (as opposed to only locally optimal ones).&lt;/p&gt;
&lt;p&gt;Suppose we have a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Convex clustering is the reformulation of k-means clustering as a convex
problem. While the two problems are not equivalent, the former can be seen as a
relaxation of the latter that allows us to easily find globally optimal
solutions (as opposed to only locally optimal ones).&lt;/p&gt;
&lt;p&gt;Suppose we have a set of points ${ x_i : i = 1, \ldots, n}$. Our goal is to
partition these points into groups such that all the elements in each group are
close to each other and are distant from points in other groups.&lt;/p&gt;
&lt;p&gt;In this post, I'll talk about an algorithm to do just that.&lt;/p&gt;
&lt;div class="img-center" style="max-width: 400px;"&gt;
  &lt;img src="/assets/img/convex_clustering/clusters.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    8 clusters of points in 2D with their respective centers.  All points of
    the same color belong to the same cluster.
  &lt;/span&gt;
&lt;/div&gt;

&lt;h1&gt;&lt;a name="k-means" href="#k-means"&gt;K-Means&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The original objective for k-means clustering is as follows. Suppose we want
to find $k$ sets $S_i$ such that every $x_i$ is in exactly 1 set $S_j$. Each $S_j$
will then have a center $\theta_j$, which is simply the average of all $x_i$ it
contains. Putting it all together, we obtain the following optimization problme,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \underset{S}{\min}  &amp;amp; &amp;amp; \sum_{j=1}^{k} \sum_{i \in S_j} ||x_i - \theta_j||&lt;em S_j _in="\in" i&gt;2^2 \
  &amp;amp; \text{subject to}   &amp;amp; &amp;amp; \theta_j = \frac{1}{|S_j|} \sum&lt;/em&gt; x_i \
  &amp;amp;                     &amp;amp; &amp;amp; \bigcup_{j} S_j = { 1 \ldots n }
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;In 2009, &lt;a href="http://dl.acm.org/citation.cfm?id=1519389"&gt;Aloise et al.&lt;/a&gt; proved that solving this problem is
NP-hard, meaning that short of enumerating every possible partition, we cannot
say whether or not we've found an optimal solution $S^{*}$. In other words, we
can approximately solve k-means, but actually solving it is very
computationally intense (with the usual caveats about $P = NP$).&lt;/p&gt;
&lt;h1&gt;&lt;a name="convex-clustering" href="#convex-clustering"&gt;Convex Clustering&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Convex clustering sidesteps this complexity result by proposing a new
problem that we &lt;em&gt;can&lt;/em&gt; solve quickly. The optimal solution for this new problem
need not coincide with that of k-means, but &lt;a href="http://www.control.isy.liu.se/research/reports/2011/2992.pdf"&gt;can be seen&lt;/a&gt; a solution to
the convex relaxation of the original problem.&lt;/p&gt;
&lt;p&gt;The idea of convex clustering is that each point $x_i$ is paired with its
associated center $u_i$, and the distance between the two is minimized. If this
were nothing else, $u_i = x_i$ would be the optimal solution, and no
clustering would happen. Instead, a penalty term is added that brings the
clusters centers close together,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \min_{u} \frac{1}{2} \sum_{i=1}^{n} ||x_i - u_i||&lt;em _="&amp;lt;" i j&gt;2^2
            + \gamma \sum&lt;/em&gt; w_{i,j} ||u_i - u_j||_p
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Notice that the distance $||x_i - u_i||_2^2$ is a squared 2-norm, but
the distance between the centers $||u_i - u_j||_p$ is a p-norm ($p \in {1, 2,
\infty }$). This sum-of-norms type penalization brings about "group sparsity"
and is used primarily because many of the elements in this sum will be 0 at the
optimum. In convex clustering, that means $u_i = u_j$ for some pairs $i$ and
$j$ -- in other words, $i$ and $j$ are clustered together!&lt;/p&gt;
&lt;h1&gt;&lt;a name="algorithms" href="#algorithms"&gt;Algorithms for Convex Clustering&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;As the convex clustering formulation is a convex problem, we automatically
get a variety of black-box algorithms capable of solving it. Unfortunately, the
number of variables in the problem is rather large -- if $x_i \in
\mathcal{R}^{d}$, then $u \in \mathcal{R}^{n \times d}$.  If $d = 5$, we cannot
reasonably expect interior point solvers such as &lt;a href="http://cvxr.com/cvx/"&gt;cvx&lt;/a&gt; to handle any more
than a few thousand points.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.icml-2011.org/papers/419_icmlpaper.pdf"&gt;Hocking et al.&lt;/a&gt; and &lt;a href="http://arxiv.org/abs/1304.0499"&gt;Chi et al.&lt;/a&gt; were the first to design
algorithms specifically for convex clustering. The former designed one
algorithm for each $p$-norm, employing active sets ($p \in {1, 2}$),
subgradient descent ($p = 2$), and the Frank-Wolfe algorithm ($p = \infty$).
The latter makes use of &lt;a href="http://www.stanford.edu/~boyd/papers/admm_distr_stats.html"&gt;ADMM&lt;/a&gt; and AMA, the latter of which reduces to
proximal gradient on a dual objective.&lt;/p&gt;
&lt;p&gt;Here, I'll describe another method for solving the convex clustering problem
based on coordinate ascent. The idea is to take the original formulation,
substitute a new primal variable $z_l = u_{l_1} - u_{l_2}$, then update a dual
variable $\lambda_l$ corresponding to each equality constraint 1 at a time. For
this problem, we can reconstruct the primal variables $u_i$ in closed form
given the dual variables, so it is easy to check how close we are to the
optimum.&lt;/p&gt;
&lt;!--
  &lt;table class="table table-hover table-bordered"&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;Memory required&lt;/th&gt;
      &lt;th&gt;per-iteration complexity&lt;/th&gt;
      &lt;th&gt;number of iterations required&lt;/th&gt;
      &lt;th&gt;parallelizability&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Clusterpath ($L_1$)&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Clusterpath ($L_2$)&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Clusterpath ($L_{\infty}$)&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ADMM&lt;/td&gt;
      &lt;td&gt;$O(pd)$&lt;/td&gt;
      &lt;td&gt;$O(pd)$&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AMA (accelerated)&lt;/td&gt;
      &lt;td&gt;$O(pd)$&lt;/td&gt;
      &lt;td&gt;$O(pd)$&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Coordinate Ascent&lt;/td&gt;
      &lt;td&gt;$O(pd)$&lt;/td&gt;
      &lt;td&gt;$O(pd)$&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;

  For $p =$ number of pairs with $w_l &gt; 0$, $n =$ the number of points $x_i$,
$d =$ the dimensionality of $x_i$, $c = $ the current number of clusters
--&gt;

&lt;h1&gt;&lt;a name="reformulation" href="#reformulation"&gt;Problem Reformulation&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;To describe the dual problem being maximized, we first need to modify the
primal problem. First, let $z_l = u_{l_1} - u_{l_2}$. Then we can write the
objective function as,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \underset{S}{\min}  &amp;amp; &amp;amp; \frac{1}{2} \sum_{i=1}^{n} ||x_i - u_i||&lt;em l&gt;2^2
                            + \gamma \sum&lt;/em&gt; w_{l} ||z_l||&lt;em l_1&gt;p \
  &amp;amp; \text{subject to}   &amp;amp; &amp;amp; z_l = u&lt;/em&gt; - u_{l_2}
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;a href="http://arxiv.org/abs/1304.0499"&gt;Chi et al.&lt;/a&gt; show on page 6 that the dual of this problem is then,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \underset{\lambda}{\max}  &amp;amp; &amp;amp; - \frac{1}{2} \sum_{i} ||\Delta_i||&lt;em l&gt;2^2
                                  - \sum&lt;/em&gt; \lambda_l^T (x_{l_1} - x_{l_2}) \
  &amp;amp; \text{subject to}         &amp;amp; &amp;amp; ||\lambda_l||_{p^{&lt;/em&gt;}} \le \gamma w_l \
  &amp;amp;                           &amp;amp; &amp;amp; \Delta_{i} = \sum_{l: l_1 = i} \lambda_l - \sum_{l : l_2 = i} \lambda_l
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;In this notation, $||\cdot||_{p^{*}}$ is the dual norm of $||\cdot||_p$. The
primal variables $u$ and dual variables $\lambda$ are then related by the
following equation,&lt;/p&gt;
&lt;p&gt;$$
  u_i = \Delta_i + x_i
$$&lt;/p&gt;
&lt;h1&gt;&lt;a name="coordinate-ascent" href="#coordinate-ascent"&gt;Coordinate Ascent&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Now let's optimize the dual problem 1 $\lambda_k$ at a time. First, notice
that $\lambda_k$ will only appear in 2 $\Delta_i$ terms -- $\Delta_{k_1}$ and
$\Delta_{k_2}$. After dropping all terms independent of $\lambda_k$, we now get
the following problem,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \underset{\lambda_k}{\min}  &amp;amp; &amp;amp; \frac{1}{2} (||\Delta_{k_1}||&lt;em k_2&gt;2^2 + ||\Delta&lt;/em&gt;||&lt;em k_1&gt;2^2)
                                    + \lambda_k^T (x&lt;/em&gt; - x_{k_2}) \
  &amp;amp; \text{subject to}         &amp;amp; &amp;amp; ||\lambda_k||_{p^{&lt;/em&gt;}} \le \gamma w_k \
  &amp;amp;                           &amp;amp; &amp;amp; \Delta_{k_1} = \sum_{l: l_1 = k_1} \lambda_l - \sum_{l : l_2 = k_1} \lambda_l \
  &amp;amp;                           &amp;amp; &amp;amp; \Delta_{k_2} = \sum_{l: l_1 = k_2} \lambda_l - \sum_{l : l_2 = k_2} \lambda_l
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;We can pull $\lambda_k$ out of $\Delta_{k_1}$ and $\Delta_{k_2}$ to get,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  ||\Delta_{k_1}||&lt;em k_1&gt;2^2 &amp;amp; = ||\lambda_k||_2^2 + ||\Delta&lt;/em&gt; - \lambda_k||&lt;em k_1&gt;2^2 + 2 \lambda_k^T (\Delta&lt;/em&gt; - \lambda_k) \
  ||\Delta_{k_2}||&lt;em k_2&gt;2^2 &amp;amp; = ||\lambda_k||_2^2 + ||\Delta&lt;/em&gt; + \lambda_k||&lt;em k_2&gt;2^2 - 2 \lambda_k^T (\Delta&lt;/em&gt; + \lambda_k)
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Let's define $\tilde{\Delta_{k_1}} = \Delta_{k_1} - \lambda_k$ and
$\tilde{\Delta_{k_2}} = \Delta_{k_2} + \lambda_k$ and add $||\frac{1}{2}
(\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})||_2^2$ to the
objective.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \underset{\lambda_k}{\min}  &amp;amp; &amp;amp; ||\lambda_k||&lt;em k_1&gt;2^2
                                    + 2 \frac{1}{2} \lambda_k^T (\tilde{\Delta&lt;/em&gt;} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})
                                    + ||\frac{1}{2} (\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})||&lt;em&gt;2^2 \
  &amp;amp; \text{subject to}         &amp;amp; &amp;amp; ||\lambda_k||&lt;/em&gt;{p^{&lt;/em&gt;}} \le \gamma w_k \
  &amp;amp;                           &amp;amp; &amp;amp; \tilde{\Delta_{k_1}} = \sum_{l: l_1 = k_1; l \ne k} \lambda_l - \sum_{l : l_2 = k_1; l \ne k} \lambda_l \
  &amp;amp;                           &amp;amp; &amp;amp; \tilde{\Delta_{k_2}} = \sum_{l: l_1 = k_2; l \ne k} \lambda_l - \sum_{l : l_2 = k_2; l \ne k} \lambda_l
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;We can now factor the objective into a quadratic,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  &amp;amp; \underset{\lambda_k}{\min}  &amp;amp; &amp;amp; ||\lambda_k - \left( - \frac{1}{2}(\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2}) \right) ||&lt;em&gt;2^2 \
  &amp;amp; \text{subject to}         &amp;amp; &amp;amp; ||\lambda_k||&lt;/em&gt;{p^{&lt;/em&gt;}} \le \gamma w_k \
  &amp;amp;                           &amp;amp; &amp;amp; \tilde{\Delta_{k_1}} = \sum_{l: l_1 = k_1; l \ne k} \lambda_l - \sum_{l : l_2 = k_1; l \ne k} \lambda_l \
  &amp;amp;                           &amp;amp; &amp;amp; \tilde{\Delta_{k_2}} = \sum_{l: l_1 = k_2; l \ne k} \lambda_l - \sum_{l : l_2 = k_2; l \ne k} \lambda_l
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;This problem is simply a Euclidean projection onto the ball defined by
$||\cdot||_{p^{*}}$. We're now ready to write the algorithm,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input:&lt;/strong&gt; Initial dual variables $\lambda^{(0)}$, weights $w_l$, and regularization parameter $\gamma$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize $\Delta_i^{(0)} = \sum_{l: l_1 = i} \lambda_l^{(0)} - \sum_{l: l_2 = i} \lambda_l^{(0)}$&lt;/li&gt;
&lt;li&gt;For each iteration $m = 0,1,2,\ldots$ until convergence&lt;ol&gt;
&lt;li&gt;Let $\Delta^{(m+1)} = \Delta^{(m)}$&lt;/li&gt;
&lt;li&gt;For each pair of points $l = (i,j)$ with $w_{l} &amp;gt; 0$&lt;ol&gt;
&lt;li&gt;Let $\Delta_i^{(m+1)} \leftarrow \Delta_i^{(m+1)} - \lambda_l^{(m)}$ and $\Delta_j^{(m+1)} \leftarrow \Delta_i^{(m+1)} + \lambda_l^{(m)}$&lt;/li&gt;
&lt;li&gt;$\lambda_l^{(m+1)} = \text{project}(- \frac{1}{2}(\Delta_i^{(m+1)} - \Delta_j^{(m+1)} + x_{i} - x_{j}),
                                       { \lambda : ||\lambda||_{p^{*}} \le \gamma w_l }$)&lt;/li&gt;
&lt;li&gt;$\Delta_i^{(m+1)} \leftarrow \Delta_i^{(m+1)} + \lambda_l^{(m+1)}$ and $\Delta_j^{(m+1)} \leftarrow \Delta_j^{(m+1)} - \lambda_l^{(m+1)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Return $u_i = \Delta_i + x_i$ for all $i$&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;Since we can easily construct the primal variables from the dual variables
and can evaluate the primal and dual functions in closed form, we can use the
duality gap to determine when we are converged.&lt;/p&gt;
&lt;h1&gt;&lt;a name="conclusion" href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;In this post, I introduced a coordinate ascent algorithm for convex
clustering. Empirically, the algorithm is quite quick, but it doesn't share the
parallelizability or convergence proofs of its siblings, ADMM and AMA. However,
coordinate descent has an upside: there are no parameters to tune, and every
iteration is guaranteed to improve the objective function. Within each
iteration, updates are quick asymptotically and empirically.&lt;/p&gt;
&lt;p&gt;Unfortunately, like all algorithms based on the dual for this particular
problem, the biggest burden is on memory. Whereas the primal formulation
requires the number of variables grow linearly with the number of data points,
the dual formulation can grow as high as quadratically. In addition, the primal
variables allow for centers to be merged, allowing for potential space-savings
as the algorithm is running. The dual seems to lack this property, requiring
all dual variables to be fully instantiated.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The original formulation for convex clustering was introduced by &lt;a href="http://www.control.isy.liu.se/research/reports/2011/2992.pdf"&gt;Lindsten et
al.&lt;/a&gt; and &lt;a href="http://www.icml-2011.org/papers/419_icmlpaper.pdf"&gt;Hocking et al.&lt;/a&gt;. &lt;a href="http://arxiv.org/abs/1304.0499"&gt;Chi et al.&lt;/a&gt; introduced
ADMM and AMA-based algorithms specifically designed for convex clustering.&lt;/p&gt;</content><category term="optimization"></category><category term="coordinate-descent"></category><category term="clustering"></category></entry><entry><title>Why does L1 produce sparse solutions?</title><link href="/blog/l1-sparsity.html" rel="alternate"></link><published>2013-04-22T00:00:00-07:00</published><updated>2013-04-22T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-04-22:/blog/l1-sparsity.html</id><summary type="html">&lt;p&gt;Supervised machine learning problems are typically of the form "minimize your
error while regularizing your parameters." The idea is that while many choices
of parameters may make your training error low, the goal isn't low training
error -- it's low test-time error. Thus, parameters should be minimize training
error while remaining …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Supervised machine learning problems are typically of the form "minimize your
error while regularizing your parameters." The idea is that while many choices
of parameters may make your training error low, the goal isn't low training
error -- it's low test-time error. Thus, parameters should be minimize training
error while remaining "simple," where the definition of "simple" is left up to
the regularization function. Typically, supervised learning can be phrased as
minimizing the following objective function,&lt;/p&gt;
&lt;p&gt;$$
  w^{*} = \arg\min_{w} \sum_{i} L(y_i, f(x_i; w)) + \lambda \Omega(w)
$$&lt;/p&gt;
&lt;p&gt;where $L(y_i, f(x_i; w))$ is the loss for predicting $f(x_i; w)$ when the
true label is for sample $i$ is $y_i$ and $\Omega(w)$ is a regularization
function.&lt;/p&gt;
&lt;h1&gt;&lt;a name="sparsifying-regularizers" href="#sparsifying-regularizers"&gt;Sparsifying Regularizers&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;There are many choices for $\Omega(w)$, but the ones I'm going to talk about
today are so called "sparsifying regularizers" such as $||w||_1$. These norms
are most often employed because they produce "sparse" $w^{&lt;em&gt;}$ -- that is,
$w^{&lt;/em&gt;}$ with many zeros. This is in stark contrast to other regularizers such
as $\frac{1}{2}||w||_2^2$ which leads to lots of small but nonzero entries in
$w^{*}$.&lt;/p&gt;
&lt;h1&gt;&lt;a name="why-sparse-solutions" href="#why-sparse-solutions"&gt;Why Sparse Solutions?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Feature Selection&lt;/strong&gt; One of the key reasons people turn to sparsifying
regularizers is that they lead to automatic feature selection. Quite often,
many of the entries of $x_i$ are irrelevant or uninformative to predicting
the output $y_i$. Minimizing the objective function using these extra
features will lead to lower training error, but when the learned $w^{*}$ is
employed at test-time it will depend on these features to be more informative
than they are. By employing a sparsifying regularizer, the hope is that these
features will automatically be eliminated.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpretability&lt;/strong&gt; A second reason for favoring sparse solutions is that
the model is easier to interpret. For example, a simple sentiment classifier
might use a binary vector where an entry is 1 if a word is present and 0
otherwise. If the resulting learned weights $w^{*}$ has only a few non-zero
entries, we might believe that those are the most indicative words in deciding
sentiment.&lt;/p&gt;
&lt;h1&gt;&lt;a name="nonsmooth-regularizers" href="#nonsmooth-regularizers"&gt;Non-smooth Regularizers and their Solutions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;We now come to the \$ 100 million question: why do regularizers like the 1-norm
lead to sparse solutions? At some point someone probably told you "they're our
best convex approximation to $\ell_0$ norm," but there's a better reason than
that.  In fact, I claim that any regularizer that is non-differentiable at $w_i
= 0$ and can be decomposed into a sum can lead to sparse solutions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt; The intuition lies in the idea of subgradients. Recall that the
subgradient of a (convex) function $\Omega$ at $x$ is any vector $v$ such that,&lt;/p&gt;
&lt;p&gt;$$
  \Omega(y) \ge \Omega(x) + v^T (y-x)
$$&lt;/p&gt;
&lt;p&gt;The set of all subgradients for $\Omega$ at $x$ is called the subdifferential
and is denoted $\partial \Omega(x)$. If $\Omega$ is differentiable at $x$,
then $\partial \Omega(x) = { \nabla \Omega(x) }$ -- in other words,
$\partial \Omega(x)$ contains 1 vector, the gradient. Where the
subdifferential begins to matter is when $\Omega$ &lt;em&gt;isn't&lt;/em&gt; differentiable at
$x$. Then, it becomes something more interesting.&lt;/p&gt;
&lt;p&gt;Suppose we want to minimize an unconstrained objective like the following,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x} f(x) + \lambda \Omega(x)
$$&lt;/p&gt;
&lt;p&gt;By the &lt;a href="http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions"&gt;KKT conditions&lt;/a&gt;, 0 must be in the subdifferential at
the minimizer $x^{*}$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0 &amp;amp; \in \nabla f(x^{&lt;/em&gt;}) + \partial \lambda \Omega(x^{&lt;em&gt;}) \
  - \frac{1}{\lambda} \nabla f(x^{&lt;/em&gt;}) &amp;amp; \in \partial \Omega(x^{&lt;em&gt;}) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Looking forward, we're particularly interested in when the previous
inequality holds when $x^{*} = 0$. What conditions are necessary for this to be
true?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dual Norms&lt;/strong&gt; Since we're primarily concerned with $\Omega(x) = ||x||_1$,
let's plug that in. In the following, it'll actually be easier to prove things
about any norm, so we'll drop the 1 for the rest of this section.&lt;/p&gt;
&lt;p&gt;Recal the definition of a dual norm. In particular, the dual norm of a norm
$||\cdot||$ is defined as,&lt;/p&gt;
&lt;p&gt;$$
  ||y||&lt;em 1 _le="\le" _x_="||x||"&gt;{*} = \sup&lt;/em&gt; x^{T} y
$$&lt;/p&gt;
&lt;p&gt;A cool fact is that the dual of a dual norm is the original norm. In other words,&lt;/p&gt;
&lt;p&gt;$$
  ||x|| = \sup_{||y||_{*} \le 1} y^{T} x
$$&lt;/p&gt;
&lt;p&gt;Let's take the gradient of the previous expression on both sides. A nice fact
to keep in mind is that if we take the gradient of an expression of the form
$\sup_{y} g(y, x)$, then its gradient with respect to x is $\nabla_x g(y^{&lt;em&gt;},
x)$ where $y^{&lt;/em&gt;}$ is any $y$ that achieves the $\sup$. Since $g(y, x) = y^{T}
x$, that means,&lt;/p&gt;
&lt;p&gt;$$
  \nabla_x \sup_{y} g(y, x) = \nabla_x \left( (y^{&lt;em&gt;})^T x \right) = y^{&lt;/em&gt;} = \arg\max_{||y||_{*} \le 1} y^{T} x
$$&lt;/p&gt;
&lt;p&gt;$$
  \partial ||x|| = { y^{&lt;em&gt;} :  y^{&lt;/em&gt;} = \arg\max_{||y||_{*} \le 1} y^{T} x }
$$&lt;/p&gt;
&lt;p&gt;Now let $x = 0$. Then $y^{T} x = 0$ for all $y$, so any $y$ with $||y||_{*}
\le 1$ is in $\partial ||x||$ for $x = 0$.&lt;/p&gt;
&lt;p&gt;Back to our original goal, recall that&lt;/p&gt;
&lt;p&gt;$$
  -\frac{1}{\lambda} \nabla f(x) \in \partial ||x||
$$&lt;/p&gt;
&lt;p&gt;If $||-\frac{1}{\lambda} \nabla f(x)||_{&lt;em&gt;} \le 1$ when $x=0$, then we've
already established that $-\frac{1}{\lambda} \nabla f(0)$ is in $\partial
||0||$. In other words, $x^{&lt;/em&gt;} = 0$ solves the original problem!&lt;/p&gt;
&lt;h1&gt;&lt;a name="coordinate-sparsity" href="#coordinate-sparsity"&gt;Onto Coordinate-wise Sparsity&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;We've just established that $||\frac{1}{\lambda} \nabla f(0)||_{&lt;em&gt;} \le 1$
implies $x^{&lt;/em&gt;} = 0$, but we don't want all of $x^{&lt;em&gt;}$ to be 0, we want &lt;/em&gt;some
coordinates&lt;em&gt; of $x^{&lt;/em&gt;}$ to be 0. How can we take what we just concluded and
apply it only a subvector of $x^{*}$?&lt;/p&gt;
&lt;p&gt;Rather than a general norm, let's return once again to the $L_1$ norm. The
$L_1$ norm has a very special property that will be of use here:
separability. In words, this means that the $L_1$ norm can be expressed as a
sum of functions over $x$'s individual coordinates, each independent of every
other. In particular, $||x||&lt;em i&gt;1 = \sum&lt;/em&gt; |x_{i}|$.  It's easy to see that the
function $\Omega_i(x) = |x_i|$ is independent of the rest of $x$'s elements.&lt;/p&gt;
&lt;p&gt;Let's take another look at our objective function,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \min_{x} f(x) + \lambda ||x||&lt;em x_i&gt;1
  &amp;amp; = \min&lt;/em&gt; \left( \min_{x_{-i}} f(x_i, x_{-i}) + \lambda \sum_{j} |x_j| \right) \
  &amp;amp; = \min_{x_i} g(x_i) + \lambda |x_i|
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;where $x_{-i}$ is all coordinates of $x$ except $x_i$ and $g(x_i) =
\min_{x_{-i}} f(x_i, x_{-i}) + \lambda \sum_{j \ne i} |x_j|$. Taking the
derivative of $g(x_i)$ with respect to $x_i$, we again require that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0 &amp;amp;\in \nabla_{x_i} g(x_i) + \lambda \partial |x_i| \
  -\frac{1}{\lambda} \nabla_{x_i} g(x_i) &amp;amp; \in \partial |x_i| \
  -\frac{1}{\lambda} \nabla_{x_i} f(x_i, x_{-i}^{&lt;/em&gt;}) &amp;amp; \in \partial |x_i|
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Hmm, that looks familiar. And isn't $|x_i| = ||x_i||_1$? That means that if&lt;/p&gt;
&lt;p&gt;$$
  \left| \left| \frac{1}{\lambda} \nabla_{x_i} f(x_i, x_{-i}^{&lt;em&gt;}) \right| \right|&lt;em x_i&gt;{\infty}
  = \left| \frac{1}{\lambda} \nabla&lt;/em&gt; f(x_i, x_{-i}^{&lt;/em&gt;}) \right| \le 1
$$&lt;/p&gt;
&lt;p&gt;when $x_i = 0$, then $x_i^{*} = 0$. In other words, given the optimal values
for all coordinates other than $i$, we can evaluate the derivative of
$\frac{1}{\lambda} f$ with respect to $x_i$ and check if the absolute value
of that is less than 1. If it is, then $x_i = 0$ is optimal!&lt;/p&gt;
&lt;h1&gt;&lt;a name="conclusion" href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;In the first section, we showed that in order to solve the problem
$\min_{x} f(x) + \lambda \Omega(x)$, it is necessary that $-\frac{1}{\lambda}
\nabla f(x^{&lt;em&gt;}) \in \partial \Omega(x^{&lt;/em&gt;})$. If $\Omega(x^{&lt;em&gt;})$ is
differentiable at $x^{&lt;/em&gt;}$, then there can be only 1 possible choice for
$x^{&lt;em&gt;}$, but in all other cases there are a multitude of potential solutions.
When $\Omega(x)$ isn't differentiable at $x = 0$, there is a non-singleton set
of values which $-\frac{1}{\lambda} \nabla f(x^{&lt;/em&gt;})$ can be in such that $x^{&lt;em&gt;}
= 0$ is an optimal solution. If $\Omega(x) = ||x||$, then a sufficient
condition for $x^{&lt;/em&gt;} = 0$ to be optimal is $||\frac{1}{\lambda} \nabla
f(x)||_{*} \le 1$ at $x = 0$.&lt;/p&gt;
&lt;p&gt;In the next section, we showed that in the special case of the $L_1$ norm, we
can express the norm as the sum of $L_1$ norms applied to $x$'s individual
coordinates. Because of this, we can rewrite the original optimization problem
as $\min_{x_i} g(x_i) + \lambda ||x_i||&lt;em x__-i="x_{-i"&gt;1$ where $g(x_i) = \min&lt;/em&gt;} f(x_i,
x_{-i}) + \lambda ||x_{-i}||&lt;em x_i&gt;1$. Using the same results from the previous
section, we showed that as long as $|\frac{1}{\lambda} \nabla&lt;/em&gt; f(x_i,
x_{-i}^{&lt;em&gt;})| \le 1$ when $x_i = 0$, then $x_i^{&lt;/em&gt;} = 0$ is an optimal choice. In
other words, we established conditions upon which a coordinate will be 0. This
is why the $L_1$ norm causes sparsity.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Everything written here was explained to me by the ever-knowledgable
MetaOptimize king, &lt;a href="https://twitter.com/atpassos"&gt;Alexandre Passos&lt;/a&gt;.&lt;/p&gt;</content><category term="optimization"></category><category term="sparsity"></category></entry><entry><title>Proximal Gradient Descent</title><link href="/blog/proximal-gradient-descent.html" rel="alternate"></link><published>2013-04-19T00:00:00-07:00</published><updated>2013-04-19T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-04-19:/blog/proximal-gradient-descent.html</id><summary type="html">&lt;p&gt;$ \def\prox{\text{prox}} $
  In a &lt;a href="/blog/subgradient-descent.html#usage"&gt;previous post&lt;/a&gt;, I mentioned that one cannot
hope to asymptotically outperform the $O(\frac{1}{\epsilon^2})$ convergence
rate of Subgradient Descent when dealing with a non-differentiable objective
function. This is in fact only half-true; Subgradient Descent cannot be beat
&lt;em&gt;using only first-order information …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;$ \def\prox{\text{prox}} $
  In a &lt;a href="/blog/subgradient-descent.html#usage"&gt;previous post&lt;/a&gt;, I mentioned that one cannot
hope to asymptotically outperform the $O(\frac{1}{\epsilon^2})$ convergence
rate of Subgradient Descent when dealing with a non-differentiable objective
function. This is in fact only half-true; Subgradient Descent cannot be beat
&lt;em&gt;using only first-order information&lt;/em&gt; (that is, gradients and subgradients).
In this article, I'll describe Proximal Gradient Descent, an algorithm that
exploits problem structure to obtain a rate of $O(\frac{1}{\epsilon})$. In
particular, Proximal Gradient is useful if the following 2 assumptions hold.
First, the objective function must be of the form,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x} \, g(x) + h(x)
$$&lt;/p&gt;
&lt;p&gt;with $g$ is differentiable. Second $h$ must be "simple" enough such that we
can calculate its $\prox$ operator very quickly,&lt;/p&gt;
&lt;p&gt;$$
  \prox_{h}(x) = \min_{u} h(u) + \frac{1}{2} ||u-x||_2^2
$$&lt;/p&gt;
&lt;p&gt;Using these two assumptions, we can obtain a convergence rate identical to
Gradient Descent even when optimizing non-differentiable objective functions.&lt;/p&gt;
&lt;h1&gt;&lt;a name="implementation" href="#implementation"&gt;How does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: initial iterate $x^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $t = 0, 1, 2, \ldots$&lt;ol&gt;
&lt;li&gt;Let $x^{(t+1)} = \prox_{ \alpha^{(t)} h } \left( x^{(t)} - \alpha^{(t)} \nabla g(x^{(t)}) \right)$&lt;/li&gt;
&lt;li&gt;if converged, return $x^{(t+1)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a id="intuition"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name="intuition" href="#intuition"&gt;Intuition for the $\prox$ Operator&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;At first sight, the $\prox$ operator looks suspicious.  Where did it come
from? Why did someone really think it would work?  It ends up that we can
derive the update for Gradient Descent and the update for Gradient Descent
almost identically. First, notice that the Gradient Descent Update is the
solution to the following quadratic approximation to $f(x)$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  x^{(t+1)} &amp;amp; = \arg\min_{y} f(x^{(t)}) + \nabla f(x^{(t)})^T (y-x^{(t)}) + \frac{L}{2} ||y-x^{(t)}||_2^2 \
  0     &amp;amp; = \nabla f(x^{(t)}) + L (x^{(t+1)}-x^{(t)}) \
  x^{(t+1)} &amp;amp; = x^{(t)} - \frac{1}{L} \nabla f(x^{(t)})
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;We take the gradient of the right hand side with respect to $y$ and set it to
zero in the second line.  Now replace $f$ with $g$, and add $h(y)$ to the
very end of the first line,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  x^{(t+1)}
  &amp;amp; = \arg\min_{y} g(x^{(t)}) + \nabla g(x^{(t)})^T (y-x^{(t)}) + \frac{L}{2} ||y-x^{(t)}||&lt;em y&gt;2^2 + h(y) \
  &amp;amp; = \arg\min&lt;/em&gt; g(x^{(t)}) + \frac{L}{2} \left( \frac{2}{L} \nabla g(x^{(t)})^T (y-x^{(t)}) \right) + \frac{L}{2} ||y-x^{(t)}||&lt;em y&gt;2^2 + h(y) + \frac{L}{2} ||\nabla g(x^{(t)})||_2^2 \
  &amp;amp; = \arg\min&lt;/em&gt; \frac{L}{2} || y - (x^{(t)} - \frac{1}{L} \nabla g(x^{(t)})) ||&lt;em _frac_1="\frac{1"&gt;2^2 + h(y) \
  &amp;amp; = \prox&lt;/em&gt;{L} h }(x^{(t)} - \frac{1}{L} \nabla g(x^{(t)})) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;This time, we add constants (with respect to $y$) such that we can pull the
$\nabla g(x^{(t)})^T (y-x^{(t)})$ into the quadratic term, leading us to the
Proximal Gradient Descent update.&lt;/p&gt;
&lt;h1&gt;&lt;a name="example" href="#example"&gt;A Small Example&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Let's now see how well Proximal Gradient Descent works.  For this example,
we'll solve the following problem,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x} \, \log(1 + \exp(-2x)) + ||x||_1
$$&lt;/p&gt;
&lt;p&gt;Letting $g(x) = \log(1+\exp(-2x))$ and $h(x) = ||x||_1$, it can be shown
that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \nabla g(x) &amp;amp;= \frac{1}{1 + \exp(-2x)} \left( \exp(-2x) \right) (-2) \
  \prox_{\alpha h}(x) &amp;amp; = \text{sign}(x) \max(0, \text{abs}(x) - \alpha) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;We'll use a variant of &lt;a href="#line_search"&gt;Backtracking Line Search&lt;/a&gt; modified for
Proximal Gradient Descent and an initial choice of $x^{(0)} = 5$.&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/proximal_gradient_descent/convergence.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows how quickly the objective function decreases as the
    number of iterations increases.
  &lt;/span&gt;
&lt;/div&gt;

&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/proximal_gradient_descent/iterates.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number.
  &lt;/span&gt;
&lt;/div&gt;

&lt;h1&gt;&lt;a name="proof" href="#proof"&gt;Why does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Proximal Gradient Descent, like regular Gradient Descent, is a "descent"
method where the objective value is guaranteed to decrease. In fact, the
assumptions for Proximal Gradient Descent's $g(x)$ are the identical to the
Gradient Descent assumptions for $f(x)$. The only additional condition is
that $h(x)$ be convex,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$g(x)$ is convex, differentiable, and finite for all $x$&lt;/li&gt;
&lt;li&gt;a finite solution $x^{*}$ exists&lt;/li&gt;
&lt;li&gt;$\nabla g(x)$ is Lipschitz continuous with constant $L$. That is, there must
   be an $L$ such that,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
  || \nabla g(x) - \nabla g(y) ||_2 \le L || x - y ||_2 \qquad \forall x,y
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$h(x)$ is convex&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Proof Outline&lt;/strong&gt; The majority of the convergence proof for Proximal Gradient
Descent is identical to the proof for regular Gradient Descent. The key is to
carefully choose a function $G_{\alpha}(x)$ that can take the place of $\nabla
f(x)$.  Once it is defined, we can rephrase Proximal Gradient Descent as
$x^{(t+1)} = x^{(t)} - \alpha^{(t)} G_{\alpha^{(t)}}(x^{(t)})$. Next, we'll
show that,&lt;/p&gt;
&lt;p&gt;$$
  (g+h)(x^{(t+1)}) \le (g+h)(x^{&lt;em&gt;}) + G_{\alpha^{(t)}}(x^{(t)})^T (x-x^{&lt;/em&gt;}) - \frac{\alpha^{(t)}}{2} ||G_{\alpha^{(t)}}(x^{(t)})||_2^2
$$&lt;/p&gt;
&lt;p&gt;Once we have this, we can repeat the Gradient Descent proof verbatim with $g
+ h \rightarrow f$ and $G_{\alpha^{(t)}}(x^{(t)}) \rightarrow \nabla
  f(x^{(t)})$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; Phrase Proximal Gradient Descent as $x^{(t+1)} = x^{(t)} - \alpha^{(t)} G_{\alpha^{(t)}}(x^{(t)})$. Define $G$
as follows,&lt;/p&gt;
&lt;p&gt;$$
  G_{\alpha}(x) \triangleq \frac{1}{\alpha} (x - \prox_{\alpha h}( x - \alpha \nabla g(x)))
$$&lt;/p&gt;
&lt;p&gt;Now let $x^{+} \triangleq x^{(t+1)}$, $x \triangleq x^{(t)}$, and $\alpha
\triangleq \alpha^{(t)}$. Using $G$, we can reframe Proximal Gradient Descent
as a typical descent method,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  x^{+}
  &amp;amp;= x - \alpha G_{\alpha}(x) \
  &amp;amp;= x - \alpha \left(
      \frac{1}{\alpha} (x - \prox_{\alpha h}( x - \alpha \nabla g(x))
    \right) \
  &amp;amp;= x - (x - \prox_{\alpha h}( x - \alpha \nabla g(x)) \
  &amp;amp;= \prox_{\alpha h}( x - \alpha \nabla g(x)) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Show that $G_{\alpha}(x)$ can be used like Gradient
Descent's $\nabla f(x)$. Our goals is to obtain a statement identical to
$f(x^{+}) \le f(x^{&lt;em&gt;}) + \nabla f(x)^T (x-x^{&lt;/em&gt;}) - \frac{\alpha}{2} ||\nabla
f(x)||&lt;em _alpha="\alpha"&gt;2^2$ except with $G&lt;/em&gt;(x)$ instead of $\nabla f(x)$.  Once we
have this, the rest of the proof is exactly the same as Gradient Descent's.
Begin by recalling the Lipschitz condition on $g$,&lt;/p&gt;
&lt;p&gt;$$
  g(y) \le g(x) + \nabla g(x)^T (y-x) + \frac{L}{2} ||y-x||_2^2
$$&lt;/p&gt;
&lt;p&gt;Substitute $y = x^{+} = x - \alpha G_{\alpha}(x)$ to obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  g(x - \alpha G_{\alpha}(x))
  &amp;amp; \le g(x) + \nabla g(x)^T(x - \alpha G_{\alpha}(x) - x) + \frac{L}{2}||x - \alpha G_{\alpha}(x) - x||&lt;em _alpha="\alpha"&gt;2^2 \
  &amp;amp; = g(x) - \alpha \nabla g(x)^T G&lt;/em&gt;(x) + \frac{L ( \alpha )^2}{2}||G_{\alpha}(x)||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Assume then that $\alpha \le \frac{1}{L}$ (this is what Backtracking Line
Search does), We can upper bound $\frac{L ( \alpha )^2}{2} \le
\frac{\alpha}{2}$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  g(x - \alpha G_{\alpha}(x))
  &amp;amp; \le g(x) - \alpha \nabla g(x)^T G_{\alpha}(x) + \frac{ \alpha }{2}||G_{\alpha}(x)||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Then add $h(x - \alpha G_{\alpha}(x))$ to both sides,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  (g+h)(x - \alpha G_{\alpha}(x))
  &amp;amp; \le g(x) - \alpha \nabla g(x)^T G_{\alpha}(x) + \frac{ \alpha }{2}||G_{\alpha}(x)||&lt;em _alpha="\alpha"&gt;2^2 \
  &amp;amp; \quad + h(x - \alpha G&lt;/em&gt;(x)) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Next, we'll upper bound $g(x)$ and $h(x - \alpha G_{\alpha}(x))$ using the definition of convexity. The following 2 equations hold for all $z$, For $g$, we'll use,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  g(z) &amp;amp; \ge g(x) + \nabla g(x)^T (z-x) \
  g(z) + \nabla g(x)^T (x-z) &amp;amp; \ge g(x) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;For $h$ we have something a bit more mysterious,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  h(z)
  &amp;amp; \ge h(x - \alpha G_{\alpha}(x)) + [G_{\alpha}(x) - \nabla g(x)]^T (z-(x - \alpha G_{\alpha}(x))) \
  h(z) + [G_{\alpha}(x) - \nabla g(x)]^T(x - \alpha G_{\alpha}(x) - z)
  &amp;amp; \ge h(x - \alpha G_{\alpha}(x))
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Where did that come from? It so happens that $G_{\alpha}(x) - \nabla
g(x)$ is a valid subgradient for $h$ at $x - \alpha G_{\alpha}(x)$.
Recall the 2 definitions we have for $x^{+}$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  x^{+}
  &amp;amp;= \prox_{\alpha h} (x - \alpha \nabla g(x)) \
  &amp;amp;= \arg\min_{u} \alpha h(u) + \frac{1}{2} ||u - (x - \alpha \nabla g(x))||&lt;em _alpha="\alpha"&gt;2^2 \
  0
  &amp;amp; \in \alpha \partial h(x^{+}) + x^{+} - (x - \alpha \nabla g(x)) \
  (x - \alpha \nabla g(x)) - x^{+}
  &amp;amp; \in \alpha \partial h(x^{+}) \
  (x - \alpha \nabla g(x)) - (x - \alpha G&lt;/em&gt;(x))
  &amp;amp; \in \alpha \partial h(x^{+}) \
  \alpha [G_{\alpha}(x)) - \nabla g(x)]
  &amp;amp; \in \alpha \partial h(x^{+}) \
  [G_{\alpha}(x)) - \nabla g(x)]
  &amp;amp; \in \partial h(x^{+}) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Thus, $G_{\alpha}(x)) - \nabla g(x)$ is a valid subgradient for $h$ at
$x^{+} \triangleq x - \alpha G_{\alpha}(x)$, and the previous lower
bound on $h(z)$ holds.  Putting the previous two inequalities back into the
preceding equation and canceling out, we can see that for all $z$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  (g+h)(x - \alpha G_{\alpha}(x))
  &amp;amp; \le g(x) - \alpha \nabla g(x)^T G_{\alpha}(x) + \frac{ \alpha }{2}||G_{\alpha}(x)||&lt;em _alpha="\alpha"&gt;2^2 \
  &amp;amp; \quad + h(x - \alpha G&lt;/em&gt;(x)) \
  (g+h)(x - \alpha G_{\alpha}(x))
  &amp;amp; \le \left( g(z) + \nabla g(z)^T (x-z) \right) - \alpha \nabla g(x)^T G_{\alpha}(x) + \frac{ \alpha }{2}||G_{\alpha}(x)||&lt;em _alpha="\alpha"&gt;2^2 \
  &amp;amp; \quad + \left( h(z) + [G&lt;/em&gt;(x) - \nabla g(x)]^T (x - \alpha G_{\alpha}(x) - z) \right) \
  &amp;amp; = (g+h)(z) + G_{\alpha}(x)^T (x-z) - \frac{ \alpha }{2}||G_{\alpha}(x)||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Now let $z = x^{*}$ to get,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  (g+h)(x^{+})
  &amp;amp; \le (g+h)(x^{&lt;/em&gt;}) + G_{\alpha}(x)^T (x-x^{&lt;em&gt;}) - \frac{ \alpha }{2}||G_{\alpha}(x)||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Looking back at Step 1 of the &lt;a href="/blog/gradient-descent.html#proof"&gt;Gradient Descent
Proof&lt;/a&gt;, you can see that this equation is exactly the
same as the one used before except that $G_{\alpha}(x)$ replaces $\nabla
f(x)$. Following the rest of the Gradient Descent proof, we find that of we
want $(g+h)(x^{(t)}) - (g+h)(x^{*}) \le \epsilon$, we need
$O(\frac{1}{\epsilon})$ iterations, just like Gradient Descent.&lt;/p&gt;
&lt;h1&gt;&lt;a name="usage" href="#usage"&gt;When should I use it?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Proximal Gradient Descent requires being able to easily calculate
$\prox_{\alpha h}(x)$.  The benefits of doing so are clear -- we can reach an
$\epsilon$-approximate solution in far fewer iterations than Subgradient
Descent. But this is only valuable if the cost of an iteration of Proximal Gradient
Descent is similar to that of Subgradient Descent. For some choices of $h(x)$,
this actually holds (see &lt;a href="#common_prox_functions"&gt;Common Prox Functions&lt;/a&gt;
below); it is then that Proximal Gradient Descent should be used. For other
cases where no closed-form solution exists, it is often better to stick with
Subgradient Descent.&lt;/p&gt;
&lt;h1&gt;&lt;a name="extensions" href="#extensions"&gt;Extensions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a id="line_search"&gt;&lt;/a&gt;
  &lt;strong&gt;Step Size&lt;/strong&gt; The proof above assumes the step size $\alpha^{(t)} \le
\frac{1}{L}$ ($L$ is the Lipschitz constant of $g(x)$). Rather than guessing
for such values, Backtracking Line Search can be employed with a slight
modification. Recall that Backtracking Line Search chooses $\alpha^{(t)}$ such
that,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{(t)} - \alpha^{(t)} \nabla f(x^{(t)}))
  &amp;amp; \le f(x^{(t)}) - \frac{\alpha^{(t)}}{2}|| \nabla f(x^{(t)})||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;If $\alpha^{(t)} \le \frac{1}{L}$, this statement must hold. To see why, let's write out where the condition came from,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{(t+1)})
  &amp;amp; \le f(x^{(t)}) + \nabla f(^{(t)})^T (x^{(t+1)} - x^{(t)}) + \frac{1}{2 \alpha^{(t)}}||x^{(t+1)} - x^{(t)}||_2^2 \
  f(x^{(t)} - \alpha^{(t)} \nabla f(x^{(t)}))
  &amp;amp; \le f(x^{(t)}) + \nabla f(^{(t)})^T (x^{(t)} - \alpha^{(t)} \nabla f(x^{(t)}) - x^{(t)}) \
  &amp;amp; \quad + \frac{1}{2 \alpha^{(t)}}||x^{(t)} - \alpha^{(t)} \nabla f(x^{(t)}) - x^{(t)}||_2^2 \
  f(x^{(t)} - \alpha^{(t)} \nabla f(x^{(t)}))
  &amp;amp; \le f(x^{(t)}) - \alpha^{(t)} ||\nabla f(^{(t)}) ||_2^2 + \frac{\alpha^{(t)}}{2}|| \nabla f(x^{(t)})||_2^2 \
  f(x^{(t)} - \alpha^{(t)} \nabla f(x^{(t)}))
  &amp;amp; \le f(x^{(t)}) - \frac{\alpha^{(t)}}{2}|| \nabla f(x^{(t)})||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;If we assume that $f$ is $L$-Lipschitz, then $\alpha^{(t)} = \frac{1}{L}$ is
precisely the Lipschitz assumption. Recall now that for this problem, we have
$G_{\alpha^{(t)}}(x^{(t)})$ in place of $\nabla f(x^{(t)})$. Replacing $f$ with
$g+h$ and $\nabla f(x)$ with $G_{\alpha}(x)$, we come to a similar condition,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  (g+h)(x^{(t+1)})
  &amp;amp; \le (g+h)(x^{(t)}) - \frac{\alpha^{(t)}}{2}|| G_{\alpha^{(t)}}(x^{(t)}) ||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;In other words, we can perform Backtracking Line Search for Proximal Gradient Descent as follows,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: iterate $x^{(t)}$, initial step size $\alpha_0$, step factor $\beta$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha = \alpha_0$&lt;/li&gt;
&lt;li&gt;While True&lt;ol&gt;
&lt;li&gt;Calculate $G_{\alpha}(x^{(t)}) = \frac{1}{\alpha}( x - \prox_{\alpha h}(x^{(t)} - \alpha \nabla g(x)) )$&lt;/li&gt;
&lt;li&gt;if $(g+h)(x^{(t+1)}) \le (g+h)(x^{(t)}) - \frac{\alpha}{2}|| G_{\alpha}(x^{(t)}) ||_2^2$, set $\alpha^{(t)} = \alpha$ and return&lt;/li&gt;
&lt;li&gt;otherwise set $\alpha \leftarrow \alpha \beta$ and continue&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h1&gt;&lt;a name="common-prox" href="#common-prox"&gt;Common $\prox$ Functions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;There are several common choices for $h(x)$ that admit particularly efficient
$\prox$ functions. If your objective function contains one of these, consider
applying Proximal Gradient immediately -- you'll converge far faster than
Subgradient Descent.&lt;/p&gt;
&lt;table class="table table-bordered table-centered"&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;$h(x)$&lt;/th&gt;
      &lt;th&gt;$\prox_{\alpha h}(x)$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$||x||_1$&lt;/td&gt;
      &lt;td&gt;$\text{sign}(x) \max(0, \text{abs}(x) - \alpha)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\frac{1}{2}||x||_2^2$&lt;/td&gt;
      &lt;td&gt;$\frac{1}{1 + \alpha} x$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$||x||_2$&lt;/td&gt;
      &lt;td&gt;$\left( 1 - \frac{\alpha}{||x||_2} \right) x$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$||x||_{\infty}$&lt;/td&gt;
      &lt;td&gt;
          $\text{sign}(x) \min( \text{abs}(x), \theta )$

        where

          $\theta = \frac{1}{\rho} \sum_{j : |x_j| &gt; |x_{(\rho)}|} (|x_j| - \alpha)$

        where $x_{(i)}$ is is the $i$-th largest element of $x$ in magnitude and
        $\rho$ is the smallest $i$ such that
        $\sum_{j : |x_j| &gt; |x_{(i)}|} (|x_j| - |x_{(i)}|) &lt; \alpha$.
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\frac{1}{2} x^T Q x + b^T x$&lt;/td&gt;
      &lt;td&gt;$(\alpha Q + I)^{-1} (x - \alpha b)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Proof of Convergence&lt;/strong&gt; The original proof of convergence is thanks to
Laurent El Ghaoui's &lt;a href="http://www.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf"&gt;EE227a slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;List of Proximal Functions&lt;/strong&gt;The list of proximal functions is taken from
John Duchi's article on &lt;a href="http://www.cs.berkeley.edu/~jduchi/projects/DuchiSi09c.pdf"&gt;Forward-Backward Splitting (FOBOS)&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name="reference-impl" href="#reference-impl"&gt;Reference Implementation&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;proximal_gradient_descent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                              &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Proximal Gradient Descent&lt;/span&gt;

&lt;span class="sd"&gt;  Parameters&lt;/span&gt;
&lt;span class="sd"&gt;  ----------&lt;/span&gt;
&lt;span class="sd"&gt;  g_gradient : function&lt;/span&gt;
&lt;span class="sd"&gt;      Compute the gradient of `g(x)`&lt;/span&gt;
&lt;span class="sd"&gt;  h_prox : function&lt;/span&gt;
&lt;span class="sd"&gt;      Compute prox operator for h * alpha&lt;/span&gt;
&lt;span class="sd"&gt;  x0 : array&lt;/span&gt;
&lt;span class="sd"&gt;      initial value for x&lt;/span&gt;
&lt;span class="sd"&gt;  alpha : function&lt;/span&gt;
&lt;span class="sd"&gt;      function computing step sizes&lt;/span&gt;
&lt;span class="sd"&gt;  n_iterations : int, optional&lt;/span&gt;
&lt;span class="sd"&gt;      number of iterations to perform&lt;/span&gt;

&lt;span class="sd"&gt;  Returns&lt;/span&gt;
&lt;span class="sd"&gt;  -------&lt;/span&gt;
&lt;span class="sd"&gt;  xs : list&lt;/span&gt;
&lt;span class="sd"&gt;      intermediate values for x&lt;/span&gt;
&lt;span class="sd"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;backtracking_line_search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;alpha_0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
  &lt;span class="n"&gt;beta&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.9&lt;/span&gt;
  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha_0&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;G&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;
      &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;search&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yannopt.plotting&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plotting&lt;/span&gt;

  &lt;span class="c1"&gt;### PROXIMAL GRADIENT DESCENT ###&lt;/span&gt;

  &lt;span class="c1"&gt;# problem definition&lt;/span&gt;
  &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;g_gradient&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;h_prox&lt;/span&gt;      &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;alpha&lt;/span&gt;       &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;backtracking_line_search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;x0&lt;/span&gt;          &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;
  &lt;span class="n"&gt;n_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

  &lt;span class="c1"&gt;# run gradient descent&lt;/span&gt;
  &lt;span class="n"&gt;iterates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;proximal_gradient_descent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                  &lt;span class="n"&gt;g_gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;h_prox&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;
             &lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;### PLOTTING ###&lt;/span&gt;

  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iterates_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/iterates.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.69314718055994529&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iteration_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                      &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/convergence.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                      &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.69314718055994529&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="optimization"></category><category term="first-order"></category><category term="proximal"></category></entry><entry><title>Accelerated Gradient Descent</title><link href="/blog/accelerated-gradient-descent.html" rel="alternate"></link><published>2013-04-12T00:00:00-07:00</published><updated>2013-04-12T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-04-12:/blog/accelerated-gradient-descent.html</id><summary type="html">&lt;p&gt;In the mid-1980s, Yurii Nesterov hit the equivalent of an academic home run.
At the same time, he established the Accelerated Gradient Method, proved that
its convergence rate superior to Gradient Descent ($O(1/\sqrt{\epsilon})$
iterations instead of $O(1/\epsilon)$), and then proved that no other
first-order (that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the mid-1980s, Yurii Nesterov hit the equivalent of an academic home run.
At the same time, he established the Accelerated Gradient Method, proved that
its convergence rate superior to Gradient Descent ($O(1/\sqrt{\epsilon})$
iterations instead of $O(1/\epsilon)$), and then proved that no other
first-order (that is, gradient-based) algorithm could ever hope to beat it.
What a boss.&lt;/p&gt;
&lt;p&gt;&lt;a href="/blog/gradient-descent.html"&gt;Continuing&lt;/a&gt; &lt;a href="/blog/subgradient-descent.html"&gt;my analogy&lt;/a&gt;, imagine
that you are standing on the side of a mountain and want to reach the bottom.
If you were to follow the Accelerated Gradient Method, you'd do something like
this,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;ol&gt;
&lt;li&gt;Look around you and see which way points the most dowards&lt;/li&gt;
&lt;li&gt;Take a step in that direction&lt;/li&gt;
&lt;li&gt;Take another step in that direction, even if it starts taking you uphill.
     Then repeat.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;As we'll see, that last unintuitive bit is the key to Accelerated Gradient
Descent's speed.&lt;/p&gt;
&lt;h1&gt;&lt;a name="implementation" href="#implementation"&gt;How does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;We begin by assuming the we want to minimize an unconstrained function $f$,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x} \, f(x)
$$&lt;/p&gt;
&lt;p&gt;As with Gradient Descent, we'll assume that $f$ is differentiable and that we
can easily compute its gradient $\nabla f(x)$. Then the Accelerated Gradient
Method is defined as,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: initial iterate $y^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $t = 1, 2, \ldots$&lt;ol&gt;
&lt;li&gt;Let $x^{(t)} = y^{(t-1)} - \alpha^{(t)} \nabla f(y^{(t-1)})$&lt;/li&gt;
&lt;li&gt;if converged, return $x^{(t)}$&lt;/li&gt;
&lt;li&gt;Let $y^{(t)} = x^{(t)} + \frac{t-1}{t+2} (x^{(t)} - x^{(t-1)})$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h1&gt;&lt;a name="example" href="#example"&gt;A Small Example&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;We'll try out Accelerated Gradient on the &lt;a href="/blog/gradient-descent.html#example"&gt;same example used to showcase
Gradient Descent&lt;/a&gt;. We'll use the objective function
$f(x) = x^4$, meaning that $\nabla_x f(x) = 4 x^3$. For a step size, we'll use
Backtracking Line Search where the largest acceptable step size is $0.05$.
Finally, we'll start at $x^{(0)} = 1$.  Compare these 2 graphs to the ones for
Gradient Descent.&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/accelerated_gradient/convergence.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows how quickly the objective function decreases as the
    number of iterations increases.
  &lt;/span&gt;
&lt;/div&gt;

&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/accelerated_gradient/iterates.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number.
  &lt;/span&gt;
&lt;/div&gt;

&lt;h1&gt;&lt;a name="proof" href="#proof"&gt;Why does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The assumptions for the Accelerated Gradient Method are identical to Gradient
Descent's. In particular, we assume,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f$ is convex, differentiable, and finite for all $x$&lt;/li&gt;
&lt;li&gt;a finite solution $x^{*}$ exists&lt;/li&gt;
&lt;li&gt;$\nabla f(x)$ is Lipschitz continuous with constant $L$. That is, there must
   be an $L$ such that,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
  || \nabla f(x) - \nabla f(y) ||_2 \le L || x - y ||_2 \qquad \forall x,y
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; As explained in my post on &lt;a href="/blog/gradient-descent.html"&gt;Gradient
Descent&lt;/a&gt;, these assumptions give us 2 things. Assumption 1
gives us a linear lower bound on $f$,&lt;/p&gt;
&lt;p&gt;$$
  f(y) \ge f(x) + \nabla f(x)^T (y-x) \qquad \forall x, y
$$&lt;/p&gt;
&lt;p&gt;Assumption 3 then gives us a quadratic upper bound on $f$,&lt;/p&gt;
&lt;p&gt;$$
  f(y) \le f(x) + \nabla f(x)^T (y-x) + \frac{L}{2} ||x - y||_2^2 \qquad \forall x, y
$$&lt;/p&gt;
&lt;p&gt;Both of these will prove critical in the following proof.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof Outline&lt;/strong&gt; The proof for the Accelerated Gradient Method is the
trickiest one yet. We'll see that everything fits into place just right, but
that there's very little intuition as to where it all came from.&lt;/p&gt;
&lt;p&gt;We begin in Step 1 by defining a third iterate $v^{(t)}$ that's a
linear combination of $x^{(t)}$ and $x^{(t-1)}$. These iterates are purely for
the sake of the proof and are not computed during the algorithm. Using these
iterates, we upper bound $f(x^{(t+1)})$ in terms of $f(x^{(t)})$, $f(x^{&lt;em&gt;})$,
the distance between $v^{(t+1)}$ and $x^{&lt;/em&gt;}$, and the distance between
$v^{(t)}$ and $x^{*}$.&lt;/p&gt;
&lt;p&gt;Step 2 involves upper bounding the error $f(x^{(t+1)}) - f(x^{&lt;em&gt;})$ by $f(x^{0})
- f(x^{&lt;/em&gt;})$ and the distance between $v^{(0)}$ and $x^{*}$ using our very
careful choice of $\frac{t-1}{t+2}$.&lt;/p&gt;
&lt;p&gt;Finally, Step 3 brings it all together by bounding $f(x^{(t+1)}) - f(x^{*})$ by
a term depending on $1/(t+2)^2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; upper bounding $f(x^{(t+1)})$.  First, define the following and assume that $\theta^{(0)} = 1$, $v^{(0)} = x^{(0)}$,&lt;/p&gt;
&lt;p&gt;$$
  v^{(t)}
  = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} \qquad
  \theta^{(t)}
  = \frac{2}{t+2}
$$&lt;/p&gt;
&lt;p&gt;There are 2 consequences of this definition for $v^{(t)}$. The first is that $y^{(t)}$ is a linear combination of $v^{(t)}$ and $x^{(t)}$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \color{fuchsia} y^{(t)}
  &amp;amp; = x^{(t)} + \frac{t-1}{t+2} ( x^{(t)} - x^{(t-1)} ) \
  &amp;amp; = \frac{(t+2) + (t-1)}{t+2} x^{(t)} - \frac{t-1}{t+2} x^{(t-1)} \
  &amp;amp; = \frac{t+1}{t+2} x^{(t)} - \frac{t-1}{t+2} x^{(t-1)} + \frac{t}{t+2} x^{(t)} \
  &amp;amp; = \frac{2}{t+2} \left( \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} \right) + (1 - \frac{2}{t+2}) x^{(t)} \
  &amp;amp; = \color{fuchsia} \theta^{(t)} v^{(t)} + (1 - \theta^{(t)}) x^{(t)} \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;The second is that $v^{(t)}$ is a gradient step on $v^{(t)}$, except that the gradient is evaluated at $y^{(t)}$ (work backwards from the following),&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \color{OrangeRed} v^{(t+1)}
  &amp;amp; = \color{OrangeRed} v^{(t)} - \frac{\alpha^{(t+1)}}{\theta^{(t)}} \nabla f(y^{(t)}) \
  &amp;amp; = v^{(t)} + \frac{1}{\theta^{(t)}} ( y^{(t)} - \alpha^{(t+1)} \nabla f(y^{(t)}) - y^{(t)} ) \
  &amp;amp; = v^{(t)} + \frac{1}{\theta^{(t)}} ( x^{(t+1)} - y^{(t)} ) \
  \frac{t+2}{2} x^{(t+1)} - \frac{t}{2} x^{(t)}
  &amp;amp; = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} + \frac{1}{\theta^{(t)}} ( x^{(t+1)} - y^{(t)} ) \
  &amp;amp; = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} + \frac{t+2}{2} ( x^{(t+1)} - y^{(t)} ) \
  - \frac{t}{2} x^{(t)}
  &amp;amp; = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} - \frac{t+2}{2} y^{(t)} \
  y^{(t)}
  &amp;amp; = \frac{2t+1}{t+2} x^{(t)} - \frac{t-1}{t+2} x^{(t-1)} \
  &amp;amp; = x^{(t)} + \frac{t-1}{t+2} ( x^{(t)} - x^{(t-1)} )
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;With these 2 properties in hand, let's upper bound $f(x^{(t)})$.  Let $\alpha^{(t)} \le \frac{1}{L}$ and define
$x^{+} \triangleq x^{(t)}$,
$x \triangleq x^{(t-1)}$,
$v^{+} \triangleq v^{(t)}$,
$v \triangleq v^{(t-1)}$,
$\alpha^{+} \triangleq \alpha^{(t)}$,
$y \triangleq y^{(t-1)}$,
$\theta \triangleq \theta^{(t-1)}$&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{+})
  &amp;amp; = f(y - \alpha^{+} \nabla f(y)) \
  &amp;amp; \le {\color{red} f(y) - \frac{\alpha^{+}}{2} || \nabla f(y) ||_2^2} \
  &amp;amp; \le {\color{blue} (1-\theta) f(x) + \theta f(x^{&lt;/em&gt;}) + \nabla f(y)^T (y - (1-\theta) x + \theta x^{&lt;em&gt;}) } - \frac{\alpha^{+}}{2} || \nabla f(y) ||_2^2 \
  &amp;amp; = (1-\theta) f(x) + \theta f(x^{&lt;/em&gt;}) + \nabla f(y)^T ({\color{fuchsia} \theta v} + \theta x^{&lt;em&gt;}) - \frac{\alpha^{+}}{2} || \nabla f(y) ||_2^2 \
  &amp;amp; = (1-\theta) f(x) + \theta f(x^{&lt;/em&gt;}) + \frac{\theta^2}{2 \alpha^{+}} \left(
        \frac{2 \alpha^{+}}{\theta} \nabla f(y)^T (v-x) - \left( \frac{\alpha^{+}}{\theta} \right)^2 || \nabla f(y) ||_2^2
        \pm ||v - x^{&lt;em&gt;}||_2^2
      \right) \
  &amp;amp; = (1-\theta) f(x) + \theta f(x^{&lt;/em&gt;}) + \frac{\theta^2}{2 \alpha^{+}} \left(
        ||v - x^{&lt;em&gt;}||_2^2 - ||v - x^{&lt;/em&gt;} - \frac{\alpha^{+}}{\theta} \nabla f(y)||_2^2 
      \right) \
  &amp;amp; = (1-\theta) f(x) + \theta f(x^{&lt;em&gt;}) + \frac{\theta^2}{2 \alpha^{+}} \left(
        ||v - x^{&lt;/em&gt;}||_2^2 - ||{\color{OrangeRed} v^{+} } - x^{&lt;em&gt;}||_2^2
      \right) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Using the Lipschitz assumption on $||\nabla f(y)||_2$ and that $\alpha^{+} \le \frac{1}{L}$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \color{red}
  f(y - \alpha^{+} \nabla f(y))
  &amp;amp; \le f(y) + \nabla f(y)^T (y - \alpha^{+} \nabla f(y) - y) + \frac{L}{2}{ ||y - \alpha^{+} \nabla f(y) - y ||_2^2 } \
  &amp;amp; = f(y) - \alpha^{+} || \nabla f(y) ||_2^2 + \frac{L (\alpha^{+})^2}{2} || \nabla f(y) ||_2^2 \
  &amp;amp; = f(y) - \frac{\alpha^{+}}{2} ( 2 - L \alpha^{+} ) || \nabla f(y) ||_2^2 \
  &amp;amp; \le \color{red} f(y) - \frac{\alpha^{+}}{2} || \nabla f(y) ||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Using convexity's linear lower bound and its line-over-function properties,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \color{blue}
  f( (1-\theta)x + \theta x^{&lt;/em&gt;} )
  &amp;amp; \ge f(y) + \nabla f(y)^T ( (1-\theta)x + \theta x^{&lt;em&gt;} - y) \
  (1-\theta) f(x) + \theta f(x^{&lt;/em&gt;}) 
  &amp;amp; \ge f(y) + \nabla f(y)^T ( (1-\theta)x + \theta x^{&lt;em&gt;} - y) \
  f(y)
  &amp;amp; \le \color{blue} (1-\theta) f(x) + \theta f(x^{&lt;/em&gt;}) + \nabla f(y)^T(y - (1-\theta) x + \theta x^{&lt;em&gt;} )
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Upper bounding $f(x^{(t+1)}) - f(x^{*})$. We begin by manipulating the result from step 1 into something such that the left and right side look almost the same,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{(t+1)})
  &amp;amp; \le (1-\theta^{(t)}) f(x^{(t)}) + \theta^{(t)} f(x^{&lt;/em&gt;}) \
  &amp;amp; \quad + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} \left(
        ||v^{(t)} - x^{&lt;em&gt;}||_2^2 - \underbrace{ ||v^{(t+1)} - x^{&lt;/em&gt;}||&lt;em _text_move="\text{move" other side to&gt;2^2 }&lt;/em&gt;}
     \right) \
  f(x^{(t+1)}) + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{&lt;em&gt;}||_2^2
  &amp;amp; \le (1-\theta^{(t)}) f(x^{(t)}) + \theta^{(t)} f(x^{&lt;/em&gt;}) \
  &amp;amp; \quad + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t)} - x^{&lt;em&gt;}||_2^2 \underbrace{ \pm f(x^{&lt;/em&gt;}) }&lt;em _text_group="\text{group" together&gt;{=0} \
  f(x^{(t+1)}) - f(x^{&lt;em&gt;}) + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{&lt;/em&gt;}||_2^2
  &amp;amp; \le (1-\theta^{(t)}) f(x^{(t)}) \underbrace{ - f(x^{&lt;em&gt;}) + \theta^{(t)} f(x^{&lt;/em&gt;}) }&lt;/em&gt;} \
  &amp;amp; \quad + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t)} - x^{&lt;em&gt;}||_2^2 \
  f(x^{(t+1)}) - f(x^{&lt;/em&gt;}) + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{&lt;em&gt;}||_2^2
  &amp;amp; \le (1-\theta^{(t)}) f(x^{(t)}) - (1-\theta^{(t)}) f(x^{&lt;/em&gt;}) \
  &amp;amp; \quad + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t)} - x^{&lt;em&gt;}||_2^2 \
  \frac{1}{( \theta^{(t)} )^2} \left( f(x^{(t+1)}) - f(x^{&lt;/em&gt;}) \right) + \frac{1}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{&lt;em&gt;}||_2^2
  &amp;amp; \le \frac{ (1-\theta^{(t)}) }{ (\theta^{(t)})^2 } \left( f(x^{(t)}) - f(x^{&lt;/em&gt;}) \right) \
  &amp;amp; \quad + \frac{1}{2 \alpha^{(t+1)}} ||v^{(t)} - x^{&lt;em&gt;}||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Remember how $\theta^{(t)} = \frac{2}{t+2}$? Well that means $\theta^{(t)} &amp;gt; 0$ and thus $\frac{1 - \theta^{(t)}}{ (\theta^{(t)})^2 } \le \frac{1}{ (\theta^{(t)})^2 }$. Subbing that into the previous equation lets us apply it recursively to obtain,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \frac{1}{( \theta^{(t)} )^2} \left( f(x^{(t+1)}) - f(x^{&lt;/em&gt;}) \right) + \frac{1}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{&lt;em&gt;}||_2^2
  &amp;amp; \le \frac{ (1-\theta^{(0)}) }{ (\theta^{(0)})^2 } \left( f(x^{(0)}) - f(x^{&lt;/em&gt;}) \right) + \frac{1}{2 \alpha^{(1)}} ||v^{(0)} - x^{&lt;em&gt;}||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; Bound the error in terms of $\frac{1}{(t+2)^2}$.  Recall that $\theta^{(0)} = 1$ and $v^{(0)} = x^{(0)}$. Then,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \frac{1}{( \theta^{(t)} )^2} \left( f(x^{(t+1)}) - f(x^{&lt;/em&gt;}) \right) 
    + \underbrace{ \frac{1}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{&lt;em&gt;}||&lt;em 0 _ge="\ge" _text_="\text{," drop so&gt;2^2 }&lt;/em&gt;}
  &amp;amp; \le \underbrace{ \frac{ (1-\theta^{(0)}) }{ (\theta^{(0)})^2 } }_{= 0} \left( f(x^{(0)}) - f(x^{&lt;/em&gt;}) \right)
    + \frac{1}{2 \alpha^{(1)}} ||\underbrace{ v^{(0)} }_{x^{(0)}} - x^{&lt;em&gt;}||_2^2 \
  \frac{1}{( \theta^{(t)} )^2} \left( f(x^{(t+1)}) - f(x^{&lt;/em&gt;}) \right)
  &amp;amp; \le \frac{1}{2 \alpha^{(1)}} || x^{(0)} - x^{&lt;em&gt;} ||_2^2 \
  f(x^{(t+1)}) - f(x^{&lt;/em&gt;})
  &amp;amp; \le \frac{( \theta^{(t)} )^2}{2 \alpha^{(1)}} || x^{(0)} - x^{&lt;em&gt;} ||_2^2 \
  &amp;amp; = \frac{2}{ (t+2)^2 \alpha^{(1)} } || x^{(0)} - x^{&lt;/em&gt;} ||_2^2
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Thus, the convergence rate of the Accelerated Gradient Method is
$O(\frac{1}{\sqrt{\epsilon}})$. Woo!&lt;/p&gt;
&lt;p&gt;&lt;a id="usage"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name="usage" href="#usage"&gt;When should I use it?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The Accelerated Gradient Method trumps Gradient Descent in every way theoretically, but the latter is still more widely used and preferred.  Why? The fact is that Accelerated Gradient is much more of a pain to implement. Whereas with Gradient Descent you can simply check if your new iterate's score is less than the previous one, Accelerated Gradient's score may increase before decreasing again. Accelerated Gradient is also extremely sensitive to step size -- if $\alpha^{(t)}$ isn't in $(0, \frac{1}{L}]$, &lt;em&gt;it will diverge&lt;/em&gt;. Accelerated Gradient is a powerful but fickle tool. Use it when you can, but keep Gradient Descent handy if everything is going awry.&lt;/p&gt;
&lt;h1&gt;&lt;a name="extensions" href="#extensions"&gt;Extensions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Step Size&lt;/strong&gt; As mentioned previously, Accelerated Gradient is very fickle
with step size.  While Backtracking Line Search can and should be used when
possible, it is essential that the constants be set such that $\alpha^{(t+1)}
\le \frac{1}{L}$. In other words, when,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(y^{(t)} - \alpha^{(t+1)} \nabla f(y^{(t)}))
  &amp;amp; \le f(y^{(t)}) + \nabla f(y^{(t)})^T ((y^{(t)} - \alpha^{(t+1)} \nabla f(y^{(t)})) - y^{(t)}) \
  &amp;amp; \quad + \frac{1}{2 \alpha^{(t+1)} } ||(y^{(t)} - \alpha^{(t+1)} \nabla f(y^{(t)})) - y^{(t)}||_2^2 \
  &amp;amp; = f(y^{(t)}) - \alpha^{(t+1)} || \nabla f(y^{(t)}) ||_2^2 + \frac{ \alpha^{(t+1)} }{2} ||\nabla f(y^{(t)}) ||_2^2 \
  &amp;amp; = f(y^{(t)}) - \frac{\alpha^{(t+1)}}{2} || \nabla f(y^{(t)}) ||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Typically the $\frac{1}{2}$ in the last part of the last line is traded for another constant. &lt;em&gt;This will not work for Accelerated Gradient!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Checking Convergence&lt;/strong&gt; As with Gradient Descent and Subgradient Descent,
there is no real way to be certain when $f(x^{(t)}) - f(x^{*}) &amp;lt; \epsilon$
without some problem-specific knowledge. Instead, it is common to stop after a
fixed number of iterations or when $||\nabla f(x^{(t)})||_2$ is small.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Proof of Convergence&lt;/strong&gt; The proof of convergence is thanks to Lieven
Vandenberghe's &lt;a href="http://math.sjtu.edu.cn/faculty/zw2109/course/sp04-2-gradient.pdf"&gt;EE236c slides&lt;/a&gt; hosted by Zaiwen Wen.&lt;/p&gt;
&lt;h1&gt;&lt;a name="reference-impl" href="#reference-impl"&gt;Reference Implementation&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;accelerated_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Accelerated Gradient Method&lt;/span&gt;

&lt;span class="sd"&gt;  Parameters&lt;/span&gt;
&lt;span class="sd"&gt;  ----------&lt;/span&gt;
&lt;span class="sd"&gt;  gradient : function&lt;/span&gt;
&lt;span class="sd"&gt;      Computes the gradient of the objective function at x&lt;/span&gt;
&lt;span class="sd"&gt;  y0 : array&lt;/span&gt;
&lt;span class="sd"&gt;      initial value for x&lt;/span&gt;
&lt;span class="sd"&gt;  alpha : function&lt;/span&gt;
&lt;span class="sd"&gt;      function computing step sizes&lt;/span&gt;
&lt;span class="sd"&gt;  n_iterations : int, optional&lt;/span&gt;
&lt;span class="sd"&gt;      number of iterations to perform&lt;/span&gt;

&lt;span class="sd"&gt;  Returns&lt;/span&gt;
&lt;span class="sd"&gt;  -------&lt;/span&gt;
&lt;span class="sd"&gt;  xs : list&lt;/span&gt;
&lt;span class="sd"&gt;      intermediate values for x&lt;/span&gt;
&lt;span class="sd"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;ys&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;y0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;
    &lt;span class="n"&gt;y_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;BacktrackingLineSearch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;

  &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;
    &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alpha&lt;/span&gt;
    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
      &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mf"&gt;0.99&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yannopt.plotting&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plotting&lt;/span&gt;

  &lt;span class="c1"&gt;### ACCELERATED GRADIENT ###&lt;/span&gt;

  &lt;span class="c1"&gt;# problem definition&lt;/span&gt;
  &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;     &lt;span class="c1"&gt;# the function to minimize&lt;/span&gt;
  &lt;span class="n"&gt;gradient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;  &lt;span class="c1"&gt;# its gradient&lt;/span&gt;
  &lt;span class="n"&gt;alpha&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BacktrackingLineSearch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;x0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
  &lt;span class="n"&gt;n_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

  &lt;span class="c1"&gt;# run gradient descent&lt;/span&gt;
  &lt;span class="n"&gt;iterates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accelerated_gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;### PLOTTING ###&lt;/span&gt;

  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iterates_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/iterates.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iteration_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                      &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/convergence.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="optimization"></category><category term="first-order"></category><category term="accelerated"></category></entry><entry><title>Subgradient Descent</title><link href="/blog/subgradient-descent.html" rel="alternate"></link><published>2013-04-11T00:00:00-07:00</published><updated>2013-04-11T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-04-11:/blog/subgradient-descent.html</id><summary type="html">&lt;p&gt;Not far from &lt;a href="/blog/gradient-descent.html"&gt;Gradient Descent&lt;/a&gt; is another first-order
descent algorithm (that is, an algorithm that only relies on the first
derivative) is Subgradient Descent. In implementation, they are in fact
identical. The only difference is on the assumptions placed on the objective
function we wish to minimize, $f(x)$.  If …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Not far from &lt;a href="/blog/gradient-descent.html"&gt;Gradient Descent&lt;/a&gt; is another first-order
descent algorithm (that is, an algorithm that only relies on the first
derivative) is Subgradient Descent. In implementation, they are in fact
identical. The only difference is on the assumptions placed on the objective
function we wish to minimize, $f(x)$.  If you were to follow the Subgradient
Descent algorithm to walk down a mountain, it would look something like this,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;ol&gt;
&lt;li&gt;Look around you and see which way points the most downwards. If there are multiple directions that are equally downwards, just pick one.&lt;/li&gt;
&lt;li&gt;Take a step in that direction. Then repeat.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;h1&gt;&lt;a name="implementation" href="#implementation"&gt;How does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;As before, we adopt the usual problem definition,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x} \, f(x)
$$&lt;/p&gt;
&lt;p&gt;But this time, we don't assume $f$ is differentiable. Instead, we assume $f$
is convex, implying that for all $x$ there exists a $g_{x}$ such that,&lt;/p&gt;
&lt;p&gt;$$
  f(y) \ge f(x) + g_{x}^T (y - x)
$$&lt;/p&gt;
&lt;p&gt;If $f$ is differentiable at $x$ and is convex, then $\nabla f(x)$ is the only
value for $g_{x}$ that satisfies this property, but if $f$ is convex but
non-differentiable at $x$, there will be other options.&lt;/p&gt;
&lt;p&gt;The set of all $g_x$ that satisfies this property called the
&lt;strong&gt;subdifferential&lt;/strong&gt; of $f$ at $x$ and is denoted $\partial f(x)$. Given that we
have an algorithm for finding a point in the subdifferential, Subgradient
Descent is&lt;/p&gt;
&lt;figure&gt;
  &lt;img src="/assets/img/subgradient_descent/subgradient.png"&gt;&lt;/img&gt;
  &lt;figcaption&gt;
    $f$ is differentiable at $x_1$, so there's only one possible subgradient
    (the actual gradient). At $x_2$, $f$ isn't differentiable, so $g_2$ and
    $g_3$ are both in $\partial f(x_2)$. Image taken from [EE392o slides][subgradient].
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: initial iterate $x^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $t = 0, 1, \ldots$&lt;ol&gt;
&lt;li&gt;if converged, return $x^{(t)}$&lt;/li&gt;
&lt;li&gt;Compute a &lt;a href="http://www.stanford.edu/class/ee392o/subgrad.pdf"&gt;subgradient&lt;/a&gt; of $f$ at $x^{(t)}$, $g^{(t)} \in \partial f(x^{(t)})$&lt;/li&gt;
&lt;li&gt;$x^{(t+1)} = x^{(t)} - \alpha^{(t)} g^{(t)}$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;The initial iterate $x^{(0)}$ can be selected arbitrarily, but $\alpha^{(t)}$
must be selected more carefully than in Gradient Descent. A common choice is
$\frac{1}{t}$.&lt;/p&gt;
&lt;p&gt;&lt;a id="example"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;&lt;a name="example" href="#example"&gt;A Small Example&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Let's watch Subgradient Descent do its thing. We'll use $f(x) = |x|$ as our
objective function, giving us $sign(x)$ as a valid way to compute subgradients.
We'll use the &lt;a href="#polyak"&gt;Polyak Step Size&lt;/a&gt; and initialize with $x^{(0)} = 0.75$.&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/subgradient_descent/convergence.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows how the objective value changes as the number of iterations
    increase. We can see that, unlike Gradient Descent, it isn't strictly
    decreasing. This is expected!
  &lt;/span&gt;
&lt;/div&gt;

&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/subgradient_descent/iterates.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number.
  &lt;/span&gt;
&lt;/div&gt;

&lt;h1&gt;&lt;a name="proof" href="#proof"&gt;Why does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Now let's prove that Subgradient Descent can find $x^{*} = \arg\min_x f(x)$.
We begin by making the following assumptions,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f$ is convex and finite for all $x$&lt;/li&gt;
&lt;li&gt;a finite solution $x^{*}$ exists&lt;/li&gt;
&lt;li&gt;$f$ is Lipschitz with constant $G$. That is,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
  || f(x) - f(y) ||_2 \le G || x - y ||_2 \qquad \forall x,y
$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The initial distance to $x^{*}$ is bounded by $R$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
  || x^{(0)} - x^{*} || \le R
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; Looking back at the convergence proof of Gradient Descent, we
see that the main difference is in assumption 3. Before, we assumed that the
$\nabla f$ was Lipschitz, but now we assume that $f$ is Lipschitz. The
reason for this is because non-smooth functions cannot have a Lipschitz
Subgradient function (Imagine 2 different subgradients for $f$, $g_x$ and
$g_y$, such that $g_x \ne g_y$ and $x = y$. Then $||x-y||_2 = 0$ but $||g_x -
g_y||_2 &amp;gt; 0$).  However, this assumption does guarantee one thing: that $g_x
\le G$ for all $x$.&lt;/p&gt;
&lt;p&gt;Assumption 4 isn't really a condition at all.  It's just a notational
convenience for later.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof Outline&lt;/strong&gt; The proof for Gradient Descent relied on $f(x^{(t)}) -
f(x^{&lt;em&gt;})$ decreasing with each iteration, but the proof for Subgradient Descent
relies on decreasing the (upper bound on) Euclidean distance between $x^{(t)}$
and the set of all possible $x^{&lt;/em&gt;}$.&lt;/p&gt;
&lt;p&gt;We begin by upper bounding the current distance to the optimal point by the
previous distance ($||x^{(t)} - x^{&lt;em&gt;}||_2$), the previous error ($f(x^{(t)}) -
f(x^{&lt;/em&gt;})$), and the norm of the subgradient ($||g^{(t)}||_2$).  Next, we
recursively apply the previous finding across all $t$ to bound the sum of
errors by the &lt;em&gt;initial&lt;/em&gt; distance to $x^{*}$ and the sum of all subgradient
norms.  Then, we lower bound the sum of all errors with a minimum over $t$,
giving us an upper bound on our error at iteration $t+1$. Finally, we use
Assumption 4. to make that bound go to zero.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; Upper bound $||x^{(t+1)} - x^{&lt;em&gt;}||$. Let $x^{&lt;/em&gt;}$ be any point in
$\arg\min_{x} f(x)$. Then,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  ||x^{(t+1)} - x^{&lt;/em&gt;}||_2^2
  = &amp;amp; ||x^{(t)} - \alpha^{(t)} g^{(t)} - x^{&lt;em&gt;}||_2^2
    &amp;amp;&amp;amp; \text{# Definition of $x^{(t+1)}$} \
  = &amp;amp; ||x^{(t)} - x^{&lt;/em&gt;}||_2^2 - 2 \alpha^{(t)} \langle g^{(t)}, x^{(t)} - x^{&lt;em&gt;} \rangle + ( \alpha^{(t)} )^2 ||g^{(t)}||_2^2
    \
  \le &amp;amp; ||x^{(t)} - x^{&lt;/em&gt;}||_2^2 - 2 \alpha^{(t)} ( f(x^{(t)}) - f(x^{&lt;em&gt;}) ) + ( \alpha^{(t)} )^2 ||g^{(t)}||_2^2
    \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Our last step uses $f(x^{&lt;em&gt;}) \ge f(x^{(t)}) + \langle g^{(t)}, x^{&lt;/em&gt;} - x^{(t)} \rangle$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; Upper bound $||x^{(t+1)} - x^{&lt;em&gt;}||$ by $||x^{(0)} - x^{&lt;/em&gt;}||$.
First, we apply Step 1 recursively to bound the current distance to $x^{*}$&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  ||x^{(t+1)} - x^{&lt;/em&gt;}||&lt;em _tau="t-1"&gt;2^2
  \le &amp;amp; ||x^{(t)} - x^{&lt;em&gt;}||_2^2 - 2 \alpha^{(t)} ( f(x^{(t)}) - f(x^{&lt;/em&gt;}) ) + ( \alpha^{(t)} )^2 ||g^{(t)}||_2^2
    \
  \le &amp;amp; \left( ||x^{(t-1)} - x^{&lt;em&gt;}||_2^2 - 2 \alpha^{(t-1)} ( f(x^{(t-1)}) - f(x^{&lt;/em&gt;}) ) + ( \alpha^{(t-1)} )^2 ||g^{(t-1)}||_2^2 \right) \
      &amp;amp; \quad - 2 \alpha^{(t)} ( f(x^{(t)}) - f(x^{&lt;em&gt;}) ) + ( \alpha^{(t)} )^2 ||g^{(t)}||_2^2
      &amp;amp;&amp;amp; \text{# Apply recursion}\
    = &amp;amp; ||x^{(t-1)} - x^{&lt;/em&gt;}||_2^2
        - 2 \sum&lt;/em&gt;^{t} \alpha^{(\tau)} ( f(x^{(\tau)}) - f(x^{&lt;em&gt;}) )
        + \sum_{\tau=t-1}^{t} ( \alpha^{(\tau)} )^2 ||g^{(\tau)}||_2^2 \
  \vdots \
  \le &amp;amp; ||x^{(0)} - x^{&lt;/em&gt;}||&lt;em _tau="0"&gt;2^2
        - 2 \sum&lt;/em&gt;^{t} \alpha^{(\tau)} ( f(x^{(\tau)}) - f(x^{&lt;em&gt;}) )
        + \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )^2 ||g^{(\tau)}||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Then we drop $||x^{(t+1)} - x^{*}||_2^2$ from the left side it's lower bounded by zero,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  0
  \le &amp;amp; ||x^{(0)} - x^{&lt;/em&gt;}||&lt;em _tau="0"&gt;2^2
        - 2 \sum&lt;/em&gt;^{t} \alpha^{(\tau)} ( f(x^{(\tau)}) - f(x^{&lt;em&gt;}) )
        + \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )^2 ||g^{(\tau)}||&lt;em _tau="0"&gt;2^2 \
  2 \sum&lt;/em&gt;^{t} \alpha^{(\tau)} ( f(x^{(\tau)}) - f(x^{&lt;/em&gt;}) )
  \le &amp;amp; ||x^{(0)} - x^{&lt;em&gt;}||&lt;em _tau="0"&gt;2^2
        + \sum&lt;/em&gt;^{t} ( \alpha^{(\tau)} )^2 ||g^{(\tau)}||_2^2 \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; Upper bound current error. First, notice that we can lower bound the
contents of the sum on the left with the minimum across $\tau$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \sum_{\tau=0}^{t} \alpha^{(\tau)} ( f(x^{(\tau)}) - f(x^{&lt;/em&gt;}) )
  \ge &amp;amp; \left( \min_{\tau \in 0 \ldots t} f(x^{(\tau)}) - f(x^{&lt;em&gt;}) \right) \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;Then divide by $2 \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  2 \left( \min_{\tau \in 0 \ldots t} f(x^{(\tau)}) - f(x^{&lt;/em&gt;}) \right) \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )
  \le &amp;amp; 2 \sum_{\tau=0}^{t} \alpha^{(\tau)} ( f(x^{(\tau)}) - f(x^{&lt;em&gt;}) ) \
  \le &amp;amp; ||x^{(0)} - x^{&lt;/em&gt;}||&lt;em _tau="0"&gt;2^2
          + \sum&lt;/em&gt;^{t} ( \alpha^{(\tau)} )^2 ||g^{(\tau)}||&lt;em 0 _in="\in" _ldots="\ldots" _tau="\tau" t&gt;2^2 \
  \left( \min&lt;/em&gt; f(x^{(\tau)}) \right) - f(x^{&lt;em&gt;})
  \le &amp;amp; \frac{
          ||x^{(0)} - x^{&lt;/em&gt;}||&lt;em _tau="0"&gt;2^2
          + \sum&lt;/em&gt;^{t} ( \alpha^{(\tau)} )^2 ||g^{(\tau)}||&lt;em _tau="0"&gt;2^2
        }{
          2 \sum&lt;/em&gt;^{t} ( \alpha^{(\tau)} )
        } \
  \left( \min_{\tau \in 0 \ldots t} f(x^{(\tau)}) \right) - f(x^{&lt;em&gt;})
  \le &amp;amp; \frac{
          R^2
          + G^2 \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )^2
        }{
          2 \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )
        } \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; Making the bound go to zero.  Let $\alpha^{(\tau)} = \frac{R}{G
\sqrt{t}}$ (this is the minimizer of the right hand side for constant
$\alpha^{(\tau)}$). Then,&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  \left( \min_{\tau \in 0 \ldots t} f(x^{(\tau)}) \right) - f(x^{&lt;/em&gt;})
  \le &amp;amp; \frac{
          R^2 + G^2 \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )^2
        }{
          2 \sum_{\tau=0}^{t} ( \alpha^{(\tau)} )
        } \
    = &amp;amp; \frac{
          R^2 + G^2 \frac{R^2}{G^2} \sum_{\tau=0}^{t} \frac{1}{t+1}
        }{
          2 \frac{R}{G} \sum_{\tau=0}^{t} \frac{1}{\sqrt{t+1}}
        } \
    = &amp;amp; \frac{ RG }{ 2 \sqrt{t+1} }
        + \frac{ RG } { 2 \sqrt{t+1} }
    = \frac{ RG }{ \sqrt{t+1} }
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Thus, we can conclude that if we want $f(x^{(t)}) - f(x^{*}) \le \epsilon$,
we need $O(\frac{1}{\epsilon^2})$ iterations. Compared to Gradient
Descent's $O(\frac{1}{\epsilon})$ convergence rate, Subgradient Descent looks
pretty bad!&lt;/p&gt;
&lt;h1&gt;&lt;a name="usage" href="#usage"&gt;When should I use it?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;As the implementation of Gradient Descent and Subgradient Descent are
essentially the same, ease of use is always the first reason to use Subgradient
Descent. Similarly, Subgradient Descent requires a minimal memory footprint,
and has thus found a large following in the large scale machine learning
community.&lt;/p&gt;
&lt;p&gt;As far as black box, first-order for non-differentiable convex problems go,
it can be shown that Subgradient Descent is as (asymptotically) fast as we can
hope for. That doesn't mean Subgradient Descent is as fast as you can get for
your specific problem. Proximal Gradient methods, for example, are one such
family of algorithms that allow you to exploit the properties of differentiable
problems even if your problem isn't.&lt;/p&gt;
&lt;h1&gt;&lt;a name="extensions" href="#extensions"&gt;Extensions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Step Size&lt;/strong&gt; As stated previously, a common choice of step size is
$\alpha^{(t)} = \frac{1}{t}$, but that's far from your only choice. Indeed, any
step rule that satisfies the following conditions works when inserted into the
above proof,&lt;/p&gt;
&lt;p&gt;$$
  \sum_{t=0}^{\infty} \alpha^{(t)} = \infty \qquad
  \sum_{t=0}^{\infty} ( \alpha^{(t)} )^2 &amp;lt; \infty
$$&lt;/p&gt;
&lt;p&gt;For example, $\alpha^{(t)} = \frac{a}{b + t^{c}}$ for positive constants $a$
and $b$ and $c \in (0.5, 1]$ also works. These conditions are referred to as
being square-summable but not summable.&lt;/p&gt;
&lt;p&gt;If $f(x^{*})$ is known ahead of time, another choice is &lt;a href="http://www.stanford.edu/class/ee364b/lectures/subgrad_method_slides.pdf"&gt;Polyak's Step
Size&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;$$
\alpha^{(t)} = \frac{ f(x^{(t)}) - f(x^{*}) }
                    { ||g^{(t)}||_2^2 }
$$&lt;/p&gt;
&lt;p&gt;If $f(x^{*})$ isn't know, then $\alpha^{(t)} = \frac{ f(x^{(t)}) -
f^{(t)}&lt;em best&gt;{best} + \gamma^{(t)} }{ ||g^{(t)}||_2^2 }$ is also valid for
$f^{(t)}&lt;/em&gt; = \min_{\tau \in 0\ldots t} f(x^{(t)})$ and $\gamma^{(t)}$
being square-summable and not summable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Checking Convergence&lt;/strong&gt; In short, there are no easy ways to know when to stop
with Subgradient Descent. Checking if $\nabla f(x)$ is small doesn't make sense
because $\nabla f(x)$ isn't defined at some points and $g_x$ doesn't
necessarily get small near $x \triangleq x^{*}$. Instead, a fixed number of
iterations is typically used.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Proof of Convergence&lt;/strong&gt; The proof of convergence for Subgradient Descent is
taken nearly verbatim from Stephen Boyd's &lt;a href="http://www.stanford.edu/class/ee392o/subgrad_method.pdf"&gt;lecture notes for
EE392o&lt;/a&gt; course in 2003.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Polyak Step Size&lt;/strong&gt; The algorithm for the Polyak step size was taken from
page 23 of Stephen Boyd's &lt;a href="http://www.stanford.edu/class/ee364b/lectures/subgrad_method_slides.pdf"&gt;lecture slides for EE364b&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a name="reference-impl" href="#reference-impl"&gt;Reference Implementation&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;subgradient_descent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;subgradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Subgradient Descent&lt;/span&gt;

&lt;span class="sd"&gt;  Parameters&lt;/span&gt;
&lt;span class="sd"&gt;  ----------&lt;/span&gt;
&lt;span class="sd"&gt;  function : function&lt;/span&gt;
&lt;span class="sd"&gt;      Computes the objective function&lt;/span&gt;
&lt;span class="sd"&gt;  subgradient : function&lt;/span&gt;
&lt;span class="sd"&gt;      Computes a gradient for the objective function at x&lt;/span&gt;
&lt;span class="sd"&gt;  x0 : array&lt;/span&gt;
&lt;span class="sd"&gt;      initial value for x&lt;/span&gt;
&lt;span class="sd"&gt;  alpha : function&lt;/span&gt;
&lt;span class="sd"&gt;      function computing step sizes&lt;/span&gt;
&lt;span class="sd"&gt;  n_iterations : int, optional&lt;/span&gt;
&lt;span class="sd"&gt;      number of iterations to perform&lt;/span&gt;

&lt;span class="sd"&gt;  Returns&lt;/span&gt;
&lt;span class="sd"&gt;  -------&lt;/span&gt;
&lt;span class="sd"&gt;  xs : list&lt;/span&gt;
&lt;span class="sd"&gt;      intermediate values for x&lt;/span&gt;
&lt;span class="sd"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="n"&gt;x_best&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subgradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_best&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_best&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
      &lt;span class="n"&gt;x_best&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x_plus&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;polyak&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f_x_best&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f_x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;f_x_best&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yannopt.plotting&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plotting&lt;/span&gt;

  &lt;span class="c1"&gt;### SUBGRADIENT DESCENT ###&lt;/span&gt;

  &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;
  &lt;span class="n"&gt;subgradient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign&lt;/span&gt;
  &lt;span class="n"&gt;x0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.75&lt;/span&gt;
  &lt;span class="n"&gt;n_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

  &lt;span class="n"&gt;iterates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subgradient_descent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;subgradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;polyak&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;iterates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;### PLOTTING ###&lt;/span&gt;

  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iterates_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/iterates.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iteration_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                      &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/convergence.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="optimization"></category><category term="first-order"></category><category term="subgradient"></category></entry><entry><title>Gradient Descent</title><link href="/blog/gradient-descent.html" rel="alternate"></link><published>2013-04-10T00:00:00-07:00</published><updated>2013-04-10T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-04-10:/blog/gradient-descent.html</id><summary type="html">&lt;p&gt;Gradient Descent is perhaps the most intuitive of all optimization
algorithms. Imagine you're standing on the side of a mountain and want to reach
the bottom. You'd probably do something like this,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;ol&gt;
&lt;li&gt;Look around you and see which way points the most downwards&lt;/li&gt;
&lt;li&gt;Take a step in that direction, then …&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Gradient Descent is perhaps the most intuitive of all optimization
algorithms. Imagine you're standing on the side of a mountain and want to reach
the bottom. You'd probably do something like this,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;ol&gt;
&lt;li&gt;Look around you and see which way points the most downwards&lt;/li&gt;
&lt;li&gt;Take a step in that direction, then repeat&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;Well that's Gradient Descent!&lt;/p&gt;
&lt;h1&gt;&lt;a name="implementation" href="#implementation"&gt;How does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;So how do we frame Gradient Descent mathematically? As usual, we define our
problem in terms of minimizing a function,&lt;/p&gt;
&lt;p&gt;$$
  \min_{x} \, f(x)
$$&lt;/p&gt;
&lt;p&gt;We assume that $f$ is differentiable. That is, we can easily compute,&lt;/p&gt;
&lt;p&gt;$$
  \nabla_x \, f(x) = \begin{pmatrix}
    \frac{d f(x)}{d x_1} \
    \frac{d f(x)}{d x_2} \
    \vdots \
  \end{pmatrix}
$$&lt;/p&gt;
&lt;p&gt;Given this, Gradient Descent is simply the following,&lt;/p&gt;
&lt;p&gt;&lt;!-- TODO Replace well with something more contextually meaningful --&gt;
&lt;div class="pseudocode" markdown&gt;
  &lt;strong&gt;Input&lt;/strong&gt;: initial iterate $x^{(0)}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For $t = 0, 1, \ldots$&lt;ol&gt;
&lt;li&gt;if converged, return $x^{(t)}$&lt;/li&gt;
&lt;li&gt;Compute the &lt;a href="http://en.wikipedia.org/wiki/Gradient"&gt;gradient&lt;/a&gt; of $f$ at $x^{(t)}$, $g^{(t)}
   \triangleq \nabla f(x^{(t)})$&lt;/li&gt;
&lt;li&gt;$x^{(t+1)} = x^{(t)} - \alpha^{(t)} g^{(t)}$
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The initial iterate $x^{(0)}$ can be selected arbitrarily, and step size
$\alpha^{(t)}$ can be selected by &lt;a href="#line_search"&gt;Line Search&lt;/a&gt;, a small constant, or
simply $\frac{1}{t}$.&lt;/p&gt;
&lt;h1&gt;&lt;a name="example" href="#example"&gt;A Small Example&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Let's look at Gradient Descent in action. We'll use the objective function
$f(x) = x^4$, meaning that $\nabla_x f(x) = 4 x^3$. For a step size, we'll
choose a constant step size $\alpha_t = 0.05$. Finally, we'll start at $x^{(0)}
= 1$.&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/gradient_descent/animation.gif"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    Gradient Descent in action. The curved line is the $f(x)$, and the flat
    line is its linear approximation, $\hat{f}(y) = f(x) + \nabla_x f(x)^T
    (y-x)$, which is what Gradient Descent follows.
  &lt;/span&gt;
&lt;/div&gt;

&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/gradient_descent/convergence.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows how quickly the objective function decreases as the
    number of iterations increases.
  &lt;/span&gt;
&lt;/div&gt;

&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/gradient_descent/iterates.png"&gt;&lt;/img&gt;
  &lt;span class="caption"&gt;
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number.
  &lt;/span&gt;
&lt;/div&gt;

&lt;h1&gt;&lt;a name="proof" href="#proof"&gt;Why does it work?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Gradient Descent works, but it isn't guaranteed to find the optimal solution
to our problem (that is, $x^{*} = \arg\min_{x} f(x)$) without a few assumptions.
In particular,&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$f$ is convex and finite for all $x$&lt;/li&gt;
&lt;li&gt;a finite solution $x^{*}$ exists&lt;/li&gt;
&lt;li&gt;$\nabla f(x)$ is Lipschitz continuous with constant $L$. If $f$ is twice
   differentiable, this means that the largest eigenvalue of the Hessian is
   bounded by $L$ ($\nabla^2 f(x) \preceq LI$). But more directly, there must
   be an $L$ such that,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$
  || \nabla f(x) - \nabla f(y) ||_2 \le L || x - y ||_2 \qquad \forall x,y
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; So what do these assumptions give us?  Assumption 1 tells us
that $f$ is lower bounded by an affine function,&lt;/p&gt;
&lt;p&gt;$$
  f(y) \ge f(x) + \nabla f(x)^T (y - x)  \qquad \forall x,y
$$&lt;/p&gt;
&lt;p&gt;Assumption 3 also tells us that $f$ is upper bounded by a quadratic (this is
not obvious),&lt;/p&gt;
&lt;p&gt;$$
  f(y) \le f(x) + \nabla f(x)^T (y - x) + \frac{L}{2} || y - x ||_2^2
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof Outline&lt;/strong&gt; Now let's dive into the proof. Our plan of attack is as
follows. First, we upper bound the error $f(x^{(t+1)}) - f(x^{&lt;em&gt;})$ in terms of
$||x^{(t)} - x^{&lt;/em&gt;}||_2^2$ and $||x^{(t+1)} - x^{&lt;em&gt;}||_2^2$.  We then sum these upper
bounds across $t$ to upper bound the sum of errors in terms of $||x^{(0)} -
x^{&lt;/em&gt;}||_2^2$. Finally, we use the fact that $f(x^{(t)})$ is decreasing in $t$ to
take an average of that sum to bound $f(x^{(t+1)}) - f(x^{&lt;em&gt;})$ in terms of $||x^{(0)}
- x^{&lt;/em&gt;}||_2^2$ and $t$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: upper bounding $f(x^{(t+1)}) - f(x^{*})$.  Let $x^{+} \triangleq
x^{(t+1)}$, $x \triangleq x^{(t)}$, and $\alpha \triangleq \alpha^{(t)}$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{+})
  \le &amp;amp; f(x) + \nabla f(x)^T (x^{+} - x) + \frac{L}{2}||x^{+} - x||_2^2 &amp;amp;&amp;amp; \text{# Quadratic upper bound} \
  = &amp;amp; f(x) + \nabla f(x)^T (- \alpha \nabla f(x)) + \frac{L}{2}||- \alpha \nabla f(x)||_2^2 &amp;amp;&amp;amp; \text{# Definition of $x^{+}$} \
  = &amp;amp; f(x) - \alpha || \nabla f(x) ||_2^2 + \frac{\alpha^2 L}{2} ||\nabla f(x)||_2^2 \
  = &amp;amp; f(x) - \alpha\left( 1 - \frac{\alpha L}{2} \right) || \nabla f(x) ||_2^2  \
  \le &amp;amp; f(x) - \frac{\alpha}{2} || \nabla f(x) ||_2^2  &amp;amp;&amp;amp; \text{# Assuming $\frac{\alpha L}{2} \leq \frac{1}{2}$} \
  \le &amp;amp; f(x^{&lt;/em&gt;}) + \nabla f(x)^T (x - x^{&lt;em&gt;}) - \frac{\alpha}{2} || \nabla f(x) ||_2^2  &amp;amp;&amp;amp; \text{# Linear lower bound on $f(x)$} \
  = &amp;amp; f(x^{&lt;/em&gt;}) + \nabla f(x)^T (x - x^{&lt;em&gt;}) - \frac{\alpha}{2} || \nabla f(x) ||_2^2 \
    &amp;amp; \quad \pm \frac{1}{2 \alpha} \left( ||x||_2^2 + ||x^{&lt;/em&gt;}||_2^2 + x^T x^{&lt;em&gt;} \right)\
  = &amp;amp; f(x^{&lt;/em&gt;}) + \frac{1}{2 \alpha} \left(
      ||x - x^{&lt;em&gt;}||_2^2 - ||(x - \alpha \nabla f(x)) - x^{&lt;/em&gt;}||_2^2
    \right) \
  = &amp;amp; f(x^{&lt;em&gt;}) + \frac{1}{2 \alpha} \left(
      ||x - x^{&lt;/em&gt;}||_2^2 - ||x^{+} - x^{&lt;em&gt;}||_2^2
    \right) \
\end{align&lt;/em&gt;}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Upper bound $\sum_{t=1}^{T} f(x^{(t)}) - f(x^{*})$ by summing across
all $t$. At this point we'll assume that $\alpha^{(t)}$ is the same for all
$t$.&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{(t)}) - f(x^{&lt;/em&gt;})
  &amp;amp; \le \frac{1}{2 \alpha^{(t)}} \left(
    ||x^{(t)} - x^{&lt;em&gt;}||_2^2 - ||x^{(t+1)} - x^{&lt;/em&gt;}||&lt;em t="1"&gt;2^2
  \right) \
  \sum&lt;/em&gt;^{T} f(x^{(t)}) - f(x^{&lt;em&gt;})
  &amp;amp; \le \frac{1}{2 \alpha} \sum_{t=1}^{T} \left(
    ||x^{(t)} - x^{&lt;/em&gt;}||_2^2 - ||x^{(t+1)} - x^{&lt;em&gt;}||_2^2
  \right) \
  &amp;amp; = \frac{1}{2 \alpha} \left(
    ||x^{(0)} - x^{&lt;/em&gt;}||_2^2 - ||x_1 - x^{&lt;em&gt;}||_2^2 + ||x_1 - x^{&lt;/em&gt;}||_2^2 - ||x_2 - x^{&lt;em&gt;}||_2^2 + \ldots
  \right) \
  &amp;amp; = \frac{1}{2 \alpha} \left( ||x^{(0)} - x^{&lt;/em&gt;}||_2^2 - ||x^{(t)} - x^{&lt;em&gt;}||_2^2
  \right) \
  &amp;amp; \le \frac{1}{2 \alpha} ||x^{(0)} - x^{&lt;/em&gt;}||_2^2 \
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Upper bound $f(x^{(t+1)}) - f(x^{*})$ by using the fact that
$f(x^{(t+1)}) &amp;lt; f(x^{(t)})$&lt;/p&gt;
&lt;p&gt;$$
\begin{align&lt;em&gt;}
  f(x^{(T)}) - f(x^{&lt;/em&gt;})
  &amp;amp; \le \frac{1}{T} \sum_{t=1}^{T} ( f(x^{(t)}) - f(x^{&lt;em&gt;}) ) \
  &amp;amp; \le \frac{1}{2 \alpha T} ||x^{(0)} - x^{&lt;/em&gt;}||_2^2
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Thus, we can conclude that if we want to reach an error tolerance $f(x^{T}) -
f(x^{*}) \le \epsilon$, we need $O(\frac{1}{\epsilon})$ iterations.  In other
words, Gradient Descent has a "convergence rate" of $O(\frac{1}{T})$.&lt;/p&gt;
&lt;h1&gt;&lt;a name="usage" href="#usage"&gt;When should I use it?&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Because it's so easy to implement, Gradient Descent should be the first thing
to try if you need to implement an optimization from scratch. So long as you
calculate the gradient right, it's practically impossible to make a mistake.
If you have access to an &lt;a href="http://justindomke.wordpress.com/2009/02/17/automatic-differentiation-the-most-criminally-underused-tool-in-the-potential-machine-learning-toolbox/"&gt;automatic differentiation library&lt;/a&gt; to do
the gradient computation for you, even better!  In addition, Gradient Descent
requires a minimal memory footprint, making it ideal for problems where $x$ is
very high dimensional.&lt;/p&gt;
&lt;p&gt;As we'll see in later posts, Gradient Descent trades memory for speed. The
number of iterations required to reach a desired accuracy is actually quite
large if you want accuracy on the order of $10^{-8}$, and there are algorithms
that are much faster if computation of the &lt;a href="http://en.wikipedia.org/wiki/Hessian_matrix"&gt;Hessian&lt;/a&gt; is feasible.
Even when considering the same memory requirements, there is another
gradient-based method with better convergence rates. &lt;/p&gt;
&lt;h1&gt;&lt;a name="extensions" href="#extensions"&gt;Extensions&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Step Size&lt;/strong&gt; The proof above relies on a constant step size, but quicker
convergence can be obtained when using &lt;a href="#line_search"&gt;Line Search&lt;/a&gt;, wherein
$\alpha^{(t)}$ is chosen to (approximately) find $\alpha^{(t)} = \arg\min_{\alpha}
f(x^{(t)} - \alpha \nabla f(x^{(t)}))$. Keep in mind that unless $0 \le t \le
\frac{1}{L}$, &lt;em&gt;Gradient Descent will not converge!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Checking Convergence&lt;/strong&gt; We have shown that the algorithm's error at iteration
$T$ relies on $T$ and the distance between $x^{(0)}$ and $x^{&lt;em&gt;}$, the latter of
which is unknown.  How then can we check if we're "close enough"? A typical
choice is simply to stop after a fixed number of iterations, but another common
alternative is to quit when $||\nabla f(x^{(t)})||&lt;em g&gt;2 &amp;lt; \epsilon&lt;/em&gt;$ for a
chosen $\epsilon_{g}$.  The intuition for this comes from the assumption that
$f$ is "strongly convex" with constant $m$, which then implies that $||x -
x^{&lt;/em&gt;}||_2 \le \frac{2}{m}||\nabla f(x)||_2$ (see &lt;a href="http://www.stanford.edu/~boyd/cvxbook/"&gt;Convex
Optimization&lt;/a&gt;, page 460, equation 9.10).&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Proof of Convergence&lt;/strong&gt; The proof of convergence for Gradient Descent is
adapted from slide 1-18 of of UCLA's &lt;a href="http://www.ee.ucla.edu/~vandenbe/236C/lectures/gradient.pdf"&gt;EE236C lecture on Gradient
Methods&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a id="line_search"&gt;&lt;/a&gt;
  &lt;strong&gt;Line Search&lt;/strong&gt; The algorithm for Backtracking Line Search, a smart method
for choosing step sizes, can be found on slide 10-6 of UCLA's &lt;a href="http://www.ee.ucla.edu/ee236b/lectures/unconstrained.pdf"&gt;EE236b lecture
on unconstrained optimization&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;&lt;a name="reference-impl" href="#reference-impl"&gt;Reference Implementation&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Here's a quick implementation of gradient descent,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;gradient_descent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Gradient Descent&lt;/span&gt;

&lt;span class="sd"&gt;  Parameters&lt;/span&gt;
&lt;span class="sd"&gt;  ----------&lt;/span&gt;
&lt;span class="sd"&gt;  gradient : function&lt;/span&gt;
&lt;span class="sd"&gt;      Computes the gradient of the objective function at x&lt;/span&gt;
&lt;span class="sd"&gt;  x0 : array&lt;/span&gt;
&lt;span class="sd"&gt;      initial value for x&lt;/span&gt;
&lt;span class="sd"&gt;  alpha : function&lt;/span&gt;
&lt;span class="sd"&gt;      function computing step sizes&lt;/span&gt;
&lt;span class="sd"&gt;  n_iterations : int, optional&lt;/span&gt;
&lt;span class="sd"&gt;      number of iterations to perform&lt;/span&gt;

&lt;span class="sd"&gt;  Returns&lt;/span&gt;
&lt;span class="sd"&gt;  -------&lt;/span&gt;
&lt;span class="sd"&gt;  xs : list&lt;/span&gt;
&lt;span class="sd"&gt;      intermediate values for x&lt;/span&gt;
&lt;span class="sd"&gt;  &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
  &lt;span class="n"&gt;xs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;
    &lt;span class="n"&gt;xs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;xs&lt;/span&gt;

&lt;span class="c1"&gt;# This generates the plots that appear above&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;

  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pylab&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pl&lt;/span&gt;
  &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;yannopt.plotting&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plotting&lt;/span&gt;

  &lt;span class="c1"&gt;### GRADIENT DESCENT ###&lt;/span&gt;

  &lt;span class="c1"&gt;# problem definition&lt;/span&gt;
  &lt;span class="n"&gt;function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;  &lt;span class="c1"&gt;# the function to minimize&lt;/span&gt;
  &lt;span class="n"&gt;gradient&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;  &lt;span class="c1"&gt;# its gradient&lt;/span&gt;
  &lt;span class="n"&gt;step_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;
  &lt;span class="n"&gt;x0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;
  &lt;span class="n"&gt;n_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

  &lt;span class="c1"&gt;# run gradient descent&lt;/span&gt;
  &lt;span class="n"&gt;iterates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradient_descent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;step_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;### PLOTTING ###&lt;/span&gt;

  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iterates_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                     &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/iterates.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;plotting&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_iteration_vs_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                      &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/convergence.png&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_star&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="c1"&gt;# make animation&lt;/span&gt;
  &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;makedirs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/animation&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;OSError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;pass&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;x_plus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iterates&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;
    &lt;span class="n"&gt;f_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;x_min&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;
    &lt;span class="n"&gt;x_max&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;

    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;f(x)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_max&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f_hat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_min&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;f_hat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_max&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x_plus&lt;/span&gt;&lt;span class="p"&gt;)],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;savefig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;figures/animation/&lt;/span&gt;&lt;span class="si"&gt;%02d&lt;/span&gt;&lt;span class="s1"&gt;.png&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;pl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="gradient"></category><category term="descent"></category><category term="first-order"></category><category term="optimization"></category></entry><entry><title>Topic Models aren't hard</title><link href="/blog/topic-models-arent-hard.html" rel="alternate"></link><published>2013-01-21T00:00:00-08:00</published><updated>2013-01-21T00:00:00-08:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2013-01-21:/blog/topic-models-arent-hard.html</id><summary type="html">&lt;p&gt;In 2002, &lt;a href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf"&gt;Latent Dirichlet Allocation&lt;/a&gt; (LDA) was published at NIPS, one
of the most highly regarded conferences for research loosely labeled as
"Artificial Intelligence". The next 5 or so years led to a flurry of
incremental model extensions and alternative inference methods, though none
have achieved the popularity of their …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In 2002, &lt;a href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf"&gt;Latent Dirichlet Allocation&lt;/a&gt; (LDA) was published at NIPS, one
of the most highly regarded conferences for research loosely labeled as
"Artificial Intelligence". The next 5 or so years led to a flurry of
incremental model extensions and alternative inference methods, though none
have achieved the popularity of their namesake.&lt;/p&gt;
&lt;p&gt;Latent Dirichlet Allocation -- an extremely complex name for a not-so-complex
idea. In this post, I will explain what the LDA model says, what it does &lt;em&gt;not&lt;/em&gt;
say, and how we as researchers should look at it.&lt;/p&gt;
&lt;h1&gt;&lt;a name="model" href="#model"&gt;The Model&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Let's begin by appreciating Latent Dirichlet Allocation in its most natural
form -- the graphical model.  I hope you like Greek letters...&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/lda/graphical-model.png"&gt;&lt;/img&gt;
&lt;/div&gt;

&lt;p&gt;About now, you should have an ephemeral feeling of happiness and
understanding beyond anything you've ever experienced before, as if your eyes
had just opened for the first time.  Do you feel it? No? Yeah, I didn't think
so.&lt;/p&gt;
&lt;p&gt;Let's break it down a little, without the math.  Take a look at the following
4 plots.  Each subplot contains samples drawn from 1 of 3 clusters, and each
plot contains samples from the same clusters.  The difference between each
subplot is that the &lt;em&gt;number of samples&lt;/em&gt; from each cluster is different.&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/lda/gaussians-nocolor.jpg"&gt;&lt;/img&gt;
&lt;/div&gt;

&lt;p&gt;Having trouble?  It's a rather difficult problem, especially with only 4
subplots. What if you had a 100,000 subplots instead? Do you think you could
figure it out then?  Here's a plot of the same data with points colored
according to their cluster,&lt;/p&gt;
&lt;div class="img-center"&gt;
  &lt;img src="/assets/img/lda/gaussians-color.jpg"&gt;&lt;/img&gt;
&lt;/div&gt;

&lt;p&gt;Even if you don't realize it yet, you now understand Latent Dirichlet
Allocation. In fact, Latent Dirichlet Allocation is just an extension of the
lowly Mixture Model.  "How so?", you ask?  Well let's look at how we might
generate data from a Mixture Model.&lt;/p&gt;
&lt;p&gt;In a mixture model, each data point is sampled independently. The algorithm
for generating a sample given the model's parameters is given by the following
python snippet.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample_mixture_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_data_points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cluster_weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cluster_parameters&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_data_points&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;cluster&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample_categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cluster_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cluster_parameters&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cluster&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="n"&gt;sample_normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variance&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Simple, right?  First, a cluster is chosen for this data point. Each cluster
has some probability of being chosen, given by &lt;code&gt;cluster_weights[i]&lt;/code&gt;.  Once a
cluster has been chosen, the data point is generated from a Normal distribution
with mean and covariance specific to the cluster.  The idea is that each
cluster has its own mean and covariance, so with enough samples we'll be able
to tell the clusters apart.&lt;/p&gt;
&lt;p&gt;So how does this relate to LDA?  In LDA, each "document" (in our case,
subplot) is nothing more than a Mixture Model. The novel part of LDA is that
there isn't just one document that we see a ton of samples from, but many
documents that we only see a few samples from.  Furthermore, each document has
its own version of &lt;code&gt;cluster_weights&lt;/code&gt; -- our only boon is that all documents
share the same &lt;code&gt;cluster_parameters&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To make that concrete, let's look at how we would generate samples from LDA.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sample_lda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_data_points_per_document&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;all_cluster_weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cluster_parameters&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;n_documents&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;all_cluster_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# number of documents&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_documents&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sample_mixture_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_data_points_per_document&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                       &lt;span class="n"&gt;all_cluster_weights&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
                                       &lt;span class="n"&gt;cluster_parameters&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
      &lt;span class="k"&gt;yield&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;document_number&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;data_point&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that here we don't just return the data point by itself.  In LDA, we
know which "document" each data point comes from, which is just a little bit
more information than we have in a regular old Mixture Model.&lt;/p&gt;
&lt;p&gt;Finally, I have to admit that I lied a little.  What I've described so far
isn't &lt;em&gt;quite&lt;/em&gt; LDA, but it's pretty damn close.  In the above pseudocode, I
assumed that the model parameters were already given, but LDA actually
assumes the parameters are unknown and defines a probability distribution over
them (a &lt;a href="http://en.wikipedia.org/wiki/Dirichlet_distribution"&gt;Dirichlet distribution&lt;/a&gt;, in fact).  Secondly, the examples
above generate data points from Normal distributions where as LDA generates
samples from the &lt;a href="http://en.wikipedia.org/wiki/Multinomial_distribution"&gt;Multinomial distribution&lt;/a&gt;. Other than that, you
now understand Latent Dirichlet Allocation, the core of nearly every Topic
Model in existence.&lt;/p&gt;
&lt;h1&gt;&lt;a name="appendix" href="#appendix"&gt;Appendix&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Here's the MATLAB code for generating the two plots above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;%% parameters&lt;/span&gt;
&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;n_clusters&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;n_documents&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c"&gt;%% reset ye olde random seed&lt;/span&gt;
&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;mcg16807&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Seed&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;RandStream&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setDefaultStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="c"&gt;%% generate parameters for each cluster&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nb"&gt;i&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n_clusters&lt;/span&gt;
  &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;(:,:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;eye&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;(:,:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;(:,:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;(:,:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;(:,:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;(:,:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;

&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;hold&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;n_documents&lt;/span&gt;
  &lt;span class="c"&gt;%% generate document-specific weights&lt;/span&gt;
  &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_clusters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

  &lt;span class="c"&gt;%% generate samples for this document&lt;/span&gt;
  &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nb"&gt;i&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;rand&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mnrnd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mvnrnd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;(:,&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;(:,:,&lt;/span&gt;&lt;span class="n"&gt;z&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)));&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;

  &lt;span class="c"&gt;%% plotting! yay!&lt;/span&gt;
  &lt;span class="n"&gt;subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

  &lt;span class="c"&gt;% without color&lt;/span&gt;
  &lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,:),&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,:));&lt;/span&gt;
  &lt;span class="c"&gt;% with color&lt;/span&gt;
  &lt;span class="c"&gt;% scatter(x(1,:), x(2,:), &amp;#39;CData&amp;#39;, c&amp;#39;);&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="topic-models"></category><category term="bayesian"></category><category term="lda"></category></entry><entry><title>ADMM: parallelizing convex optimization</title><link href="/blog/admm.html" rel="alternate"></link><published>2012-06-24T00:00:00-07:00</published><updated>2012-06-24T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2012-06-24:/blog/admm.html</id><summary type="html">&lt;p&gt;In the previous post, we considered Stochastic Gradient Descent, a popular method for optimizing "separable" functions (that is, functions that are purely sums of other functions) in a large, distributed environment. However, Stochastic Gradient Descent is not the only algorithm out there.&lt;/p&gt;
&lt;p&gt;So why consider anything else? First of all …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the previous post, we considered Stochastic Gradient Descent, a popular method for optimizing "separable" functions (that is, functions that are purely sums of other functions) in a large, distributed environment. However, Stochastic Gradient Descent is not the only algorithm out there.&lt;/p&gt;
&lt;p&gt;So why consider anything else? First of all, we have to choose step sizes $\alpha_{t,i}$. While there are theoretical constraints on how it must behave (e.g. $\alpha_t = \frac{1}{t^k}$ is guaranteed to converge), there is a lot of freedom in the constants, and finding just the right one can be painful. It often ends up that even though Stochastic Gradient Descent guarantees an asymptotic convergence rate, you only have enough time to make a handful of passes over the dataset, far too little time for the asymptotics to kick in.&lt;/p&gt;
&lt;p&gt;Secondly, Stochastic Gradient Descent is naturally &lt;em&gt;sequential&lt;/em&gt;. You have to update $w_{t,i}$ before you can update $w_{t,i+1}$ (well, not quite. See &lt;a href="http://arxiv.org/abs/1106.5730"&gt;HOGWILD!&lt;/a&gt;). This means that Stochastic Gradient Descent is great for data streaming in one-by-one, but isn't of much help in MapReduce-style frameworks.&lt;/p&gt;
&lt;p&gt;Alternating Direction Method of Multipliers (ADMM) is an entirely different method of distributed optimization that is far better oriented for MapReduce and which only requires a single parameter to specify the learning rate. However, using it requires quite a bit more mathematical preparation.&lt;/p&gt;
&lt;p&gt;The basic idea is that if we have an optimization problem specified as follows,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  &amp;amp; \min_{x,z} f(x) + g(z)  \
  &amp;amp; \text{s.t. } A x + B z = c
\end{align}
$$&lt;/p&gt;
&lt;p&gt;Then we can derive the Lagrangian and add a quadratic penalty for violating the constraint,&lt;/p&gt;
&lt;p&gt;$$
L_{\rho}(x,z,y) = f(x) + g(z) + y^T (Ax + Bz -c) + \frac{\rho}{2} || Ax + Bz - c ||_2^2
$$&lt;/p&gt;
&lt;p&gt;Finally we apply the following algorithm&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt; Initial primal and dual iterates $x_{0}$, $z_{0}$, and $y_{0}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Optimize over the first primal variable,&lt;/p&gt;
&lt;p&gt;$$
  x_{t+1} = \text{argmin}&lt;em _rho="\rho"&gt;x L&lt;/em&gt;(x,z_t, y_t)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optimize over the second primal variable,&lt;/p&gt;
&lt;p&gt;$$
  z_{t+1} = \text{argmin}&lt;em _rho="\rho"&gt;x L&lt;/em&gt;(x_{t+1},z, y_t)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Take a gradient step for the dual variable&lt;/p&gt;
&lt;p&gt;$$
  y_{t+1} = y_t + \rho (A x_{t+1} + B z_{t+1} - c)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;Notice the choice of step size for updating $y_t$ and the addition of a quadratic term to the Lagrangian; these are the critical addition of ADMM.&lt;/p&gt;
&lt;p&gt;The question now becomes, how can we apply this seemingly restricted method to make a distributed algorithm? Suppose we want to minimize our usual separable function&lt;/p&gt;
&lt;p&gt;$$
\min_x \sum_i f_i(x)
$$&lt;/p&gt;
&lt;p&gt;We can reformulate this problem by giving each $f_i$ its own $x_i$, and requiring that $x_i = z$ at the very end.&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  &amp;amp; \min_{x_i, z} \sum_i f_i(x_i)   \
  &amp;amp; \text{s.t.} \quad \forall i \quad x_i = z
\end{align}
$$&lt;/p&gt;
&lt;p&gt;This means that we can optimize each $x_i$ independently, then aggregate their solutions to update $z$ (the one true $x$), and finally use both of those to update $y$. Let's see how this works out exactly. The augmented Lagrangian would be,&lt;/p&gt;
&lt;p&gt;$$
L_{\rho}(x,z,y) = \sum_{i} \left( 
    f_i(x_i) + y^T (x_i - z) + \frac{\rho}{2} || x_i - z ||_2^2
  \right)
$$&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt; Initial primal and dual iterates $x_{0}$, and $y_{0}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For each machine $i$ in parallel, optimize the local variable $x_i$&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  x_{t+1, i} &amp;amp; = \text{argmin}&lt;em t_i="t,i"&gt;x f_i(x) 
    + y&lt;/em&gt;^T (x - z_t) 
    + \frac{\rho}{2} (x-z)^T (x-z) \
\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aggregate the resulting $x_{t,i+1}$ and optimize the global variable $z$,&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
  z_{t+1} &amp;amp;= \text{argmin}&lt;em t_i="t,i"&gt;z y&lt;/em&gt;^T (x_{t+1, i} - z) 
    + \frac{\rho}{2} (x_{t+1, i} - z)^T (x_{t+1, i} - z)  \
  &amp;amp;= \frac{1}{N} \sum_{i=1}^{N} \left( 
    x_{t+1, i} + \frac{1}{\rho} y_{t, i}
  \right)
\end{align}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the dual variables $y_{t,i}$,&lt;/p&gt;
&lt;p&gt;$$
  y_{t+1, i} = y_{t, i} + \rho ( x_{t+1,i} - z_{t+1} )
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;This is already pretty cool, but there's even more. It ends up that ADMM works splendidly even when we add a regularization penalty to the primal problem, such as the $L_2$ or $L_1$ norm. You can find out all of these cool things and more in the Stephen Boyd's &lt;a href="http://www.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf"&gt;paper&lt;/a&gt; and &lt;a href="http://videolectures.net/nipsworkshops2011_boyd_multipliers/"&gt;lecture&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On a final note, the proofs on convergence for ADMM are currently not as complete as those for other methods like Stochastic Gradient Descent. While it is known that the dual variable $y_t$ will converge as long as $f$ and $g$ are convex and a solution exists, we can only prove convergence of the primal variables $x_t$ and $z_t$ if they are constrained to lie in a polyhedron at this point in time.&lt;/p&gt;
&lt;h1&gt;&lt;a name="references" href="#references"&gt;References&lt;/a&gt;&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf"&gt;Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/pdf/1112.2295.pdf"&gt;A Proof of Convergence For the Alternating Direction Method of Multipliers Applied to Polyhedral-Constrained Functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://arxiv.org/abs/1106.5730"&gt;HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://videolectures.net/nipsworkshops2011_boyd_multipliers/"&gt;Alternating Direction Method of Multipliers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="admm"></category><category term="optimization"></category><category term="distributed"></category></entry><entry><title>Stochastic Gradient Descent and Sparse $L_2$ regularization</title><link href="/blog/sparse-l2.html" rel="alternate"></link><published>2012-05-10T00:00:00-07:00</published><updated>2012-05-10T00:00:00-07:00</updated><author><name>Daniel Duckworth</name></author><id>tag:None,2012-05-10:/blog/sparse-l2.html</id><summary type="html">&lt;p&gt;Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter $w$ is simply the sum of the losses of each sample $i$, i.e.,&lt;/p&gt;
&lt;p&gt;$$
  L(w) = \sum_{i} l(x_i, y_i, w)
$$&lt;/p&gt;
&lt;p&gt;Basically any loss function you can think …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter $w$ is simply the sum of the losses of each sample $i$, i.e.,&lt;/p&gt;
&lt;p&gt;$$
  L(w) = \sum_{i} l(x_i, y_i, w)
$$&lt;/p&gt;
&lt;p&gt;Basically any loss function you can think of in the i.i.d sample regime can be composed this way. Since we assumed that your dataset was huge, there's no way you’re going to be able to load it all into memory for BFGS, so you choose to use Stochastic Gradient Descent. The update for sample $i$ with step size $\eta_t$ would then be,&lt;/p&gt;
&lt;p&gt;$$
  w_{t+1} = w_t - \eta_t \nabla_w l(x_i, y_i, w_t)
$$&lt;/p&gt;
&lt;p&gt;So far, so good. If $\nabla_w l(x_i, y_i, w)$ is sparse, then you only need to change a handful of $w$'s components. Of course, being the astute Machine Learning expert that you are, you know that you’re going to need some regularization. Let's redefine the total loss and take a look at our new update equation,&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
  L(w) &amp;amp; = \sum_{i} l(x_i, y_i, w) + \frac{\lambda}{2}||w||&lt;em t_1="t+1"&gt;2^2  \
  w&lt;/em&gt; &amp;amp; = w_t - \eta_t \left( \nabla_w l(x_i, y_i, w_t) + \lambda w_t \right)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Uh oh. Now that $w$ appears in our Stochastic Gradient Descent update equation, you’re going to have change every non-zero element of $w$ at every iteration, even if $\nabla_w l(x_i, y_i, w)$ is sparse! Whatever shall you do?&lt;/p&gt;
&lt;p&gt;The answer isn't as scary as you might think. Let’s do some algebraic manipulation from $t = 0$,&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
  w_{1}
  &amp;amp; = w_0 - \eta_0 \left( \nabla_w l(x_i, y_i, w_0) + \lambda w_0 \right) \
  &amp;amp; = w_0 - \eta_0 \nabla_w l(x_i, y_i, w_0) - \eta_0 \lambda w_0 \
  &amp;amp; = (1 - \eta_0 \lambda ) w_0 - \eta_0 \nabla_w l(x_i, y_i, w_0) \
  &amp;amp; = (1 - \eta_0 \lambda ) \left(
      w_0 - \frac{\eta_0}{1-\eta_0 \lambda } \nabla_w l(x_i, y_i, w_0)
    \right) \
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Do you see it now? $L_2$ regularization is really just a rescaling of $w_t$ at every iteration. Thus instead of keeping $w_t$, let’s keep track of,&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
  c_t &amp;amp; = \prod_{\tau=0}^t (1-\eta_{\tau} \lambda )  \
  \bar{w}_t &amp;amp; = \frac{w_t}{c_t}
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where you update $\bar{w}_t$ and $c_t$ by,&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
  \bar{w}&lt;em t_1="t+1"&gt;{t+1}
  &amp;amp; = \bar{w}_t - \frac{\eta_t}{(1 - \eta_t) c_t} \nabla_w l(x_i, w_i, c_t \bar{w}_t) \
  c&lt;/em&gt;
  &amp;amp; = (1 - \eta_t \lambda) c_t
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;And that’s it! As a final note, depending what value you choose for $\lambda$, $c_t$ is going to get really big or really small pretty fast. The usual "take the log" tricks aren't going to fly, either, as $c_t$ need not be positive. The only way around it I’ve found is to check every iteration if $c_t$ is getting out of hand, then transform $\bar{w}_{t} \leftarrow \bar{w}_t c_t$ and $c_t \leftarrow 1$ if it is.&lt;/p&gt;
&lt;p&gt;Finally, credit should be given where credit is due. This is a slightly more detailed explanation of &lt;a href="http://blog.smola.org/post/940672544/fast-quadratic-regularization-for-online-learning&amp;gt;"&gt;Alex Smola&lt;/a&gt; blog post from about a year ago, which in turn is accredited to Leon Bottou.&lt;/p&gt;</content><category term="regularization"></category><category term="sparsity"></category></entry></feed>